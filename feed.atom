<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://Furyton.github.io/misc-feed/index.html</id>
    <title>osmos::feed</title>
    <updated>2023-03-02T12:33:02.086Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="https://Furyton.github.io/misc-feed/index.html"/>
    <link rel="self" href="https://Furyton.github.io/misc-feed/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[Learning time-scales in two-layers neural networks]]></title>
        <id>http://arxiv.org/abs/2303.00055</id>
        <link href="http://arxiv.org/abs/2303.00055"/>
        <updated>2023-03-02T07:15:44.567Z</updated>
        <summary type="html"><![CDATA[Rapha\"el Berthier, Andrea Montanari, Kangjie Zhou]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Actor-Critic: A New Conditional Cross-Entropy Method for Policy
  Improvement]]></title>
        <id>http://arxiv.org/abs/1810.09103</id>
        <link href="http://arxiv.org/abs/1810.09103"/>
        <updated>2023-03-02T07:15:44.561Z</updated>
        <summary type="html"><![CDATA[Samuel Neumann, Sungsu Lim, Ajin Joseph, Yangchen Pan, Adam White,
  Martha White]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning curves for deep structured Gaussian feature models]]></title>
        <id>http://arxiv.org/abs/2303.00564</id>
        <link href="http://arxiv.org/abs/2303.00564"/>
        <updated>2023-03-02T07:15:44.542Z</updated>
        <summary type="html"><![CDATA[Jacob A. Zavatone-Veth, Cengiz Pehlevan]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Features Model with General Convex Regularization: A Fine Grained
  Analysis with Precise Asymptotic Learning Curves]]></title>
        <id>http://arxiv.org/abs/2204.02678</id>
        <link href="http://arxiv.org/abs/2204.02678"/>
        <updated>2023-03-02T07:15:44.537Z</updated>
        <summary type="html"><![CDATA[David Bosch, Ashkan Panahi, Ayca \"Ozcelikkale, Devdatt Dubhash]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Berkeley Working Papers in Middle Iranian Philology]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58116</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58116&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=berkeley-working-papers-in-middle-iranian-philology"/>
        <updated>2023-03-02T03:19:06.000Z</updated>
        <summary type="html"><![CDATA[Meant to send this more than a month ago. Interesting new journal in Iranian Studies Berkeley Working Papers in Middle Iranian Philology is a new open access e-journal hosted by UC Berkeley’s Department of Middle Eastern Languages and Cultures and edited by Adam Benkato and Arash Zeini. It publishes short and longer articles or research […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[keep thunderbird alive!]]></title>
        <id>http://xianblog.wordpress.com/?p=52316</id>
        <link href="https://xianblog.wordpress.com/2023/03/02/52316/"/>
        <updated>2023-03-01T23:23:28.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the attention mechanism in sequence models]]></title>
        <id>63f42411c79d98004d9f8686</id>
        <link href="https://www.jeremyjordan.me/attention/"/>
        <updated>2023-03-01T22:44:22.000Z</updated>
        <summary type="html"><![CDATA[In this blog post, we'll discuss a key innovation in sequence-to-sequence model architectures: the attention mechanism. This architecture innovation dramatically improved model performance for sequence-to-sequence tasks such as machine translation and text summarization. 
Moreover, the success of this attention mechanism led to the seminal paper, "Attention Is]]></summary>
        <author>
            <name>Jeremy Jordan</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teaching old labels new tricks in heterogeneous graphs]]></title>
        <id>http://ai.googleblog.com/2023/03/teaching-old-labels-new-tricks-in.html</id>
        <link href="http://ai.googleblog.com/2023/03/teaching-old-labels-new-tricks-in.html"/>
        <updated>2023-03-01T18:15:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Minji Yoon, Research Intern, and Bryan Perozzi, Research Scientist, Google Research, Graph Mining Team  
Industrial applications of machine learning are commonly composed of various items that have differing data modalities or feature distributions. Heterogeneous graphs (HGs) offer a unified view of these multimodal data systems by defining multiple types of nodes (for each data type) and edges (for the relation between data items). For instance, e-commerce networks might have [user, product, review] nodes or video platforms might have [channel, user, video, comment] nodes. Heterogeneous graph neural networks (HGNNs) learn node embeddings summarizing each node’s relationships into a vector. However, in real world HGs, there is often a label imbalance issue between different node …]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning ReLU networks to high uniform accuracy is intractable]]></title>
        <id>http://arxiv.org/abs/2205.13531</id>
        <link href="http://arxiv.org/abs/2205.13531"/>
        <updated>2023-03-01T07:15:24.618Z</updated>
        <summary type="html"><![CDATA[Julius Berner, Philipp Grohs, Felix Voigtlaender]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Agent-based Graph Neural Networks]]></title>
        <id>http://arxiv.org/abs/2206.11010</id>
        <link href="http://arxiv.org/abs/2206.11010"/>
        <updated>2023-03-01T07:15:24.573Z</updated>
        <summary type="html"><![CDATA[Karolis Martinkus, P\'al Andr\'as Papp, Benedikt Schesch, Roger
  Wattenhofer]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How optimal transport can tackle gender biases in multi-class
  neural-network classifiers for job recommendations?]]></title>
        <id>http://arxiv.org/abs/2302.14063</id>
        <link href="http://arxiv.org/abs/2302.14063"/>
        <updated>2023-03-01T07:15:24.513Z</updated>
        <summary type="html"><![CDATA[Fanny Jourdan, Titon Tshiongo Kaninku, Nicholas Asher, Jean-Michel
  Loubes, Laurent Risser]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Hidden Markov Models Using Conditional Samples]]></title>
        <id>http://arxiv.org/abs/2302.14753</id>
        <link href="http://arxiv.org/abs/2302.14753"/>
        <updated>2023-03-01T07:15:24.508Z</updated>
        <summary type="html"><![CDATA[Sham M. Kakade, Akshay Krishnamurthy, Gaurav Mahajan, Cyril Zhang]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding The Robustness of Self-supervised Learning Through Topic
  Modeling]]></title>
        <id>http://arxiv.org/abs/2203.03539</id>
        <link href="http://arxiv.org/abs/2203.03539"/>
        <updated>2023-03-01T07:15:24.466Z</updated>
        <summary type="html"><![CDATA[Zeping Luo, Shiyou Wu, Cindy Weng, Mo Zhou, Rong Ge]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[snapshot of Les Diablerets³ [jatp]]]></title>
        <id>http://xianblog.wordpress.com/?p=52249</id>
        <link href="https://xianblog.wordpress.com/2023/03/01/snapshot-of-les-diablerets%c2%b3-jatp/"/>
        <updated>2023-02-28T23:23:04.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Datasets at your fingertips in Google Search]]></title>
        <id>http://ai.googleblog.com/2023/02/datasets-at-your-fingertips-in-google.html</id>
        <link href="http://ai.googleblog.com/2023/02/datasets-at-your-fingertips-in-google.html"/>
        <updated>2023-02-28T22:13:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Natasha Noy, Research Scientist, and Omar Benjelloun, Software Engineer, Google Research  
Access to datasets is critical to many of today's endeavors across verticals and industries, whether scientific research, business analysis, or public policy. In the scientific community and throughout various levels of the public sector, reproducibility and transparency are essential for progress, so sharing data is vital. For one example, in the United States a recent new policy requires free and equitable access to outcomes of all federally funded research, including data and statistical information along with publications. 

To facilitate discovery of content with this level of statistical detail and better distill this information from across the web, Google now makes it easier to sear…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google Research, 2022 & beyond: Research community engagement]]></title>
        <id>http://ai.googleblog.com/2023/02/google-research-2022-beyond-research.html</id>
        <link href="http://ai.googleblog.com/2023/02/google-research-2022-beyond-research.html"/>
        <updated>2023-02-28T19:40:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Posted by Leslie Yeh, Director, University Relations    
    

(This is Part 9 in our series of posts covering different topical areas of research at Google. You can find other posts in the series here.)
   

Sharing knowledge is essential to Google’s research philosophy — it accelerates technological progress and expands capabilities community-wide. Solving complex problems requires bringing together diverse minds and resources collaboratively. This can be accomplished through building local and global connections with multidisciplinary experts and impacted communities. In partnership with these stakeholders, we bring our technical leadership, product footprint, and resources to make progress against some of society's greatest opportunities and challenges.   
 We at Google see i…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alef’s Corner: Democracy (Israel, 2023)]]></title>
        <id>http://gilkalai.wordpress.com/?p=23914</id>
        <link href="https://gilkalai.wordpress.com/2023/02/28/alefs-corner-democracy-israel-2023/"/>
        <updated>2023-02-28T16:27:04.000Z</updated>
        <summary type="html"><![CDATA[Democracy in Hebrew is דמוקרטיה represented by the letter “dalet” ד]]></summary>
        <author>
            <name>Gil Kalai</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alef’s Corner: Democracy (Israel, 2023)]]></title>
        <id>http://gilkalai.wordpress.com/?p=23914</id>
        <link href="https://gilkalai.wordpress.com/2023/02/28/alefs-corner-democracy-israel-2023/"/>
        <updated>2023-02-28T16:27:04.000Z</updated>
        <summary type="html"><![CDATA[Democracy in Hebrew is דמוקרטיה represented by the letter “dalet” ד]]></summary>
        <author>
            <name>Gil Kalai</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Syllable rhythm in English and Mandarin]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58120</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58120&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=syllable-rhythm-in-english-and-mandarin"/>
        <updated>2023-02-28T16:20:00.000Z</updated>
        <summary type="html"><![CDATA[I've always been skeptical of the distinction between "stress-timed" and "syllable-timed" languages, at least as a claim about the phonetic facts of speech timing as opposed to the psychological dimensions of speech production and perception. Syllable durations in all languages vary widely, due to differences in the intrinsic durations of different vowels and consonants, the […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[生成扩散模型漫谈（十八）：得分匹配 = 条件得分匹配]]></title>
        <id>https://kexue.fm/archives/9509</id>
        <link href="https://kexue.fm/archives/9509"/>
        <updated>2023-02-28T13:23:00.000Z</updated>
        <summary type="html"><![CDATA[在前面的介绍中，我们多次提及“得分匹配”和“条件得分匹配”，它们是扩散模型、能量模型等经常出现的概念，特别是很多文章直接说扩散模型的训练目标是“得分匹配”，但事实上当前主流的扩散模型如DDPM的...]]></summary>
        <author>
            <name>苏剑林</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChatGPT writes VHM]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58111</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58111&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=chatgpt-writes-vhm"/>
        <updated>2023-02-28T10:47:25.000Z</updated>
        <summary type="html"><![CDATA[[This is a guest post by ChatGPT] Write a Language Log blog post by Victor H. Mair Title: "The Wonders of the Chinese Language" As one of the world's oldest and most complex languages, Chinese has captivated linguists and language enthusiasts for centuries. From its unique writing system to its vast vocabulary and intricate grammar, […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empowering Graph Representation Learning with Test-Time Graph
  Transformation]]></title>
        <id>http://arxiv.org/abs/2210.03561</id>
        <link href="http://arxiv.org/abs/2210.03561"/>
        <updated>2023-02-28T07:15:52.573Z</updated>
        <summary type="html"><![CDATA[Wei Jin, Tong Zhao, Jiayuan Ding, Yozen Liu, Jiliang Tang, Neil Shah]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural networks for learning personality traits from natural language]]></title>
        <id>http://arxiv.org/abs/2302.13782</id>
        <link href="http://arxiv.org/abs/2302.13782"/>
        <updated>2023-02-28T07:15:52.566Z</updated>
        <summary type="html"><![CDATA[Giorgia Adorni]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principled and Efficient Transfer Learning of Deep Models via Neural
  Collapse]]></title>
        <id>http://arxiv.org/abs/2212.12206</id>
        <link href="http://arxiv.org/abs/2212.12206"/>
        <updated>2023-02-28T07:15:52.526Z</updated>
        <summary type="html"><![CDATA[Xiao Li, Sheng Liu, Jinxin Zhou, Xinyu Lu, Carlos Fernandez-Granda,
  Zhihui Zhu, Qing Qu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Pixel Shortcut: on the Learning Preference of Deep Neural Networks]]></title>
        <id>http://arxiv.org/abs/2205.12141</id>
        <link href="http://arxiv.org/abs/2205.12141"/>
        <updated>2023-02-28T07:15:52.516Z</updated>
        <summary type="html"><![CDATA[Shutong Wu, Sizhe Chen, Cihang Xie, Xiaolin Huang]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[a journal of the plague, sword, and famine year [with avocados]]]></title>
        <id>http://xianblog.wordpress.com/?p=52168</id>
        <link href="https://xianblog.wordpress.com/2023/02/28/a-journal-of-the-plague-sword-and-famine-year-with-avocados/"/>
        <updated>2023-02-27T23:23:19.000Z</updated>
        <summary type="html"><![CDATA[Read two books by Alix E. Harrow, A Spindle Splintered and A Mirror Mended, which are modern takes on Sleeping Beauty and Snow White. Rather hilarious for their tone and dry humour, if rather YAs… And Undercover, a novella by Tasmyn Muir. Rather well-build steampunk around a moving city and… zombies. Plus a new volume […]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Iteration marks and repeaters in ancient Chinese texts]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58102</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58102&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=iteration-marks-and-repeaters-in-ancient-chinese-texts"/>
        <updated>2023-02-27T17:49:47.000Z</updated>
        <summary type="html"><![CDATA[Let us begin this post with a brief introduction to the 16th-century Hokkien (Minnan) drama, Tale of the Lychee Mirror: The Tale of the Lychee Mirror (traditional Chinese: 荔鏡記; simplified Chinese: 荔镜记; pinyin: Lì jìng jì; Pe̍h-ōe-jī: Nāi-kèng-kì, Lē-kèng-kì) is a play written by an unknown author in the Ming dynasty. Tân Saⁿ and Gō͘-niû […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Extraction from RLWE to LWE]]></title>
        <id>https://jeremykun.com/?p=118643</id>
        <link href="https://jeremykun.com/2023/02/27/sample-extraction-from-rlwe-to-lwe/"/>
        <updated>2023-02-27T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[In this article I’ll derive a trick used in FHE called sample extraction. In brief, it allows one to partially convert a ciphertext in the Ring Learning With Errors (RLWE) scheme to the Learning With Errors (LWE) scheme. Here are some other articles I’ve written about other FHE building blocks, though they are not prerequisites […]]]></summary>
        <author>
            <name>j2kun</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Linearized Laplace Approximation for Bayesian Deep Learning]]></title>
        <id>http://arxiv.org/abs/2302.12565</id>
        <link href="http://arxiv.org/abs/2302.12565"/>
        <updated>2023-02-27T07:15:52.156Z</updated>
        <summary type="html"><![CDATA[Luis A. Ortega, Sim\'on Rodr\'iguez Santana, Daniel Hern\'andez-Lobato]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Moments: Robustly Learning Affine Transformations with
  Asymptotically Optimal Error]]></title>
        <id>http://arxiv.org/abs/2302.12289</id>
        <link href="http://arxiv.org/abs/2302.12289"/>
        <updated>2023-02-27T07:15:52.139Z</updated>
        <summary type="html"><![CDATA[He Jia, Pravesh K . Kothari, Santosh S. Vempala]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reward Learning as Doubly Nonparametric Bandits: Optimal Design and
  Scaling Laws]]></title>
        <id>http://arxiv.org/abs/2302.12349</id>
        <link href="http://arxiv.org/abs/2302.12349"/>
        <updated>2023-02-27T07:15:52.115Z</updated>
        <summary type="html"><![CDATA[Kush Bhatia, Wenshuo Guo, Jacob Steinhardt]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lower Bounds on the Depth of Integral ReLU Neural Networks via Lattice
  Polytopes]]></title>
        <id>http://arxiv.org/abs/2302.12553</id>
        <link href="http://arxiv.org/abs/2302.12553"/>
        <updated>2023-02-27T07:15:52.109Z</updated>
        <summary type="html"><![CDATA[Christian Haase, Christoph Hertrich, Georg Loho]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty Injection: A Deep Learning Method for Robust Optimization]]></title>
        <id>http://arxiv.org/abs/2302.12304</id>
        <link href="http://arxiv.org/abs/2302.12304"/>
        <updated>2023-02-27T07:15:52.100Z</updated>
        <summary type="html"><![CDATA[Wei Cui, Wei Yu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diophantine riddle]]></title>
        <id>http://xianblog.wordpress.com/?p=52260</id>
        <link href="https://xianblog.wordpress.com/2023/02/27/diophantine-riddle/"/>
        <updated>2023-02-26T23:23:08.000Z</updated>
        <summary type="html"><![CDATA[The weekly riddle from The Riddler is to find solutions to the Diophantine equation c³-c=b²+4 (when b and c are positive integers). First, forget about ChatGPT since it states this is a Pell equation. With a wrong argument. Second, when running a basic R code, using as.double to handle larger integers, the only solution less […]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Taiwan Navy recruitment ad language puzzle]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58094</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58094&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=taiwan-navy-recruitment-ad-language-puzzle"/>
        <updated>2023-02-26T20:50:02.000Z</updated>
        <summary type="html"><![CDATA[Photo of a Taiwan Naval Academy recruitment ad in the Taipei MRT which references the One Piece ワンピース manga series from Japan: First some basic information about One Piece, then I'll explain what the Chinese says: One Piece (stylized in all caps) is a Japanese manga series written and illustrated by Eiichiro Oda. It has been serialized in Shueisha's shōnen manga magazine […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diversification of Proto-Austronesian]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58099</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58099&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=diversification-of-proto-austronesian"/>
        <updated>2023-02-26T19:53:15.000Z</updated>
        <summary type="html"><![CDATA[Important archeological news from Tainan: South Taiwan park renovation project paused after archaeological artifacts unearthed  Artifact pieces belonging to neolithic Niuchouzi Culture discovered, date back to 3000-4500 years ago. By Stephanie Chiang, Taiwan News (2/26/23) Finds include "orange-colored pottery made of fine sand-bearing rope patterns, polished hoe-axes, polished adze-chisels, and shell mounds." The nature of […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[surveying homelessness]]></title>
        <id>http://xianblog.wordpress.com/?p=52284</id>
        <link href="https://xianblog.wordpress.com/2023/02/26/surveying-homelessness/"/>
        <updated>2023-02-25T23:23:12.000Z</updated>
        <summary type="html"><![CDATA[A recent NYT article, entitled “582,462 and Counting“, is describing how the USA Federal Administation is running a yearly survey of homeless people. By sending agents and volunteers in the streets and shelters to get an idea of the magnitude of the problem. The figure of 582,462 was the one produced by HUD (the US […]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transcription vs. transliteration vs. translation in cartography]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58088</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58088&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=transcription-vs-transliteration-vs-translation-in-cartography"/>
        <updated>2023-02-25T03:49:40.000Z</updated>
        <summary type="html"><![CDATA[In this post, I wanted to do something that I thought would be fairly simple, viz., address the question of the "rectification" of Russian place names in areas proximate to populations speaking Sinitic languages.  This sort of rectification is also a hot topic where Russia borders on Ukraine.  There, however, the task is simpler, because […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bernard l’Hermite [jatp]]]></title>
        <id>http://xianblog.wordpress.com/?p=52272</id>
        <link href="https://xianblog.wordpress.com/2023/02/25/bernard-lhermite-jatp/"/>
        <updated>2023-02-24T23:23:28.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A vision-language approach for foundational UI understanding]]></title>
        <id>http://ai.googleblog.com/2023/02/a-vision-language-approach-for.html</id>
        <link href="http://ai.googleblog.com/2023/02/a-vision-language-approach-for.html"/>
        <updated>2023-02-24T20:21:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Yang Li, Research Scientist, and Gang Li, Software Engineer, Google Research   
The computational understanding of user interfaces (UI) is a key step towards achieving intelligent UI behaviors. Previously, we investigated various UI modeling tasks, including widget captioning, screen summarization, and command grounding, that address diverse interaction scenarios such as automation and accessibility. We also demonstrated how machine learning can help user experience practitioners improve UI quality by diagnosing tappability confusion and providing insights for improving UI design. These works along with those developed by others in the field have showcased how deep neural networks can potentially transform end user experiences and the interaction design practice.  

With these su…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chelsea Finn - Neural networks make stuff up. What should we do about it?]]></title>
        <id>https://mlfoundations.org/talk/finn/</id>
        <link href="https://mlfoundations.org/talk/finn/"/>
        <updated>2023-02-24T14:00:00.000Z</updated>
        <summary type="html"><![CDATA[When machine learning models are deployed into the world, they inevitably encounter scenarios that differ from their training data, either novel contexts where the appropriate answers may differ or scenarios with new out-of-distribution inputs. Unfortunately, in such situations, deep neural network models make up answers or misunderstand the context, making the models unreliable. Even if a model makes useful predictions for many examples, such unreliability poses considerable risks when these models are interacting with real people and ultimately precludes models from being useful in safety-critical applications. In this talk, I’ll discuss some ways that we might cope with and address the unreliability of neural network models. As an initial coping strategy, I will first discuss a technique for detecting whether some content was generated by a machine learning model, leveraging the probability distribution that the model assigns to different content. Next, I will describe an approach for enabling neural network models to better estimate what they don’t know, such that they can abstain from making predictions on such inputs (i.e. selective classification). I will lastly describe methods for adapting models with small amounts of data to improve their accuracy under distribution shift.]]></summary>
        <author>
            <name>Harvard ML Foundations</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Words are all you need? Language as an approximation for human
  similarity judgments]]></title>
        <id>http://arxiv.org/abs/2206.04105</id>
        <link href="http://arxiv.org/abs/2206.04105"/>
        <updated>2023-02-24T07:15:17.364Z</updated>
        <summary type="html"><![CDATA[Raja Marjieh, Pol van Rijn, Ilia Sucholutsky, Theodore R. Sumers,
  Harin Lee, Thomas L. Griffiths, Nori Jacoby]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayes meets Bernstein at the Meta Level: an Analysis of Fast Rates in
  Meta-Learning with PAC-Bayes]]></title>
        <id>http://arxiv.org/abs/2302.11709</id>
        <link href="http://arxiv.org/abs/2302.11709"/>
        <updated>2023-02-24T07:15:17.358Z</updated>
        <summary type="html"><![CDATA[Charles Riou, Pierre Alquier, Badr-Eddine Ch\'erief-Abdellatif]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Find a witness or shatter: the landscape of computable PAC learning]]></title>
        <id>http://arxiv.org/abs/2302.04731</id>
        <link href="http://arxiv.org/abs/2302.04731"/>
        <updated>2023-02-24T07:15:17.353Z</updated>
        <summary type="html"><![CDATA[Valentino Delle Rose, Alexander Kozachinskiy, Cristobal Rojas, Tomasz
  Steifer]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Laws For Deep Learning Based Image Reconstruction]]></title>
        <id>http://arxiv.org/abs/2209.13435</id>
        <link href="http://arxiv.org/abs/2209.13435"/>
        <updated>2023-02-24T07:15:17.347Z</updated>
        <summary type="html"><![CDATA[Tobit Klug, Reinhard Heckel]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Manifold Dimensions with Conditional Variational Autoencoders]]></title>
        <id>http://arxiv.org/abs/2302.11756</id>
        <link href="http://arxiv.org/abs/2302.11756"/>
        <updated>2023-02-24T07:15:17.265Z</updated>
        <summary type="html"><![CDATA[Yijia Zheng, Tong He, Yixuan Qiu, David Wipf]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Crisis" mentality infects China]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58083</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58083&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=crisis-mentality-infects-china"/>
        <updated>2023-02-24T06:52:17.000Z</updated>
        <summary type="html"><![CDATA[From the recent meeting between Putin and Wang Yi (Director of the Office of the Central Foreign Affairs Commission of the Chinese Communist Party): Here is the writing in yellow: Pǔqīn huì Wáng Yì yāo Xí fǎng É, lù pāo “zhèngzhì jiějué wéijī”, Měi bù lè jiàn? 普欽會王毅邀習訪俄，陸拋“政治解決危機”， 美不樂見？ "When Putin met Wang Yi and invited Xi […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[地震越来越频繁了吗？]]></title>
        <id>https://cosx.org/2023/02/earthquake/</id>
        <link href="https://cosx.org/2023/02/earthquake/"/>
        <updated>2023-02-24T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[1 写作背景 2 本文结构 3 地震背景信息 4 1973-2022 年全球地震变化 4.1 数据准备 4.2 震次趋势（年度） 4.3 震级分布（总体情况） 4.4 震级分布（按年分组） 4.4.1 抖动图 4.4.2 岭线图]]></summary>
        <author>
            <name>统计之都</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The impenetrability of cursive for students from the PRC]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58079</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58079&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-impenetrability-of-cursive-for-students-from-the-prc"/>
        <updated>2023-02-23T23:35:41.000Z</updated>
        <summary type="html"><![CDATA[Today I had a revelation about my handwriting on the blackboard. By far the majority of students in all of my classes come from mainland China.  They are by nature reticent to speak up, but when it comes to engaging in discussion about material that I have written on the board, they are essentially deadly […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[one year of Russian terror]]></title>
        <id>http://xianblog.wordpress.com/?p=52311</id>
        <link href="https://xianblog.wordpress.com/2023/02/24/one-year-under-russian-aggression/"/>
        <updated>2023-02-23T23:23:21.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-training generalist agents using offline reinforcement learning]]></title>
        <id>http://ai.googleblog.com/2023/02/pre-training-generalist-agents-using.html</id>
        <link href="http://ai.googleblog.com/2023/02/pre-training-generalist-agents-using.html"/>
        <updated>2023-02-23T21:21:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Aviral Kumar, Student Researcher, and Sergey Levine, Research Scientist, Google Research    
Reinforcement learning (RL) algorithms can learn skills to solve decision-making tasks like playing games, enabling robots to pick up objects, or even optimizing microchip designs. However, running RL algorithms in the real world requires expensive active data collection. Pre-training on diverse datasets has proven to enable data-efficient fine-tuning for individual downstream tasks in natural language processing (NLP) and vision problems. In the same way that BERT or GPT-3 models provide general-purpose initialization for NLP, large RL–pre-trained models could provide general-purpose initialization for decision-making. So, we ask the question: Can we enable similar pre-training to accele…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google Research, 2022 & beyond: Health]]></title>
        <id>http://ai.googleblog.com/2023/02/google-research-2022-beyond-health.html</id>
        <link href="http://ai.googleblog.com/2023/02/google-research-2022-beyond-health.html"/>
        <updated>2023-02-23T19:01:00.006Z</updated>
        <summary type="html"><![CDATA[Posted by Greg Corrado, Distinguished Scientist, and Yossi Matias, VP Engineering and Research, Google Research    
    

(This is Part 8 in our series of posts covering different topical areas of research at Google. You can find other posts in the series here.)
   

Google’s focus on AI stems from the conviction that this transformational technology will benefit society through its capacity to assist, complement, and empower people in almost every field and sector.  In no area is the magnitude of this opportunity greater than in the spheres of healthcare and medicine.  Commensurate with our mission to demonstrate these societal benefits, Google Research’s programs in applied machine learning (ML) have helped place Alphabet among the top five most impactful corporate research institutions …]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vignettes of quality data impoverishment in the world of PRC AI]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58068</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58068&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=vignettes-of-quality-data-impoverishment-in-the-world-of-prc-ai"/>
        <updated>2023-02-23T13:00:49.000Z</updated>
        <summary type="html"><![CDATA[Some snippets: Limited data sets a hurdle as China plays catch-up to ChatGPT Lack of high-quality Chinese texts on Internet a barrier to training AI models. Ryan McMorrow, Nian Liu, Eleanor Olcott, and Madhumita Murgia, FT, Ars Technica (2/21/23) … Baidu struggled with its previous attempt at a chatbot, known as Plato, which analysts said […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）]]></title>
        <id>https://kexue.fm/archives/9497</id>
        <link href="https://kexue.fm/archives/9497"/>
        <updated>2023-02-23T12:54:00.000Z</updated>
        <summary type="html"><![CDATA[历史总是惊人地相似。当初笔者在写《生成扩散模型漫谈（十四）：构建ODE的一般步骤（上）》（当时还没有“上”这个后缀）时，以为自己已经搞清楚了构建ODE式扩散的一般步骤，结果读者 @gaohuaz...]]></summary>
        <author>
            <name>苏剑林</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning nonparametric ordinary differential equations from noisy data]]></title>
        <id>http://arxiv.org/abs/2206.15215</id>
        <link href="http://arxiv.org/abs/2206.15215"/>
        <updated>2023-02-23T07:15:16.966Z</updated>
        <summary type="html"><![CDATA[Kamel Lahouel, Michael Wells, Victor Rielly, Ethan Lew, David Lovitz,
 , Bruno M. Jedynak]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Multiple Sources for Data-to-Text and Text-to-Data]]></title>
        <id>http://arxiv.org/abs/2302.11269</id>
        <link href="http://arxiv.org/abs/2302.11269"/>
        <updated>2023-02-23T07:15:16.956Z</updated>
        <summary type="html"><![CDATA[Song Duong, Alberto Lumbreras, Mike Gartrell, Patrick Gallinari]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SGD learning on neural networks: leap complexity and saddle-to-saddle
  dynamics]]></title>
        <id>http://arxiv.org/abs/2302.11055</id>
        <link href="http://arxiv.org/abs/2302.11055"/>
        <updated>2023-02-23T07:15:16.951Z</updated>
        <summary type="html"><![CDATA[Emmanuel Abbe, Enric Boix-Adsera, Theodor Misiakiewicz]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Pessimism in Dynamic Treatment Regimes: A Bayesian Learning
  Approach]]></title>
        <id>http://arxiv.org/abs/2210.14420</id>
        <link href="http://arxiv.org/abs/2210.14420"/>
        <updated>2023-02-23T07:15:16.921Z</updated>
        <summary type="html"><![CDATA[Yunzhe Zhou, Zhengling Qi, Chengchun Shi, Lexin Li]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Drop Edges and Adapt: a Fairness Enforcing Fine-tuning for Graph Neural
  Networks]]></title>
        <id>http://arxiv.org/abs/2302.11479</id>
        <link href="http://arxiv.org/abs/2302.11479"/>
        <updated>2023-02-23T07:15:16.855Z</updated>
        <summary type="html"><![CDATA[Indro Spinelli, Riccardo Bianchini, Simone Scardapane]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaceless pinyin]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58058</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58058&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=spaceless-pinyin"/>
        <updated>2023-02-23T01:47:27.000Z</updated>
        <summary type="html"><![CDATA[From the importer's label, carefully placed to obscure the safety instructions (the "do"s and "do not"s) of an electronic gas igniter: I have a feeling that my friend, Mark Swofford, the doyen of rules governing pinyin, will not go for this. Selected readings "Pinyin in practice" (10/13/11) "Pure Pinyin" (10/15/16) "Words in Mandarin: twin kle twin […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-modal writing among Hong Kong teens]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58044</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58044&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=multi-modal-writing-among-hong-kong-teens"/>
        <updated>2023-02-23T01:44:27.000Z</updated>
        <summary type="html"><![CDATA[From Jenny Chu: Knowing your interest in multi-modal writing systems, I thought you might be amused by the attached screencap. It is from a WhatsApp group chat of S6 (final year) students in Hong Kong; one of them is asking the others what they would like to do on the afternoon of their last day […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A crook that protects your belongings]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58026</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58026&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=a-crook-that-protects-your-belongings"/>
        <updated>2023-02-23T01:30:12.000Z</updated>
        <summary type="html"><![CDATA[The Chinese notice says: Jǐngfāng tíshì:Qǐng tuǒshàn bǎoguǎn nín de xínglǐ wùpǐn, bìng shǐyòng běndiàn wèi nín tígōng de ānquán gōu. 警方提示：请妥善保管您的行李物品，并使用本店为您提供的安全钩。 "A reminder from the police:Please take good care of your luggage and belongings, and use the safety hook provided for you by our store." About twenty years ago, at a table where I […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do not major in the changing room]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=57994</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=57994&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=do-not-major-in-the-changing-room"/>
        <updated>2023-02-23T00:56:16.000Z</updated>
        <summary type="html"><![CDATA[The Chinese notice says: qǐng wù zài gēngyī shì dà, xiǎobiàn 请勿在更衣室大，小便 "Please do not defecate or urinate in the locker / changing room." Selected readings "Pull!" (11/11/21) — with a lengthy, essential bibliography of posts on urination "Pee straight" (9/5/13) If you do a search on "defecate", "micturate", "poo", and related verbs and nouns, […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[snapshot of Les Diablerets² [jatp]]]></title>
        <id>http://xianblog.wordpress.com/?p=52245</id>
        <link href="https://xianblog.wordpress.com/2023/02/23/snapshot-of-les-diablerets%c2%b2-jatp/"/>
        <updated>2023-02-22T23:23:38.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Suppressing quantum errors by scaling a surface code logical qubit]]></title>
        <id>http://ai.googleblog.com/2023/02/suppressing-quantum-errors-by-scaling.html</id>
        <link href="http://ai.googleblog.com/2023/02/suppressing-quantum-errors-by-scaling.html"/>
        <updated>2023-02-22T16:05:00.002Z</updated>
        <summary type="html"><![CDATA[Posted by Hartmut Neven, VP of Engineering, and Julian Kelly, Director of Quantum Hardware, on behalf of the Google Quantum AI Team  
Many years from today, scientists will be able to use fault-tolerant quantum computers for large-scale computations with applications across science and industry. These quantum computers will be much bigger than today, consisting of millions of coherent quantum bits, or qubits. But there’s a catch — these basic building blocks must be good enough or the systems will be overrun with errors. 

Currently, the error rates of the qubits on our 3rd generation Sycamore processor are typically between 1 in 10,000 to 1 in 100. Through our work and that of others, we understand that developing large-scale quantum computers will require far lower error rates. We will n…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Valid Inference for Machine Learning Model Parameters]]></title>
        <id>http://arxiv.org/abs/2302.10840</id>
        <link href="http://arxiv.org/abs/2302.10840"/>
        <updated>2023-02-22T07:15:27.871Z</updated>
        <summary type="html"><![CDATA[Neil Dey, Jonathan P. Williams]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Local Norms in Exp-concave Statistical Learning]]></title>
        <id>http://arxiv.org/abs/2302.10726</id>
        <link href="http://arxiv.org/abs/2302.10726"/>
        <updated>2023-02-22T07:15:27.850Z</updated>
        <summary type="html"><![CDATA[Nikita Puchkin, Nikita Zhivotovskiy]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GDBN: a Graph Neural Network Approach to Dynamic Bayesian Network]]></title>
        <id>http://arxiv.org/abs/2302.10804</id>
        <link href="http://arxiv.org/abs/2302.10804"/>
        <updated>2023-02-22T07:15:27.844Z</updated>
        <summary type="html"><![CDATA[Yang Sun, Yifan Xie]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Infomin Learning]]></title>
        <id>http://arxiv.org/abs/2302.10701</id>
        <link href="http://arxiv.org/abs/2302.10701"/>
        <updated>2023-02-22T07:15:27.532Z</updated>
        <summary type="html"><![CDATA[Yanzhi Chen, Weihao Sun, Yingzhen Li, Adrian Weller]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Transformers without Shortcuts: Modifying Self-attention for
  Faithful Signal Propagation]]></title>
        <id>http://arxiv.org/abs/2302.10322</id>
        <link href="http://arxiv.org/abs/2302.10322"/>
        <updated>2023-02-22T07:15:27.486Z</updated>
        <summary type="html"><![CDATA[Bobby He, James Martens, Guodong Zhang, Aleksandar Botev, Andrew
  Brock, Samuel L Smith, Yee Whye Teh]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provable Copyright Protection for Generative Models]]></title>
        <id>http://windowsontheory.org/?p=8548</id>
        <link href="https://windowsontheory.org/2023/02/21/provable-copyright-protection-for-generative-models/"/>
        <updated>2023-02-22T02:22:01.000Z</updated>
        <summary type="html"><![CDATA[See arxiv link for paper by Nikhil Vyas, Sham Kakade, and me. Conditional generative models hold much promise for novel content creation. Whether it is generating a snippet of code, piece of text, or image, such models can potentially save substantial human effort and unlock new capabilities. But there is a fly in this ointment. … Continue reading Provable Copyright Protection for Generative Models]]></summary>
        <author>
            <name>Boaz Barak</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provable Copyright Protection for Generative Models]]></title>
        <id>http://windowsontheory.org/?p=8548</id>
        <link href="https://windowsontheory.org/2023/02/21/provable-copyright-protection-for-generative-models/"/>
        <updated>2023-02-22T02:22:01.000Z</updated>
        <summary type="html"><![CDATA[See arxiv link for paper by Nikhil Vyas, Sham Kakade, and me. Conditional generative models hold much promise for novel content creation. Whether it is generating a snippet of code, piece of text, or image, such models can potentially save substantial human effort and unlock new capabilities. But there is a fly in this ointment. … Continue reading Provable Copyright Protection for Generative Models]]></summary>
        <author>
            <name>Boaz Barak</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[统计之都访谈第47期：斯坦福大学青椒--雷理骅访谈]]></title>
        <id>https://cosx.org/2023/02/interview-of-lihua-lei/</id>
        <link href="https://cosx.org/2023/02/interview-of-lihua-lei/"/>
        <updated>2023-02-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[统计之都访谈第47期。2022年8月初，正是北美一年一届的 Joint Statistical Meetings。统计之都在会议间隔对雷理骅进行了采访。在本文发布之际，雷理骅]]></summary>
        <author>
            <name>统计之都</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[stops making sense [cover]]></title>
        <id>http://xianblog.wordpress.com/?p=52253</id>
        <link href="https://xianblog.wordpress.com/2023/02/22/stops-making-sense-cover/"/>
        <updated>2023-02-21T23:23:07.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google Research, 2022 & beyond: Natural sciences]]></title>
        <id>http://ai.googleblog.com/2023/02/google-research-2022-beyond-natural.html</id>
        <link href="http://ai.googleblog.com/2023/02/google-research-2022-beyond-natural.html"/>
        <updated>2023-02-21T22:16:00.006Z</updated>
        <summary type="html"><![CDATA[Posted by John Platt, Distinguished Scientist, Google Research    
    

(This is Part 7 in our series of posts covering different topical areas of research at Google. You can find other posts in the series here.)
   

It's an incredibly exciting time to be a scientist. With the amazing advances in machine learning (ML) and quantum computing, we now have powerful new tools that enable us to act on our curiosity, collaborate in new ways, and radically accelerate progress toward breakthrough scientific discoveries. 
Since joining Google Research eight years ago, I’ve had the privilege of being part of a community of talented researchers fascinated by applying cutting-edge computing to push the boundaries of what is possible in applied science. Our teams are exploring topics across the physic…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hardness of Agnostically Learning Halfspaces from Worst-Case Lattice
  Problems]]></title>
        <id>http://arxiv.org/abs/2207.14030</id>
        <link href="http://arxiv.org/abs/2207.14030"/>
        <updated>2023-02-21T07:15:27.434Z</updated>
        <summary type="html"><![CDATA[Stefan Tiegel]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Bayesian Neural Networks Need To Be Fully Stochastic?]]></title>
        <id>http://arxiv.org/abs/2211.06291</id>
        <link href="http://arxiv.org/abs/2211.06291"/>
        <updated>2023-02-21T07:15:27.410Z</updated>
        <summary type="html"><![CDATA[Mrinank Sharma, Sebastian Farquhar, Eric Nalisnick, Tom Rainforth]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sharp analysis of EM for learning mixtures of pairwise differences]]></title>
        <id>http://arxiv.org/abs/2302.10066</id>
        <link href="http://arxiv.org/abs/2302.10066"/>
        <updated>2023-02-21T07:15:27.396Z</updated>
        <summary type="html"><![CDATA[Abhishek Dhawan, Cheng Mao, Ashwin Pananjady]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Collaborative Self-Supervised Learning Method for Radiomic Data]]></title>
        <id>http://arxiv.org/abs/2302.09807</id>
        <link href="http://arxiv.org/abs/2302.09807"/>
        <updated>2023-02-21T07:15:27.343Z</updated>
        <summary type="html"><![CDATA[Zhiyuan Li, Hailong Li, Anca L. Ralescu, Jonathan R. Dillman, Nehal A.
  Parikh, Lili He]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning for inverse problems with unknown operator]]></title>
        <id>http://arxiv.org/abs/2108.02744</id>
        <link href="http://arxiv.org/abs/2108.02744"/>
        <updated>2023-02-21T07:15:27.323Z</updated>
        <summary type="html"><![CDATA[Miguel del Alamo]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Research fellow chAIrs in Grenoble [reposted]]]></title>
        <id>http://xianblog.wordpress.com/?p=52301</id>
        <link href="https://xianblog.wordpress.com/2023/02/21/research-fellow-chairs-in-grenoble-reposted/"/>
        <updated>2023-02-20T23:23:48.000Z</updated>
        <summary type="html"><![CDATA[MIAI, the Grenoble Multidisciplinary Institute in Artificial Intelligence , is opening three research fellow chairs in AI reserved to persons who have spent most of their research career outside France. To be eligible, candidates must hold a PhD from a non-French university obtained after January 2014 for male applicants and after 2014-n, where n is […]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[03/05 1pm-2pm: CRiSM Seminar]]></title>
        <id>8a1785d88659e0df01866f1c32713ef9</id>
        <link href="https://warwick.ac.uk/fac/sci/statistics/event_diary/?calendarItem=8a1785d88659e0df01866f1c32713ef8"/>
        <updated>2023-02-20T13:56:04.000Z</updated>
        <summary type="html"><![CDATA[When:
	
  		13:00
		-
		14:00, Wed, 03 May '23

	
Where: MB0.07]]></summary>
        <author>
            <name>Statistics</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Port-metriplectic neural networks: thermodynamics-informed machine
  learning of complex physical systems]]></title>
        <id>http://arxiv.org/abs/2211.01873</id>
        <link href="http://arxiv.org/abs/2211.01873"/>
        <updated>2023-02-20T07:15:42.760Z</updated>
        <summary type="html"><![CDATA[Quercus Hern\'andez, Alberto Bad\'ias, Francisco Chinesta, El\'ias
  Cueto]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Causal Representation Learning for Out-of-Distribution Motion
  Forecasting]]></title>
        <id>http://arxiv.org/abs/2302.08635</id>
        <link href="http://arxiv.org/abs/2302.08635"/>
        <updated>2023-02-20T07:15:42.717Z</updated>
        <summary type="html"><![CDATA[Shayan Shirahmad Gale Bagi, Zahra Gharaee, Oliver Schulte, Mark
  Crowley]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intrinsic and extrinsic deep learning on manifolds]]></title>
        <id>http://arxiv.org/abs/2302.08606</id>
        <link href="http://arxiv.org/abs/2302.08606"/>
        <updated>2023-02-20T07:15:42.712Z</updated>
        <summary type="html"><![CDATA[Yihao Fang, Ilsang Ohn, Vijay Gupta, Lizhen Lin]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAC-Bayesian Generalization Bounds for Adversarial Generative Models]]></title>
        <id>http://arxiv.org/abs/2302.08942</id>
        <link href="http://arxiv.org/abs/2302.08942"/>
        <updated>2023-02-20T07:15:42.696Z</updated>
        <summary type="html"><![CDATA[Sokhna Diarra Mbacke, Florence Clerc, Pascal Germain]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[snapshot of Les Diablerets¹ [jatp]]]></title>
        <id>http://xianblog.wordpress.com/?p=52240</id>
        <link href="https://xianblog.wordpress.com/2023/02/20/snapshot-of-les-diablerets-jatp/"/>
        <updated>2023-02-19T23:23:51.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FRMT: A benchmark for few-shot region-aware machine translation]]></title>
        <id>http://ai.googleblog.com/2023/02/frmt-benchmark-for-few-shot-region.html</id>
        <link href="http://ai.googleblog.com/2023/02/frmt-benchmark-for-few-shot-region.html"/>
        <updated>2023-02-17T18:20:00.004Z</updated>
        <summary type="html"><![CDATA[Posted by Parker Riley, Software Engineer, and Jan Botha, Research Scientist, Google Research  
Many languages spoken worldwide cover numerous regional varieties (sometimes called dialects), such as Brazilian and European Portuguese or Mainland and Taiwan Mandarin Chinese. Although such varieties are often mutually intelligible to their speakers, there are still important differences. For example, the Brazilian Portuguese word for “bus” is ônibus, while the European Portuguese word is autocarro. Yet, today’s machine translation (MT) systems typically do not allow users to specify which variety of a language to translate into. This may lead to confusion if the system outputs the “wrong” variety or mixes varieties in an unnatural way. Also, region-unaware MT systems tend to favor whichever v…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chatting with Claude]]></title>
        <id>http://windowsontheory.org/?p=8529</id>
        <link href="https://windowsontheory.org/2023/02/17/chatting-with-claude/"/>
        <updated>2023-02-17T16:04:27.000Z</updated>
        <summary type="html"><![CDATA[In my previous post I discussed how large language models can be thoughts of as the hero of the movie “memento” – their long-term memory is intact but they have limited context, which can be an issue in retrieving not just facts that happened after the training, but also the relevant facts that did appear … Continue reading Chatting with Claude]]></summary>
        <author>
            <name>Boaz Barak</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chatting with Claude]]></title>
        <id>http://windowsontheory.org/?p=8529</id>
        <link href="https://windowsontheory.org/2023/02/17/chatting-with-claude/"/>
        <updated>2023-02-17T16:04:27.000Z</updated>
        <summary type="html"><![CDATA[In my previous post I discussed how large language models can be thoughts of as the hero of the movie “memento” – their long-term memory is intact but they have limited context, which can be an issue in retrieving not just facts that happened after the training, but also the relevant facts that did appear … Continue reading Chatting with Claude]]></summary>
        <author>
            <name>Boaz Barak</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Batch Acquisition for Deep Bayesian Active Learning]]></title>
        <id>http://arxiv.org/abs/2301.05490</id>
        <link href="http://arxiv.org/abs/2301.05490"/>
        <updated>2023-02-17T07:15:20.289Z</updated>
        <summary type="html"><![CDATA[Aleksandr Rubashevskii, Daria Kotova, Maxim Panov]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Learning Theory Beyond Batch Binary Classification]]></title>
        <id>http://arxiv.org/abs/2302.07409</id>
        <link href="http://arxiv.org/abs/2302.07409"/>
        <updated>2023-02-17T07:15:20.246Z</updated>
        <summary type="html"><![CDATA[Preetham Mohan, Ambuj Tewari]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Deep Learning Methods for Multiview Learning]]></title>
        <id>http://arxiv.org/abs/2302.07930</id>
        <link href="http://arxiv.org/abs/2302.07930"/>
        <updated>2023-02-17T07:15:19.926Z</updated>
        <summary type="html"><![CDATA[Hengkang Wang, Han Lu, Ju Sun, Sandra E Safo]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aligning Language Models with Preferences through f-divergence
  Minimization]]></title>
        <id>http://arxiv.org/abs/2302.08215</id>
        <link href="http://arxiv.org/abs/2302.08215"/>
        <updated>2023-02-17T07:15:19.920Z</updated>
        <summary type="html"><![CDATA[Dongyoung Go, Tomasz Korbak, Germ\'an Kruszewski, Jos Rozen, Nahyeon
  Ryu, Marc Dymetman]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Python's multiprocessing performance problem]]></title>
        <id>https://pythonspeed.com/articles/faster-multiprocessing-pickle/</id>
        <link href="https://pythonspeed.com/articles/faster-multiprocessing-pickle/"/>
        <updated>2023-02-17T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Because Python has limited parallelism when using threads, using worker processes is a common way to take advantage of multiple CPU cores.
The multiprocessing module is built-in to the standard library, so it’s frequently used for this purpose.
But while multiple processes let you take advantage of multiple CPUs, moving data between processes can be very slow.
And that can reduce some of the performance benefits of using worker processes.
Let’s see:
Why processes can have performance problems that threads don’t.
A number of ways to work around or deal with this performance overhead.
A bad solution you don’t want to us.
Read more...]]></summary>
        <author>
            <name>Python⇒Speed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google新搜出的优化器Lion：效率与效果兼得的“训练狮”]]></title>
        <id>https://kexue.fm/archives/9473</id>
        <link href="https://kexue.fm/archives/9473"/>
        <updated>2023-02-16T14:38:00.000Z</updated>
        <summary type="html"><![CDATA[昨天在Arixv上发现了Google新发的一篇论文《Symbolic Discovery of Optimization Algorithms》，主要是讲自动搜索优化器的，咋看上去没啥意思，因为...]]></summary>
        <author>
            <name>苏剑林</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficiently Learning Neural Networks: What Assumptions May Suffice?]]></title>
        <id>http://arxiv.org/abs/2302.07426</id>
        <link href="http://arxiv.org/abs/2302.07426"/>
        <updated>2023-02-16T08:21:41.214Z</updated>
        <summary type="html"><![CDATA[Amit Daniely, Nathan Srebro, Gal Vardi]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatially heterogeneous learning by a deep student machine]]></title>
        <id>http://arxiv.org/abs/2302.07419</id>
        <link href="http://arxiv.org/abs/2302.07419"/>
        <updated>2023-02-16T07:15:32.227Z</updated>
        <summary type="html"><![CDATA[Hajime Yoshino]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cliff-Learning]]></title>
        <id>http://arxiv.org/abs/2302.07348</id>
        <link href="http://arxiv.org/abs/2302.07348"/>
        <updated>2023-02-16T07:15:32.187Z</updated>
        <summary type="html"><![CDATA[Tony T. Wang, Igor Zablotchi, Nir Shavit, Jonathan S. Rosenfeld]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Understanding Ensemble, Knowledge Distillation and
  Self-Distillation in Deep Learning]]></title>
        <id>http://arxiv.org/abs/2012.09816</id>
        <link href="http://arxiv.org/abs/2012.09816"/>
        <updated>2023-02-16T07:15:32.180Z</updated>
        <summary type="html"><![CDATA[Zeyuan Allen-Zhu, Yuanzhi Li]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Online Conformal Prediction via Strongly Adaptive Online
  Learning]]></title>
        <id>http://arxiv.org/abs/2302.07869</id>
        <link href="http://arxiv.org/abs/2302.07869"/>
        <updated>2023-02-16T07:15:32.107Z</updated>
        <summary type="html"><![CDATA[Aadyot Bhatnagar, Huan Wang, Caiming Xiong, Yu Bai]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous PDE Dynamics Forecasting with Implicit Neural Representations]]></title>
        <id>http://arxiv.org/abs/2209.14855</id>
        <link href="http://arxiv.org/abs/2209.14855"/>
        <updated>2023-02-16T07:15:32.079Z</updated>
        <summary type="html"><![CDATA[Yuan Yin, Matthieu Kirchmeyer, Jean-Yves Franceschi, Alain
  Rakotomamonjy, Patrick Gallinari]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FriendlyCore: A novel differentially private aggregation framework]]></title>
        <id>http://ai.googleblog.com/2023/02/friendlycore-novel-differentially.html</id>
        <link href="http://ai.googleblog.com/2023/02/friendlycore-novel-differentially.html"/>
        <updated>2023-02-15T18:52:00.002Z</updated>
        <summary type="html"><![CDATA[Posted by Haim Kaplan and Yishay Mansour, Research Scientists, Google Research   
Differential privacy (DP) machine learning algorithms protect user data by limiting the effect of each data point on an aggregated output with a mathematical guarantee. Intuitively the guarantee implies that changing a single user’s contribution should not significantly change the output distribution of the DP algorithm. 

However, DP algorithms tend to be less accurate than their non-private counterparts because satisfying DP is a worst-case requirement: one has to add noise to “hide” changes in any potential input point, including "unlikely points’’ that have a significant impact on the aggregation. For example, suppose we want to privately estimate the average of a dataset, and we know that a sphere of dia…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Bayesian optimization with high-dimensional outputs using
  randomized prior networks]]></title>
        <id>http://arxiv.org/abs/2302.07260</id>
        <link href="http://arxiv.org/abs/2302.07260"/>
        <updated>2023-02-15T07:15:22.508Z</updated>
        <summary type="html"><![CDATA[Mohamed Aziz Bhouri, Michael Joly, Robert Yu, Soumalya Sarkar
 , Paris Perdikaris]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-optimal learning with average H\"older smoothness]]></title>
        <id>http://arxiv.org/abs/2302.06005</id>
        <link href="http://arxiv.org/abs/2302.06005"/>
        <updated>2023-02-15T07:15:22.492Z</updated>
        <summary type="html"><![CDATA[Steve Hanneke, Aryeh Kontorovich, Guy Kornowski]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Easy Learning from Label Proportions]]></title>
        <id>http://arxiv.org/abs/2302.03115</id>
        <link href="http://arxiv.org/abs/2302.03115"/>
        <updated>2023-02-15T07:15:22.482Z</updated>
        <summary type="html"><![CDATA[Robert Istvan Busa-Fekete, Heejin Choi, Travis Dick, Claudio Gentile,
  Andres Munoz medina]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpolation Learning With Minimum Description Length]]></title>
        <id>http://arxiv.org/abs/2302.07263</id>
        <link href="http://arxiv.org/abs/2302.07263"/>
        <updated>2023-02-15T07:15:22.474Z</updated>
        <summary type="html"><![CDATA[Naren Sarayu Manoj, Nathan Srebro]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-stationary Contextual Bandits and Universal Learning]]></title>
        <id>http://arxiv.org/abs/2302.07186</id>
        <link href="http://arxiv.org/abs/2302.07186"/>
        <updated>2023-02-15T07:15:22.450Z</updated>
        <summary type="html"><![CDATA[Moise Blanchard, Steve Hanneke, Patrick Jaillet]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google Research, 2022 & beyond: Robotics]]></title>
        <id>http://ai.googleblog.com/2023/02/google-research-2022-beyond-robotics.html</id>
        <link href="http://ai.googleblog.com/2023/02/google-research-2022-beyond-robotics.html"/>
        <updated>2023-02-14T20:48:00.005Z</updated>
        <summary type="html"><![CDATA[Posted by Kendra Byrne, Senior Product Manager, and Jie Tan, Staff Research Scientist, Robotics at Google    
    

(This is Part 6 in our series of posts covering different topical areas of research at Google. You can find other posts in the series here.)
   

Within our lifetimes, we will see robotic technologies that can help with everyday activities, enhancing human productivity and quality of life. Before robotics can be broadly useful in helping with practical day-to-day tasks in people-centered spaces — spaces designed for people, not machines — they need to be able to safely & competently provide assistance to people. 
 In 2022, we focused on challenges that come with enabling robots to be more helpful to people: 1) allowing robots and humans to communicate more efficiently and nat…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Democratizing AI with PyTorch Foundation and ROCm™ support for PyTorch]]></title>
        <id>https://pytorch.org/blog/democratizing-ai-with-pytorch/</id>
        <link href="https://pytorch.org/blog/democratizing-ai-with-pytorch/"/>
        <updated>2023-02-14T08:00:00.000Z</updated>
        <summary type="html"><![CDATA[<div type="html"/>]]></summary>
        <author>
            <name>PyTorch Website</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Absolutely Sensational Morning News – Zander Kelley and Raghua Meka proved Behrend-type bounds for 3APs]]></title>
        <id>http://gilkalai.wordpress.com/?p=23866</id>
        <link href="https://gilkalai.wordpress.com/2023/02/14/absolutely-sensational-morning-news-zander-kelley-and-raghua-meka-proved-behrend-type-bounds-for-3aps/"/>
        <updated>2023-02-14T07:42:46.000Z</updated>
        <summary type="html"><![CDATA[What is the density of a subset of that guarantees that contains a 3-term arithmetic progression? And, more generally, if the density of is what is the minimum number of 3-terms AP that contains? These problems and the more general … Continue reading →]]></summary>
        <author>
            <name>Gil Kalai</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Absolutely Sensational Morning News – Zander Kelley and Raghua Meka proved Behrend-type bounds for 3APs]]></title>
        <id>http://gilkalai.wordpress.com/?p=23866</id>
        <link href="https://gilkalai.wordpress.com/2023/02/14/absolutely-sensational-morning-news-zander-kelley-and-raghua-meka-proved-behrend-type-bounds-for-3aps/"/>
        <updated>2023-02-14T07:42:46.000Z</updated>
        <summary type="html"><![CDATA[What is the density of a subset of that guarantees that contains a 3-term arithmetic progression? And, more generally, if the density of is what is the minimum number of 3-terms AP that contains? These problems and the more general … Continue reading →]]></summary>
        <author>
            <name>Gil Kalai</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[生成扩散模型漫谈（十六）：W距离 ≤ 得分匹配]]></title>
        <id>https://kexue.fm/archives/9467</id>
        <link href="https://kexue.fm/archives/9467"/>
        <updated>2023-02-14T07:18:00.000Z</updated>
        <summary type="html"><![CDATA[Wasserstein距离（下面简称“W距离”），是基于最优传输思想来度量两个概率分布差异程度的距离函数，笔者之前在《从Wasserstein距离、对偶理论到WGAN》等博文中也做过介绍。对于很...]]></summary>
        <author>
            <name>苏剑林</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DAG Learning on the Permutahedron]]></title>
        <id>http://arxiv.org/abs/2301.11898</id>
        <link href="http://arxiv.org/abs/2301.11898"/>
        <updated>2023-02-14T07:15:42.457Z</updated>
        <summary type="html"><![CDATA[Valentina Zantedeschi, Luca Franceschi, Jean Kaddour, Matt J. Kusner,
  Vlad Niculae]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Bayesian Neural Networks via Resolution of Singularities]]></title>
        <id>http://arxiv.org/abs/2302.06035</id>
        <link href="http://arxiv.org/abs/2302.06035"/>
        <updated>2023-02-14T07:15:42.449Z</updated>
        <summary type="html"><![CDATA[Susan Wei, Edmund Lau]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Network-Inspired Kernels for Gaussian Processes in
  Semi-Supervised Learning]]></title>
        <id>http://arxiv.org/abs/2302.05828</id>
        <link href="http://arxiv.org/abs/2302.05828"/>
        <updated>2023-02-14T07:15:42.429Z</updated>
        <summary type="html"><![CDATA[Zehao Niu, Mihai Anitescu, Jie Chen]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Rigorous Framework for the Mean Field Limit of Multilayer Neural
  Networks]]></title>
        <id>http://arxiv.org/abs/2001.11443</id>
        <link href="http://arxiv.org/abs/2001.11443"/>
        <updated>2023-02-14T07:15:42.400Z</updated>
        <summary type="html"><![CDATA[Phan-Minh Nguyen, Huy Tuan Pham]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google’s Fully Homomorphic Encryption Compiler — A Primer]]></title>
        <id>https://jeremykun.com/?p=119079</id>
        <link href="https://jeremykun.com/2023/02/13/googles-fully-homomorphic-encryption-compiler-a-primer/"/>
        <updated>2023-02-13T19:34:01.000Z</updated>
        <summary type="html"><![CDATA[Back in May of 2022 I transferred teams at Google to work on Fully Homomorphic Encryption (newsletter announcement). Since then I’ve been working on a variety of projects in the space, including being the primary maintainer on github.com/google/fully-homomorphic-encryption, which is an open source FHE compiler for C++. This article will be an introduction to how […]]]></summary>
        <author>
            <name>j2kun</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Resolution Peak Demand Estimation Using Generalized Additive Models
  and Deep Neural Networks]]></title>
        <id>http://arxiv.org/abs/2203.03342</id>
        <link href="http://arxiv.org/abs/2203.03342"/>
        <updated>2023-02-13T07:15:35.979Z</updated>
        <summary type="html"><![CDATA[Jonathan Berrisch, Micha{\l} Narajewski, Florian Ziel]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MoreauGrad: Sparse and Robust Interpretation of Neural Networks via
  Moreau Envelope]]></title>
        <id>http://arxiv.org/abs/2302.05294</id>
        <link href="http://arxiv.org/abs/2302.05294"/>
        <updated>2023-02-13T07:15:35.565Z</updated>
        <summary type="html"><![CDATA[Jingwei Zhang, Farzan Farnia]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interventional Causal Representation Learning]]></title>
        <id>http://arxiv.org/abs/2209.11924</id>
        <link href="http://arxiv.org/abs/2209.11924"/>
        <updated>2023-02-13T07:15:35.551Z</updated>
        <summary type="html"><![CDATA[Kartik Ahuja, Divyat Mahajan, Yixin Wang, Yoshua Bengio]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DNArch: Learning Convolutional Neural Architectures by Backpropagation]]></title>
        <id>http://arxiv.org/abs/2302.05400</id>
        <link href="http://arxiv.org/abs/2302.05400"/>
        <updated>2023-02-13T07:15:35.538Z</updated>
        <summary type="html"><![CDATA[David W. Romero, Neil Zeghidour]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[测试函数法推导连续性方程和Fokker-Planck方程]]></title>
        <id>https://kexue.fm/archives/9461</id>
        <link href="https://kexue.fm/archives/9461"/>
        <updated>2023-02-11T11:09:00.000Z</updated>
        <summary type="html"><![CDATA[在文章《生成扩散模型漫谈（六）：一般框架之ODE篇》中，我们推导了SDE的Fokker-Planck方程；而在《生成扩散模型漫谈（十二）：“硬刚”扩散ODE》中，我们单独推导了ODE的连续性方程...]]></summary>
        <author>
            <name>苏剑林</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reportage Illustration and Travel Sketch]]></title>
        <id>http://freemind.pluskid.org/books/reportage-illustration-and-travel-sketch</id>
        <link href="http://freemind.pluskid.org/books/reportage-illustration-and-travel-sketch"/>
        <updated>2023-02-11T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[A pen and pencil and paper is nothing if you have nothing to say.
Otherwise do pretty watercolors and forget it.
《Reportage
Illustration: Visual Journalism》是一本介绍 Reportage Illustration
的书，Reportage Illustration
按照字面意思翻译大概应该叫做“新闻插画”，但也许称作“新闻速写”更恰当一些，它可以归类到
Visual
Journalism
下面，基本上是指新闻或者艺术工作人员通过现场作画和记录来报道某一个具体的事件。我最初了解到这本书以及这个行业是在公司的艺术兴趣班的一个叫做
Visual Journalism
的课上，老师带着大家一起去各种不同的场所，例如音乐会、咖啡店的脱口秀表演、户外甚至是一起观看某个著名的视频演讲等，然后让大家通过速写和文字结合的方式对现场体验进行描绘和记录，关键并不是要画出多么漂亮的画，而是要通过速写的方式记录下当时的氛围、声音、色彩、对话、天气等各种感官体验。
整体而言还挺好玩的，虽然和照片甚至录音录像相比通过速写能记录下来的信息量极少，但正因为能记录的内容有限，我们需要舍弃大量东西，反而导致我们记录中的内容是在现场令我们印象最深刻的体验（当然要做好这一点也是需要经过许多训练的）。所以再回去看自己当时的涂鸦时可能更容易重现现场的感触和记忆。然而这针对“个人”而言很合理，但是如果读者不是未来的自己，而是作为真正的新闻报道那样的形式给呈现给他人的话，看到杂乱的涂鸦和速记时能够重现的氛围体验有多少就很难讲了。所以我读这本书的一个疑问其实是这个行业究竟是如何存在的。
The Times for the 2012 London
Olympics.



…]]></summary>
        <author>
            <name>Free Mind</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google Research, 2022 & beyond: Algorithmic advances]]></title>
        <id>http://ai.googleblog.com/2023/02/google-research-2022-beyond-algorithmic.html</id>
        <link href="http://ai.googleblog.com/2023/02/google-research-2022-beyond-algorithmic.html"/>
        <updated>2023-02-10T20:08:00.022Z</updated>
        <summary type="html"><![CDATA[Posted by Vahab Mirrokni, VP and Google Fellow, Google Research    
    

(This is Part 5 in our series of posts covering different topical areas of research at Google. You can find other posts in the series here.)
   

Robust algorithm design is the backbone of systems across Google, particularly for our ML and AI models. Hence, developing algorithms with improved efficiency, performance and speed remains a high priority as it empowers services ranging from Search and Ads to Maps and YouTube. Google Research has been at the forefront of this effort, developing many innovations from privacy-safe recommendation systems to scalable solutions for large-scale ML. In 2022, we continued this journey, and advanced the state-of-the-art in several related areas. Here we highlight our progress in a …]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Roger Grosse - Studying Neural Net Generalization through Influence Functions]]></title>
        <id>https://mlfoundations.org/talk/grosse/</id>
        <link href="https://mlfoundations.org/talk/grosse/"/>
        <updated>2023-02-10T14:00:00.000Z</updated>
        <summary type="html"><![CDATA[How can we trace surprising behaviors of machine learning models back to their training data? Influence functions aim to predict how the trained model would change if a specific training example were added to the training set. I'll address two issues that have blocked their applicability to large-scale neural nets: apparent inaccuracy of the results, and the difficulty of computing inverse-Hessian-vector products. Towards the former issue, I'll reformulate the goals of influence estimation in a way that applies to overparameterized, incompletely trained models, and argue that the apparent inaccuracy was largely illusory. I'll then discuss an approach to scaling influence estimation to large language models and show some resulting insights into their patterns of generalization.]]></summary>
        <author>
            <name>Harvard ML Foundations</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on
  Graph Diffusion]]></title>
        <id>http://arxiv.org/abs/2302.04451</id>
        <link href="http://arxiv.org/abs/2302.04451"/>
        <updated>2023-02-10T07:15:38.755Z</updated>
        <summary type="html"><![CDATA[Haotian Ju, Dongyue Li, Aneesh Sharma, Hongyang R. Zhang]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering the Compositional Structure of Vector Representations with
  Role Learning Networks]]></title>
        <id>http://arxiv.org/abs/1910.09113</id>
        <link href="http://arxiv.org/abs/1910.09113"/>
        <updated>2023-02-10T07:15:38.665Z</updated>
        <summary type="html"><![CDATA[Paul Soulos, Tom McCoy, Tal Linzen, Paul Smolensky]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-ticket: Finding optimal subnetworks for few-shot learning within
  randomly initialized neural networks]]></title>
        <id>http://arxiv.org/abs/2205.15619</id>
        <link href="http://arxiv.org/abs/2205.15619"/>
        <updated>2023-02-10T07:15:38.657Z</updated>
        <summary type="html"><![CDATA[Daiki Chijiwa, Shin'ya Yamaguchi, Atsutoshi Kumagai, Yasutoshi Ida]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Benchmark on Uncertainty Quantification for Deep Learning Prognostics]]></title>
        <id>http://arxiv.org/abs/2302.04730</id>
        <link href="http://arxiv.org/abs/2302.04730"/>
        <updated>2023-02-10T07:15:38.647Z</updated>
        <summary type="html"><![CDATA[Luis Basora, Arthur Viens, Manuel Arias Chao, Xavier Olive]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DT 包速查手册]]></title>
        <id>https://cosx.org/2023/02/dt-manual/</id>
        <link href="https://cosx.org/2023/02/dt-manual/"/>
        <updated>2023-02-10T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[1 静态样式 1.1 基本说明 1.1.1 术语约定 1.1.2 参数位置 1.1.3 回调函数 1.2 表格基础 1.2.1 高度（height）、宽度（width） 1.2.2 行名（rownames） 1.2.3 列名（co]]></summary>
        <author>
            <name>统计之都</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amplification at the quantum limit]]></title>
        <id>http://ai.googleblog.com/2023/02/amplification-at-quantum-limit.html</id>
        <link href="http://ai.googleblog.com/2023/02/amplification-at-quantum-limit.html"/>
        <updated>2023-02-09T19:03:00.008Z</updated>
        <summary type="html"><![CDATA[Posted by Ted White and Ofer Naaman, Staff Research Scientists, Google Quantum AI   
The Google Quantum AI team is building quantum computers with superconducting microwave circuits, but much like a classical computer the superconducting processor at the heart of these computers is only part of the story. An entire technology stack of peripheral hardware is required to make the quantum computer work properly. In many cases these parts must be custom designed, requiring extensive research and development to reach the highest levels of performance.   

In this post, we highlight one aspect of this supplemental hardware: our superconducting microwave amplifiers. In “Readout of a Quantum Processor with High Dynamic Range Josephson Parametric Amplifiers”, published in Applied Physics Letters, w…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Modern Mathematics of Deep Learning]]></title>
        <id>http://arxiv.org/abs/2105.04026</id>
        <link href="http://arxiv.org/abs/2105.04026"/>
        <updated>2023-02-09T07:15:17.534Z</updated>
        <summary type="html"><![CDATA[Julius Berner, Philipp Grohs, Gitta Kutyniok, Philipp Petersen]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boundary Graph Neural Networks for 3D Simulations]]></title>
        <id>http://arxiv.org/abs/2106.11299</id>
        <link href="http://arxiv.org/abs/2106.11299"/>
        <updated>2023-02-09T07:15:17.420Z</updated>
        <summary type="html"><![CDATA[Andreas Mayr, Sebastian Lehner, Arno Mayrhofer, Christoph Kloss, Sepp
  Hochreiter, Johannes Brandstetter]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fortuna: A Library for Uncertainty Quantification in Deep Learning]]></title>
        <id>http://arxiv.org/abs/2302.04019</id>
        <link href="http://arxiv.org/abs/2302.04019"/>
        <updated>2023-02-09T07:15:17.394Z</updated>
        <summary type="html"><![CDATA[Gianluca Detommaso, Alberto Gasparin, Michele Donini, Matthias Seeger,
  Andrew Gordon Wilson, Cedric Archambeau]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TVAE: Triplet-Based Variational Autoencoder using Metric Learning]]></title>
        <id>http://arxiv.org/abs/1802.04403</id>
        <link href="http://arxiv.org/abs/1802.04403"/>
        <updated>2023-02-09T07:15:17.315Z</updated>
        <summary type="html"><![CDATA[Haque Ishfaq, Assaf Hoogi, Daniel Rubin]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Inferential Reproducibility of Machine Learning Research]]></title>
        <id>http://arxiv.org/abs/2302.04054</id>
        <link href="http://arxiv.org/abs/2302.04054"/>
        <updated>2023-02-09T07:15:17.308Z</updated>
        <summary type="html"><![CDATA[Michael Hagmann, Stefan Riezler]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised and semi-supervised anomaly detection with data-centric ML]]></title>
        <id>http://ai.googleblog.com/2023/02/unsupervised-and-semi-supervised.html</id>
        <link href="http://ai.googleblog.com/2023/02/unsupervised-and-semi-supervised.html"/>
        <updated>2023-02-08T18:41:00.003Z</updated>
        <summary type="html"><![CDATA[Posted by Jinsung Yoon and Sercan O. Arik, Research Scientists, Google Research, Cloud AI Team  
Anomaly detection (AD), the task of distinguishing anomalies from normal data, plays a vital role in many real-world applications, such as detecting faulty products from vision sensors in manufacturing, fraudulent behaviors in financial transactions, or network security threats. Depending on the availability of the type of data — negative (normal) vs. positive (anomalous) and the availability of their labels — the task of AD involves different challenges. 
  


(a) Fully supervised anomaly detection, (b) normal-only anomaly detection, (c, d, e) semi-supervised anomaly detection, (f) unsupervised anomaly detection.

  
While most previous works were shown to be effective for cases with fully-lab…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[17/05 1pm-2pm: CRiSM Seminar]]></title>
        <id>8a17841b862bed270186308c0ac02065</id>
        <link href="https://warwick.ac.uk/fac/sci/statistics/event_diary/?calendarItem=8a17841b862bed270186308c0ac02064"/>
        <updated>2023-02-08T10:22:09.000Z</updated>
        <summary type="html"><![CDATA[When:
	
  		13:00
		-
		14:00, Wed, 17 May '23

	
Where: MB0.07]]></summary>
        <author>
            <name>Statistics</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Self-Supervised Learning via Risk Decomposition]]></title>
        <id>http://arxiv.org/abs/2302.03068</id>
        <link href="http://arxiv.org/abs/2302.03068"/>
        <updated>2023-02-08T07:15:18.338Z</updated>
        <summary type="html"><![CDATA[Yann Dubois, Tatsunori Hashimoto, Percy Liang]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep-OSG: A deep learning approach for approximating a family of
  operators in semigroup to model unknown autonomous systems]]></title>
        <id>http://arxiv.org/abs/2302.03358</id>
        <link href="http://arxiv.org/abs/2302.03358"/>
        <updated>2023-02-08T07:15:18.328Z</updated>
        <summary type="html"><![CDATA[Junfeng Chen, Kailiang Wu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the relationship between multivariate splines and infinitely-wide
  neural networks]]></title>
        <id>http://arxiv.org/abs/2302.03459</id>
        <link href="http://arxiv.org/abs/2302.03459"/>
        <updated>2023-02-08T07:15:18.317Z</updated>
        <summary type="html"><![CDATA[Francis Bach (SIERRA)]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory-Based Meta-Learning on Non-Stationary Distributions]]></title>
        <id>http://arxiv.org/abs/2302.03067</id>
        <link href="http://arxiv.org/abs/2302.03067"/>
        <updated>2023-02-08T07:15:18.168Z</updated>
        <summary type="html"><![CDATA[Tim Genewein, Gr\'egoire Del\'etang, Anian Ruoss, Li Kevin Wenliang,
  Elliot Catt, Vincent Dutordoir, Jordi Grau-Moya, Laurent Orseau, Marcus
  Hutter, Joel Veness]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Linear Networks can Benignly Overfit when Shallow Ones Do]]></title>
        <id>http://arxiv.org/abs/2209.09315</id>
        <link href="http://arxiv.org/abs/2209.09315"/>
        <updated>2023-02-08T07:15:18.156Z</updated>
        <summary type="html"><![CDATA[Niladri S. Chatterji, Philip M. Long]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google Research, 2022 & beyond: Algorithms for efficient deep learning]]></title>
        <id>http://ai.googleblog.com/2023/02/google-research-2022-beyond-algorithms.html</id>
        <link href="http://ai.googleblog.com/2023/02/google-research-2022-beyond-algorithms.html"/>
        <updated>2023-02-07T19:42:00.020Z</updated>
        <summary type="html"><![CDATA[Posted by Sanjiv Kumar, VP and Google Fellow, Google Research    
    

(This is Part 4 in our series of posts covering different topical areas of research at Google. You can find other posts in the series here.)
   
The explosion in deep learning a decade ago was catapulted in part by the convergence of new algorithms and architectures, a marked increase in data, and access to greater compute. In the last 10 years, AI and ML models have become bigger and more sophisticated — they’re deeper, more complex, with more parameters, and trained on much more data, resulting in some of the most transformative outcomes in the history of machine learning.  
 As these models increasingly find themselves deployed in production and business applications, the efficiency and costs of these models has gon…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-screening breast cancer with machine learning and deep learning]]></title>
        <id>http://arxiv.org/abs/2302.02406</id>
        <link href="http://arxiv.org/abs/2302.02406"/>
        <updated>2023-02-07T07:14:35.000Z</updated>
        <summary type="html"><![CDATA[Rolando Gonzales Martinez, Daan-Max van Dongen]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On a continuous time model of gradient descent dynamics and instability
  in deep learning]]></title>
        <id>http://arxiv.org/abs/2302.01952</id>
        <link href="http://arxiv.org/abs/2302.01952"/>
        <updated>2023-02-07T07:14:34.986Z</updated>
        <summary type="html"><![CDATA[Mihaela Rosca, Yan Wu, Chongli Qin, Benoit Dherin]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Fine-Tuning of Deep Neural Networks with Hessian-based
  Generalization Guarantees]]></title>
        <id>http://arxiv.org/abs/2206.02659</id>
        <link href="http://arxiv.org/abs/2206.02659"/>
        <updated>2023-02-07T07:14:34.944Z</updated>
        <summary type="html"><![CDATA[Haotian Ju, Dongyue Li, Hongyang R. Zhang]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Variational Models with Unrolling and Bilevel Optimization]]></title>
        <id>http://arxiv.org/abs/2209.12651</id>
        <link href="http://arxiv.org/abs/2209.12651"/>
        <updated>2023-02-07T07:14:34.934Z</updated>
        <summary type="html"><![CDATA[Christoph Brauer, Niklas Breustedt, Timo de Wolff, Dirk A. Lorenz]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consistent Range Approximation for Fair Predictive Modeling]]></title>
        <id>http://arxiv.org/abs/2212.10839</id>
        <link href="http://arxiv.org/abs/2212.10839"/>
        <updated>2023-02-06T07:14:56.460Z</updated>
        <summary type="html"><![CDATA[Jiongli Zhu, Sainyam Galhotra, Nazanin Sabri, Babak Salimi]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Explainability to Inform Statistical Downscaling Based on Deep
  Learning Beyond Standard Validation Approaches]]></title>
        <id>http://arxiv.org/abs/2302.01771</id>
        <link href="http://arxiv.org/abs/2302.01771"/>
        <updated>2023-02-06T07:14:56.443Z</updated>
        <summary type="html"><![CDATA[Jose Gonz\'alez-Abad, Jorge Ba\~no-Medina, Jos\'e Manuel Guti\'errez]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Representativity for Machine Learning and AI Systems]]></title>
        <id>http://arxiv.org/abs/2203.04706</id>
        <link href="http://arxiv.org/abs/2203.04706"/>
        <updated>2023-02-06T07:14:56.424Z</updated>
        <summary type="html"><![CDATA[Line H. Clemmensen, Rune D. Kj{\ae}rsgaard]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hypothesis Testing and Machine Learning: Interpreting Variable Effects
  in Deep Artificial Neural Networks using Cohen's f2]]></title>
        <id>http://arxiv.org/abs/2302.01407</id>
        <link href="http://arxiv.org/abs/2302.01407"/>
        <updated>2023-02-06T07:14:56.419Z</updated>
        <summary type="html"><![CDATA[Wolfgang Messner]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time tracking of wildfire boundaries using satellite imagery]]></title>
        <id>http://ai.googleblog.com/2023/02/real-time-tracking-of-wildfire.html</id>
        <link href="http://ai.googleblog.com/2023/02/real-time-tracking-of-wildfire.html"/>
        <updated>2023-02-03T18:05:00.012Z</updated>
        <summary type="html"><![CDATA[Posted by Zvika Ben-Haim and Omer Nevo, Software Engineers, Google Research   
   As global temperatures rise, wildfires around the world are becoming more frequent and more dangerous. Their effects are felt by many communities as people evacuate their homes or suffer harm even from proximity to the fire and smoke.  

As part of Google’s mission to help people access trusted information in critical moments, we use satellite imagery and machine learning (ML) to track wildfires and inform affected communities. Our wildfire tracker was recently expanded. It provides updated fire boundary information every 10–15 minutes, is more accurate than similar satellite products, and improves on our previous work. These boundaries are shown for large fires in the continental US, Mexico, and most of Cana…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Matthieu Wyart - Data structure and curse of dimensionality in deep learning]]></title>
        <id>https://mlfoundations.org/talk/wyart/</id>
        <link href="https://mlfoundations.org/talk/wyart/"/>
        <updated>2023-02-03T14:00:00.000Z</updated>
        <summary type="html"><![CDATA[Deep learning algorithms are responsible for a technological revolution in a variety of tasks, yet understanding why they work remains a challenge.  A particularly puzzling fact is their ability to learn high-dimensional tasks. Due to the curse of dimensionality (the fact that accurate sampling cannot be achieved in high dimension), this should be generically impossible.  Learnable tasks (such as classifying images) must present a lot of structure, whose nature is debated. I will discuss three properties of data plausibly connected to their learnability: locality, sparsity, and their hierarchical/combinatorial aspect, both from  empirical and simple model viewpoints. I will discuss the importance of distinguishing  different training regimes, when a representation of the data is learnt, or not.]]></summary>
        <author>
            <name>Harvard ML Foundations</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity of Kernel-Based Q-Learning]]></title>
        <id>http://arxiv.org/abs/2302.00727</id>
        <link href="http://arxiv.org/abs/2302.00727"/>
        <updated>2023-02-03T07:14:54.848Z</updated>
        <summary type="html"><![CDATA[Sing-Yuan Yeh, Fu-Chieh Chang, Chang-Wei Yueh, Pei-Yuan Wu, Alberto
  Bernacchia, Sattar Vakili]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sharp Lower Bounds on Interpolation by Deep ReLU Neural Networks at
  Irregularly Spaced Data]]></title>
        <id>http://arxiv.org/abs/2302.00834</id>
        <link href="http://arxiv.org/abs/2302.00834"/>
        <updated>2023-02-03T07:14:54.796Z</updated>
        <summary type="html"><![CDATA[Jonathan W. Siegel]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lower Bounds for Learning in Revealing POMDPs]]></title>
        <id>http://arxiv.org/abs/2302.01333</id>
        <link href="http://arxiv.org/abs/2302.01333"/>
        <updated>2023-02-03T07:14:54.790Z</updated>
        <summary type="html"><![CDATA[Fan Chen, Huan Wang, Caiming Xiong, Song Mei, Yu Bai]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-scale Stochastic Optimization of NDCG Surrogates for Deep Learning
  with Provable Convergence]]></title>
        <id>http://arxiv.org/abs/2202.12183</id>
        <link href="http://arxiv.org/abs/2202.12183"/>
        <updated>2023-02-03T07:14:54.688Z</updated>
        <summary type="html"><![CDATA[Zi-Hao Qiu, Quanqi Hu, Yongjian Zhong, Lijun Zhang, Tianbao Yang]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Over-parameterised Shallow Neural Networks with Asymmetrical Node
  Scaling: Global Convergence Guarantees and Feature Learning]]></title>
        <id>http://arxiv.org/abs/2302.01002</id>
        <link href="http://arxiv.org/abs/2302.01002"/>
        <updated>2023-02-03T07:14:54.681Z</updated>
        <summary type="html"><![CDATA[Francois Caron, Fadhel Ayed, Paul Jung, Hoil Lee, Juho Lee, Hongseok
  Yang]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google Research, 2022 & beyond: ML & computer systems]]></title>
        <id>http://ai.googleblog.com/2023/02/google-research-2022-beyond-ml-computer.html</id>
        <link href="http://ai.googleblog.com/2023/02/google-research-2022-beyond-ml-computer.html"/>
        <updated>2023-02-02T21:13:00.008Z</updated>
        <summary type="html"><![CDATA[Posted by Phitchaya Mangpo Phothilimthana, Staff Research Scientist, and Adam Paszke, Staff Research Scientist, Google Research    
    

(This is Part 3 in our series of posts covering different topical areas of research at Google. You can find other posts in the series here.)
   
  Great machine learning (ML) research requires great systems. With the increasing sophistication of the algorithms and hardware in use today and with the scale at which they run, the complexity of the software necessary to carry out day-to-day tasks only increases. In this post, we provide an overview of the numerous advances made across Google this past year in systems for ML that enable us to support the serving and training of complex models while easing the complexity of implementation for end users. This b…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open Source Vizier: Towards reliable and flexible hyperparameter and blackbox optimization]]></title>
        <id>http://ai.googleblog.com/2023/02/open-source-vizier-towards-reliable-and.html</id>
        <link href="http://ai.googleblog.com/2023/02/open-source-vizier-towards-reliable-and.html"/>
        <updated>2023-02-02T17:56:00.002Z</updated>
        <summary type="html"><![CDATA[Posted by Xingyou (Richard) Song, Research Scientist, and Chansoo Lee, Software Engineer, Google Research, Brain Team  
Google Vizier is the de-facto system for blackbox optimization over objective functions and hyperparameters across Google, having serviced some of Google’s largest research efforts and optimized a wide range of products (e.g., Search, Ads, YouTube). For research, it has not only reduced language model latency for users, designed computer architectures, accelerated hardware, assisted protein discovery, and enhanced robotics, but also provided a reliable backend interface for users to search for neural architectures and evolve reinforcement learning algorithms. To operate at the scale of optimizing thousands of users’ critical systems and tuning millions of machine learning…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deprecation of CUDA 11.6 and Python 3.7 Support]]></title>
        <id>https://pytorch.org/blog/deprecation-cuda-python-support/</id>
        <link href="https://pytorch.org/blog/deprecation-cuda-python-support/"/>
        <updated>2023-02-02T08:00:00.000Z</updated>
        <summary type="html"><![CDATA[For the upcoming PyTorch 2.0 feature release (target March 2023), we will target CUDA 11.7 as the stable version and CUDA 11.8 as the experimental version of CUDA and Python &gt;=3.8, &lt;=3.11.]]></summary>
        <author>
            <name>PyTorch Website</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Whats Missing? Learning Hidden Markov Models When the Locations of
  Missing Observations are Unknown]]></title>
        <id>http://arxiv.org/abs/2203.06527</id>
        <link href="http://arxiv.org/abs/2203.06527"/>
        <updated>2023-02-02T07:14:33.123Z</updated>
        <summary type="html"><![CDATA[Binyamin Perets, Mark Kozdoba, Shie Mannor]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Learning of Deep Random Networks of Extensive-width]]></title>
        <id>http://arxiv.org/abs/2302.00375</id>
        <link href="http://arxiv.org/abs/2302.00375"/>
        <updated>2023-02-02T07:14:32.995Z</updated>
        <summary type="html"><![CDATA[Hugo Cui, Florent Krzakala, Lenka Zdeborov\'a]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Descent in Neural Networks as Sequential Learning in RKBS]]></title>
        <id>http://arxiv.org/abs/2302.00205</id>
        <link href="http://arxiv.org/abs/2302.00205"/>
        <updated>2023-02-02T07:14:32.969Z</updated>
        <summary type="html"><![CDATA[Alistair Shilton, Sunil Gupta, Santu Rana, Svetha Venkatesh]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum machine learning beyond kernel methods]]></title>
        <id>http://arxiv.org/abs/2110.13162</id>
        <link href="http://arxiv.org/abs/2110.13162"/>
        <updated>2023-02-02T07:14:32.960Z</updated>
        <summary type="html"><![CDATA[Sofiene Jerbi, Lukas J. Fiderer, Hendrik Poulsen Nautrup, Jonas M.
  K\"ubler, Hans J. Briegel, Vedran Dunjko]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning for $\psi$-weakly dependent processes]]></title>
        <id>http://arxiv.org/abs/2302.00333</id>
        <link href="http://arxiv.org/abs/2302.00333"/>
        <updated>2023-02-02T07:14:32.936Z</updated>
        <summary type="html"><![CDATA[William Kengne, Wade Modou]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Flan Collection: Advancing open source methods for instruction tuning]]></title>
        <id>http://ai.googleblog.com/2023/02/the-flan-collection-advancing-open.html</id>
        <link href="http://ai.googleblog.com/2023/02/the-flan-collection-advancing-open.html"/>
        <updated>2023-02-01T18:16:00.003Z</updated>
        <summary type="html"><![CDATA[Posted by Shayne Longpre, Student Researcher, and Adam Roberts, Senior Staff Software Engineer, Google Research, Brain Team  
Language models are now capable of performing many new natural language processing (NLP) tasks by reading instructions, often that they hadn’t seen before. The ability to reason on new tasks is mostly credited to training models on a wide variety of unique instructions, known as “instruction tuning”, which was introduced by FLAN and extended in T0, Super-Natural Instructions, MetaICL, and InstructGPT. However, much of the data that drives these advances remain unreleased to the broader research community. 
    
In “The Flan Collection: Designing Data and Methods for Effective Instruction Tuning”, we closely examine and release a newer and more extensive publicly ava…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Learning for Neural Networks: an algorithmic survey]]></title>
        <id>http://arxiv.org/abs/2211.11865</id>
        <link href="http://arxiv.org/abs/2211.11865"/>
        <updated>2023-02-01T07:15:14.907Z</updated>
        <summary type="html"><![CDATA[Martin Magris, Alexandros Iosifidis]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Causal View of Domain Invariant Representation Learning]]></title>
        <id>http://arxiv.org/abs/2208.06987</id>
        <link href="http://arxiv.org/abs/2208.06987"/>
        <updated>2023-02-01T07:15:14.899Z</updated>
        <summary type="html"><![CDATA[Zihao Wang, Victor Veitch]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An $l_1$-oracle inequality for the Lasso in high-dimensional mixtures of
  experts models]]></title>
        <id>http://arxiv.org/abs/2009.10622</id>
        <link href="http://arxiv.org/abs/2009.10622"/>
        <updated>2023-02-01T07:15:14.893Z</updated>
        <summary type="html"><![CDATA[TrungTin Nguyen, Hien D Nguyen, Faicel Chamroukhi, Geoffrey J
  McLachlan]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning in POMDPs is Sample-Efficient with Hindsight Observability]]></title>
        <id>http://arxiv.org/abs/2301.13857</id>
        <link href="http://arxiv.org/abs/2301.13857"/>
        <updated>2023-02-01T07:15:14.883Z</updated>
        <summary type="html"><![CDATA[Jonathan N. Lee, Alekh Agarwal, Christoph Dann, Tong Zhang]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-shot-Learning Cross-Modality Data Translation Through Mutual
  Information Guided Stochastic Diffusion]]></title>
        <id>http://arxiv.org/abs/2301.13743</id>
        <link href="http://arxiv.org/abs/2301.13743"/>
        <updated>2023-02-01T07:15:14.766Z</updated>
        <summary type="html"><![CDATA[Zihao Wang, Yingyu Yang, Maxime Sermesant, Herv\'e Delingette, Ona Wu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
</feed>