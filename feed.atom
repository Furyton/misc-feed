<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://Furyton.github.io/misc-feed/index.html</id>
    <title>osmos::feed</title>
    <updated>2023-09-01T23:13:45.284Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="https://Furyton.github.io/misc-feed/index.html"/>
    <link rel="self" href="https://Furyton.github.io/misc-feed/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[Quantum-enhanced Markov chain Monte Carlo]]></title>
        <id>http://xianblog.wordpress.com/?p=53575</id>
        <link href="https://xianblog.wordpress.com/2023/09/02/quantum-enhanced-markov-chain-monte-carlo/"/>
        <updated>2023-09-01T22:23:51.000Z</updated>
        <summary type="html"><![CDATA[A rare occurrence of an MCMC paper in Nature!!! David Layden and co-authors published this paper on 12 July, about using a quantum proposal in a Metropolis-Rosenbluth-Hastings simulation of an Ising model. More specifically, based on “quenched dynamics of a transverse-field quantum Ising model20, which can be efficiently simulated on a quantum computer21“, which amounts […]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Central Asian Turkish languages]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60479</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60479&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=central-asian-turkish-languages"/>
        <updated>2023-09-01T12:33:21.000Z</updated>
        <summary type="html"><![CDATA[I write to announce an exceptional opportunity to learn the Turkish languages of Central Asia. There is a new Turkic course at Penn: TURK 1050.   This is a survey course that introduces students to the main languages spoken in Central Asia: Kazakh, Uzbek, Kyrgyz, Azeri. The language study will facilitate student research in topics like […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Don't speak Japanese loudly outside!"]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60476</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60476&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=dont-speak-japanese-loudly-outside"/>
        <updated>2023-09-01T01:29:55.000Z</updated>
        <summary type="html"><![CDATA[Advisory to staff of the embassy of Japan in Beijing: ALPS shori mizu no kaiyō hōshutsu kaishi ni tomonau chūi kanki (2023-nen 8 tsuki 25-nichi) ALPS処理水の海洋放出開始に伴う注意喚起（2023年8月25日） Warning regarding the start of ocean discharge of ALPS-treated water (August 25, 2023) Kinō (24-nichi), fusoku no jitai ga hassei suru kanōsei wa haijo dekinai tame chūi shite itadaku […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Open MatSci ML Toolkit: A Flexible Framework for Machine Learning in
  Materials Science]]></title>
        <id>http://arxiv.org/abs/2210.17484</id>
        <link href="http://arxiv.org/abs/2210.17484"/>
        <updated>2023-09-01T01:07:38.561Z</updated>
        <summary type="html"><![CDATA[Santiago Miret, Kin Long Kelvin Lee, Carmelo Gonzales, Marcel Nassar,
  Matthew Spellings]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[0/1 Deep Neural Networks via Block Coordinate Descent]]></title>
        <id>http://arxiv.org/abs/2206.09379</id>
        <link href="http://arxiv.org/abs/2206.09379"/>
        <updated>2023-09-01T01:07:34.775Z</updated>
        <summary type="html"><![CDATA[Hui Zhang, Shenglong Zhou, Geoffrey Ye Li, Naihua Xiu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Optimal Strategies for Temporal Tasks in Stochastic Games]]></title>
        <id>http://arxiv.org/abs/2102.04307</id>
        <link href="http://arxiv.org/abs/2102.04307"/>
        <updated>2023-09-01T01:07:34.769Z</updated>
        <summary type="html"><![CDATA[Alper Kamil Bozkurt, Yu Wang, Michael M. Zavlanos, Miroslav Pajic]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Image-based Generative Adversarial Networks for Time Series
  Generation]]></title>
        <id>http://arxiv.org/abs/2112.08060</id>
        <link href="http://arxiv.org/abs/2112.08060"/>
        <updated>2023-09-01T01:07:34.763Z</updated>
        <summary type="html"><![CDATA[Justin Hellermann, Stefan Lessmann]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GRASP: A Goodness-of-Fit Test for Classification Learning]]></title>
        <id>http://arxiv.org/abs/2209.02064</id>
        <link href="http://arxiv.org/abs/2209.02064"/>
        <updated>2023-09-01T01:07:34.757Z</updated>
        <summary type="html"><![CDATA[Adel Javanmard, Mohammad Mehrabi]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[a journal of the year of fires]]></title>
        <id>http://xianblog.wordpress.com/?p=53752</id>
        <link href="https://xianblog.wordpress.com/2023/09/01/a-journal-of-the-year-of-fires/"/>
        <updated>2023-08-31T22:23:18.000Z</updated>
        <summary type="html"><![CDATA[Indeed, the year of the mega-fires in West and East Canada, with Kelowna (and UBC Okanagan where BIRS 23w5106 took place) severely hit. Travelling through BC by car and plane, we saw several wildfire fume clouds, incl. a massive one from the Yellowknife region… Read a few dozen pages of Gene Wolfe’s The Knight, in […]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No Japanese people or American dogs]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60466</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60466&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=no-japanese-people-or-american-dogs"/>
        <updated>2023-08-31T20:56:32.000Z</updated>
        <summary type="html"><![CDATA[From the Twitter / X account of the famous popular science writer and muckraker, Fang Zhouzi / Fang Shimin: 光禁止“日本人”“美国狗”入内哪够啊，还有欧盟、英国、澳大利亚、加拿大……呢？不如干脆搞一个白名单，只欢迎朝鲜人、俄国狗入内。 pic.twitter.com/fhVu6oMyZx — 方舟子 (@fangshimin) August 31, 2023 The sign says: Rìběn rénMěiguó gǒujìnzhǐ rùnèi 日本人美国狗禁止入内 "Japanese peopleAmerican dogsAre forbidden from entering" It seems that Americans are of a lower order than Japanese.  What did we […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeatherBench 2: A benchmark for the next generation of data-driven weather models]]></title>
        <id>http://blog.research.google/2023/08/weatherbench-2-benchmark-for-next.html</id>
        <link href="http://blog.research.google/2023/08/weatherbench-2-benchmark-for-next.html"/>
        <updated>2023-08-31T17:14:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Stephan Rasp, Research Scientist, and Carla Bromberg, Program Lead, Google Research





In 1950, weather forecasting started its digital revolution when researchers used the first programmable, general-purpose computer ENIAC to solve mathematical equations describing how weather evolves. In the more than 70 years since, continuous advancements in computing power and improvements to the model formulations have led to steady gains in weather forecast skill: a 7-day forecast today is about as accurate as a 5-day forecast in 2000 and a 3-day forecast in 1980. While improving forecast accuracy at the pace of approximately one day per decade may not seem like a big deal, every day improved is important in far reaching use cases, such as for logistics planning, disaster management, agr…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Get some linguists out here"]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60468</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60468&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=get-some-linguists-out-here"/>
        <updated>2023-08-31T12:02:17.000Z</updated>
        <summary type="html"><![CDATA[Email from John B.: Writing to you about a never-expected-to-see sentence, in a novel I’m reading. “And get some linguists out here as fast as you can.” (Well, but why not?) It’s a newly released off the wall novel, The Road to Roswell by Connie Willis. The heroine, Francie, has agreed to be maid of […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Viral vibe]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60461</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60461&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=viral-vibe"/>
        <updated>2023-08-31T10:23:58.000Z</updated>
        <summary type="html"><![CDATA["Chinese Song Streamed Billions of Times for ‘Satirical’ Vibe" By Miho Tamura / Yomiuri Shimbun Correspondent Yomiuri Shimbun (August 29, 2023) Here's the song, with the lyrics in characters, pinyin romanization, and a poor English translation: There are several other inferior English translations on the internet.  I have yet to find one that is serviceable, […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PyTorch/XLA SPMD: Scale Up Model Training and Serving with Automatic Parallelization]]></title>
        <id>https://pytorch.org/blog/pytorch-xla-spmd/</id>
        <link href="https://pytorch.org/blog/pytorch-xla-spmd/"/>
        <updated>2023-08-31T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today, we are delighted to announce PyTorch/XLA SPMD: the integration of GSPMD into PyTorch with an easy to use API. PyTorch developers seeking superior performance and scale can train and serve the largest neural networks while maximizing utilization of AI accelerators, such as Google Cloud TPUs.]]></summary>
        <author>
            <name>PyTorch Website</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coagent Networks Revisited]]></title>
        <id>http://arxiv.org/abs/2001.10474</id>
        <link href="http://arxiv.org/abs/2001.10474"/>
        <updated>2023-08-31T02:20:13.444Z</updated>
        <summary type="html"><![CDATA[Modjtaba Shokrian Zini, Mohammad Pedramfar, Matthew Riemer, Ahmadreza
  Moradipari, Miao Liu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Application of Zone Method based Machine Learning and Physics-Informed
  Neural Networks in Reheating Furnaces]]></title>
        <id>http://arxiv.org/abs/2308.16089</id>
        <link href="http://arxiv.org/abs/2308.16089"/>
        <updated>2023-08-31T02:20:13.399Z</updated>
        <summary type="html"><![CDATA[Ujjal Kr Dutta, Aldo Lipani, Chuan Wang, Yukun Hu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Analysis Operator Learning: Acceleration and Convergence]]></title>
        <id>http://arxiv.org/abs/1802.05584</id>
        <link href="http://arxiv.org/abs/1802.05584"/>
        <updated>2023-08-31T02:20:13.343Z</updated>
        <summary type="html"><![CDATA[Il Yong Chun, Jeffrey A. Fessler]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment]]></title>
        <id>http://arxiv.org/abs/2304.06767</id>
        <link href="http://arxiv.org/abs/2304.06767"/>
        <updated>2023-08-31T02:20:13.273Z</updated>
        <summary type="html"><![CDATA[Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui
  Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, Tong Zhang]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[survex: an R package for explaining machine learning survival models]]></title>
        <id>http://arxiv.org/abs/2308.16113</id>
        <link href="http://arxiv.org/abs/2308.16113"/>
        <updated>2023-08-31T02:20:13.266Z</updated>
        <summary type="html"><![CDATA[Miko{\l}aj Spytek, Mateusz Krzyzi\'nski, Sophie Hanna Langbein
 , Hubert Baniecki, Marvin N. Wright, Przemys{\l}aw Biecek]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Korean words for "bottle gourd"]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60457</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60457&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=korean-words-for-bottle-gourd"/>
        <updated>2023-08-30T22:51:10.000Z</updated>
        <summary type="html"><![CDATA[I spent much of the summer in Vermont ensconced in a hermit's cottage reading, writing, and, of course, running through the Green Mountains and verdant woods.  When I left last week to come back for the fall semester at Penn, I brought with me about fifty bottle gourds (Lagenaria siceraria) that had been abandoned by […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ISBA²⁴ [website]]]></title>
        <id>http://xianblog.wordpress.com/?p=53938</id>
        <link href="https://xianblog.wordpress.com/2023/08/31/isba%c2%b2%e2%81%b4-website/"/>
        <updated>2023-08-30T22:24:14.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling and improving text stability in live captions]]></title>
        <id>http://blog.research.google/2023/08/modeling-and-improving-text-stability.html</id>
        <link href="http://blog.research.google/2023/08/modeling-and-improving-text-stability.html"/>
        <updated>2023-08-30T19:34:00.002Z</updated>
        <summary type="html"><![CDATA[Posted by Vikas Bahirwani, Research Scientist, and Susan Xu, Software Engineer, Google Augmented Reality





Automatic speech recognition (ASR) technology has made conversations more accessible with live captions in remote conferencing software, mobile applications, and head-worn displays. However, to maintain real-time responsiveness, live caption systems often display interim predictions that are updated as new utterances are received. This can cause text instability (a “flicker” where previously displayed text is updated, shown in the captions on the left in the video below), which can impair users' reading experience due to distraction, fatigue, and difficulty following the conversation.






In “Modeling and Improving Text Stability in Live Captions”, presented at ACM CHI 2023, we f…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Once in a blue moon]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60454</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60454&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=once-in-a-blue-moon"/>
        <updated>2023-08-30T14:17:17.000Z</updated>
        <summary type="html"><![CDATA[From the MIT International Student Office: Blue moons are best known from the phrase “once in a blue moon,” which means “extremely rarely.” The first recorded use of this idiomatic phrase is in an anti-clerical flyer in 1528, published by William Roy and Jeremy Barlowe. In reference to the clerical corruptions, one said in Old […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empowering Clinicians and Democratizing Data Science: Large Language
  Models Automate Machine Learning for Clinical Studies]]></title>
        <id>http://arxiv.org/abs/2308.14120</id>
        <link href="http://arxiv.org/abs/2308.14120"/>
        <updated>2023-08-30T04:17:23.914Z</updated>
        <summary type="html"><![CDATA[Soroosh Tayebi Arasteh, Tianyu Han, Mahshad Lotfinia, Christiane Kuhl,
  Jakob Nikolas Kather, Daniel Truhn, Sven Nebelung]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Based Residuals in Non-linear Factor Models: Precision
  Matrix Estimation of Returns with Low Signal-to-Noise Ratio]]></title>
        <id>http://arxiv.org/abs/2209.04512</id>
        <link href="http://arxiv.org/abs/2209.04512"/>
        <updated>2023-08-30T04:17:23.909Z</updated>
        <summary type="html"><![CDATA[Mehmet Caner, Maurizio Daniele]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Empirical Bregman Divergence for Uncertain Distance
  Representation]]></title>
        <id>http://arxiv.org/abs/2304.07689</id>
        <link href="http://arxiv.org/abs/2304.07689"/>
        <updated>2023-08-30T04:17:23.640Z</updated>
        <summary type="html"><![CDATA[Zhiyuan Li, Ziru Liu, Anna Zou, Anca L. Ralescu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Theoretical Guarantees of Learning Ensembling Strategies with
  Applications to Time Series Forecasting]]></title>
        <id>http://arxiv.org/abs/2305.15786</id>
        <link href="http://arxiv.org/abs/2305.15786"/>
        <updated>2023-08-30T04:17:23.619Z</updated>
        <summary type="html"><![CDATA[Hilaf Hasson, Danielle C. Maddix, Yuyang Wang, Gaurav Gupta, Youngsuk
  Park]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[differentially private distributed Bayesian linear regression with MCMC]]></title>
        <id>http://xianblog.wordpress.com/?p=53623</id>
        <link href="https://xianblog.wordpress.com/2023/08/30/differentially-private-distributed-bayesian-linear-regression-with-mcmc/"/>
        <updated>2023-08-29T22:23:23.000Z</updated>
        <summary type="html"><![CDATA[An ICML 2023 paper by Barıs¸ Alparslan, Sinan Yıldırım¸ and Ilker Birbil that (re)addresses the issue of privacy when running a Bayesian regression analysis. Resorting to the common notion of differential privacy, imposing a limited variability if a single observation is modified, and a Gaussian randomisation of the observations. “A differentially private algorithm constrains the […]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[De-Japanification of Japanese]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60435</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60435&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=de-japanification-of-japanese"/>
        <updated>2023-08-29T22:22:30.000Z</updated>
        <summary type="html"><![CDATA[This morning in the first class of my course on "Language, Script, and Society in China", I had just spoken about the most frequent morphemes in Mandarin, Taiwanese, and Japanese (the possessive particles de 的, e, and no の) and other common terms that had no fixed characters to write them or had to borrow […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SayTap: Language to quadrupedal locomotion]]></title>
        <id>http://blog.research.google/2023/08/saytap-language-to-quadrupedal.html</id>
        <link href="http://blog.research.google/2023/08/saytap-language-to-quadrupedal.html"/>
        <updated>2023-08-29T19:57:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Yujin Tang and Wenhao Yu, Research Scientists, Google




Simple and effective interaction between human and quadrupedal robots paves the way towards creating intelligent and capable helper robots, forging a future where technology enhances our lives in ways beyond our imagination. Key to such human-robot interaction systems is enabling quadrupedal robots to respond to natural language instructions. Recent developments in large language models (LLMs) have demonstrated the potential to perform high-level planning. Yet, it remains a challenge for LLMs to comprehend low-level commands, such as joint angle targets or motor torques, especially for inherently unstable legged robots, necessitating high-frequency control signals. Consequently, most existing work presumes the provision of…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Re-Grand Opening"]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60440</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60440&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=re-grand-opening"/>
        <updated>2023-08-29T16:22:55.000Z</updated>
        <summary type="html"><![CDATA[From John Bell: I thought of you and your interest in the oddities of linguistic expression a few days ago when I noticed that the local Safeway supermarket had large signs up saying "RE-GRAND OPENING".   They had recently done some renovation in a corner of the store — enlarging the self-checkout and the Starbucks […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Estimation of Derivatives Using Plug-in Kernel Ridge Regression
  Estimators]]></title>
        <id>http://arxiv.org/abs/2006.01350</id>
        <link href="http://arxiv.org/abs/2006.01350"/>
        <updated>2023-08-29T02:49:37.845Z</updated>
        <summary type="html"><![CDATA[Zejian Liu, Meng Li]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilevel orthogonal Bochner function subspaces with applications to
  robust machine learning]]></title>
        <id>http://arxiv.org/abs/2110.01729</id>
        <link href="http://arxiv.org/abs/2110.01729"/>
        <updated>2023-08-29T02:49:37.824Z</updated>
        <summary type="html"><![CDATA[Julio Enrique Castrillon-Candas, Dingning Liu, Sicheng Yang, Mark Kon]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Calibration: Learning of Model Calibration Using Differentiable
  Expected Calibration Error]]></title>
        <id>http://arxiv.org/abs/2106.09613</id>
        <link href="http://arxiv.org/abs/2106.09613"/>
        <updated>2023-08-29T02:49:37.801Z</updated>
        <summary type="html"><![CDATA[Ondrej Bohdal, Yongxin Yang, Timothy Hospedales]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sufficient Invariant Learning for Distribution Shift]]></title>
        <id>http://arxiv.org/abs/2210.13533</id>
        <link href="http://arxiv.org/abs/2210.13533"/>
        <updated>2023-08-29T02:49:37.795Z</updated>
        <summary type="html"><![CDATA[Taero Kim, Sungjun Lim, Kyungwoo Song]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TuneUp: A Simple Improved Training Strategy for Graph Neural Networks]]></title>
        <id>http://arxiv.org/abs/2210.14843</id>
        <link href="http://arxiv.org/abs/2210.14843"/>
        <updated>2023-08-29T02:49:37.689Z</updated>
        <summary type="html"><![CDATA[Weihua Hu, Kaidi Cao, Kexin Huang, Edward W Huang, Karthik Subbian,
  Kenji Kawaguchi, Jure Leskovec]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying AI-generated images with SynthID]]></title>
        <id>https://www.deepmind.com/blog/identifying-ai-generated-images-with-synthid</id>
        <link href="https://www.deepmind.com/blog/identifying-ai-generated-images-with-synthid"/>
        <updated>2023-08-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today, in partnership with Google Cloud, we’re beta launching SynthID, a new tool for watermarking and identifying AI-generated images. It’s being released to a limited number of Vertex AI customers using Imagen, one of our latest text-to-image models that uses input text to create photorealistic images. This technology embeds a digital watermark directly into the pixels of an image, making it imperceptible to the human eye, but detectable for identification. While generative AI can unlock huge creative potential, it also presents new risks, like creators spreading false information — both intentionally or unintentionally. Being able to identify AI-generated content is critical to empowering people with knowledge of when they’re interacting with generated media, and for helping prevent the spread of misinformation.]]></summary>
        <author>
            <name>DeepMind Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayes on the Beach²⁴]]></title>
        <id>http://xianblog.wordpress.com/?p=53928</id>
        <link href="https://xianblog.wordpress.com/2023/08/29/bayes-on-the-beach%c2%b2%e2%81%b4/"/>
        <updated>2023-08-28T22:23:18.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language and politics in Hong Kong: National Security and the promotion of topolect]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60423</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60423&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=language-and-politics-in-hong-kong-national-security-and-the-promotion-of-topolect"/>
        <updated>2023-08-28T19:20:14.000Z</updated>
        <summary type="html"><![CDATA[From the Hong Kong Language Learning Association: Announcement Regarding Suspension of Hong Kong Language Learning Association Given recent events, wherein personnel from the Hong Kong National Security Department (NSD) visited both my former residence and the residence of my family members for searches and inquiries, alleging a violation of the National Security Law in connection with […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RO-ViT: Region-aware pre-training for open-vocabulary object detection with vision transformers]]></title>
        <id>http://blog.research.google/2023/08/ro-vit-region-aware-pre-training-for.html</id>
        <link href="http://blog.research.google/2023/08/ro-vit-region-aware-pre-training-for.html"/>
        <updated>2023-08-28T16:59:00.003Z</updated>
        <summary type="html"><![CDATA[Posted by Dahun Kim and Weicheng Kuo, Research Scientists, Google



The ability to detect objects in the visual world is crucial for computer vision and machine intelligence, enabling applications like adaptive autonomous agents and versatile shopping systems. However, modern object detectors are limited by the manual annotations of their training data, resulting in a vocabulary size significantly smaller than the vast array of objects encountered in reality. To overcome this, the open-vocabulary detection task (OVD) has emerged, utilizing image-text pairs for training and incorporating new category names at test time by associating them with the image content. By treating categories as text embeddings, open-vocabulary detectors can predict a wide range of unseen objects. Various techniqu…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RO-ViT: Region-aware pre-training for open-vocabulary object detection with vision transformers]]></title>
        <id>http://ai.googleblog.com/2023/08/ro-vit-region-aware-pre-training-for.html</id>
        <link href="http://ai.googleblog.com/2023/08/ro-vit-region-aware-pre-training-for.html"/>
        <updated>2023-08-28T16:59:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Dahun Kim and Weicheng Kuo, Research Scientists, Google



The ability to detect objects in the visual world is crucial for computer vision and machine intelligence, enabling applications like adaptive autonomous agents and versatile shopping systems. However, modern object detectors are limited by the manual annotations of their training data, resulting in a vocabulary size significantly smaller than the vast array of objects encountered in reality. To overcome this, the open-vocabulary detection task (OVD) has emerged, utilizing image-text pairs for training and incorporating new category names at test time by associating them with the image content. By treating categories as text embeddings, open-vocabulary detectors can predict a wide range of unseen objects. Various techniqu…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mugshot, racketeering, listless …]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60427</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60427&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=mugshot-racketeering-listless"/>
        <updated>2023-08-28T11:57:06.000Z</updated>
        <summary type="html"><![CDATA[Over the past few months, U.S. political events have given Ben Zimmer opportunity for some fun etymologies in his WSJ column: mug shot, racketeering, listless. There are plenty more targets Out There — like candidate, from Latin candidus (“dazzling white, shining, clear”); or debate, originally from Latin dis- (“apart, in different directions”) + battuere (“to […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lion/Tiger优化器训练下的Embedding异常和对策]]></title>
        <id>https://kexue.fm/archives/9736</id>
        <link href="https://kexue.fm/archives/9736"/>
        <updated>2023-08-28T06:44:00.000Z</updated>
        <summary type="html"><![CDATA[打从在《Tiger：一个“抠”到极致的优化器》提出了Tiger优化器之后，Tiger就一直成为了我训练模型的“标配”优化器。最近笔者已经尝试将Tiger用到了70亿参数模型的预训练之中，前期效果...]]></summary>
        <author>
            <name>苏剑林</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Reasoning for Physics Informed Neural Networks]]></title>
        <id>http://arxiv.org/abs/2308.13222</id>
        <link href="http://arxiv.org/abs/2308.13222"/>
        <updated>2023-08-28T02:20:34.966Z</updated>
        <summary type="html"><![CDATA[Krzysztof M. Graczyk, Kornel Witkowski]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDSketch: Integrated Planning Domain Programming and Learning]]></title>
        <id>http://arxiv.org/abs/2303.05501</id>
        <link href="http://arxiv.org/abs/2303.05501"/>
        <updated>2023-08-28T02:20:34.959Z</updated>
        <summary type="html"><![CDATA[Jiayuan Mao, Tom\'as Lozano-P\'erez, Joshua B. Tenenbaum, Leslie Pack
  Kaelbling]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Density Matrices for Probabilistic Deep Learning]]></title>
        <id>http://arxiv.org/abs/2305.18204</id>
        <link href="http://arxiv.org/abs/2305.18204"/>
        <updated>2023-08-28T02:20:34.918Z</updated>
        <summary type="html"><![CDATA[Fabio A. Gonz\'alez, Ra\'ul Ramos-Poll\'an, Joseph A. Gallego-Mejia]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Six Lectures on Linearized Neural Networks]]></title>
        <id>http://arxiv.org/abs/2308.13431</id>
        <link href="http://arxiv.org/abs/2308.13431"/>
        <updated>2023-08-28T02:20:34.912Z</updated>
        <summary type="html"><![CDATA[Theodor Misiakiewicz, Andrea Montanari]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A topological model for partial equivariance in deep learning and data
  analysis]]></title>
        <id>http://arxiv.org/abs/2308.13357</id>
        <link href="http://arxiv.org/abs/2308.13357"/>
        <updated>2023-08-28T02:20:34.906Z</updated>
        <summary type="html"><![CDATA[Lucia Ferrari, Patrizio Frosini, Nicola Quercioli, Francesca Tombari]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The geometry of diffusion guidance]]></title>
        <id>https://sander.ai/2023/08/28/geometry.html</id>
        <link href="https://sander.ai/2023/08/28/geometry.html"/>
        <updated>2023-08-27T23:00:00.000Z</updated>
        <summary type="html"><![CDATA[Guidance is a powerful method that can be used to enhance diffusion model sampling. As I’ve discussed in an earlier blog post, it’s almost like a cheat code: it can improve sample quality so much that it’s as if the model had ten times the number of parameters – an order of magnitude improvement, basically for free! This follow-up post provides a geometric interpretation and visualisation of the diffusion sampling procedure, which I’ve found particularly useful to explain how guidance works.]]></summary>
        <author>
            <name>Sander Dieleman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BayesComp²⁵ in Singapore]]></title>
        <id>http://xianblog.wordpress.com/?p=53654</id>
        <link href="https://xianblog.wordpress.com/2023/08/28/bayescomp%c2%b2%e2%81%b5-in-singapore/"/>
        <updated>2023-08-27T22:23:38.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Core socialist values]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60406</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60406&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=core-socialist-values"/>
        <updated>2023-08-27T19:58:05.000Z</updated>
        <summary type="html"><![CDATA["Chinese slogans on London wall hold mirror to society: artist" Zhejiang-born Yique tries to find his place in UK after Brick Lane work TAY HAN NEE, Nikkei Asia Here are the 24 Chinese characters that constitute the twelve disyllabic words that make up the official "Core Socialist Values" of the PRC government: National values Prosperity […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Calimony"]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60400</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60400&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=calimony"/>
        <updated>2023-08-27T13:28:59.000Z</updated>
        <summary type="html"><![CDATA[I recently learned about the lexical blend calimony, which refers to a planned multi-million-dollar annual payment from UCLA (the University of California, Los Angeles) to Berkeley (the University of California, Berkeley, known in athletic contexts as "California" or "Cal"). Why will this payment exist? And why is it a metaphorical form of alimony? Short form: […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bilingual book takes top honors at New Zealand Children's Book Awards]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60382</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60382&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=bilingual-book-takes-top-honors-at-new-zealand-childrens-book-awards"/>
        <updated>2023-08-27T12:55:14.000Z</updated>
        <summary type="html"><![CDATA[Press comment:   "A bilingual book about the Māori creation story has won the highest accolade in children's literature."   Awards Announcement:   "Te Wehenga: The Separation of Ranginui and Papatūānuku presents the Māori creation pūrākau in a bold design using universal elements recognised across iwi. The bilingual text is poetic, and integrated into the artwork […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[23w5106 [group picture]]]></title>
        <id>http://xianblog.wordpress.com/?p=53766</id>
        <link href="https://xianblog.wordpress.com/2023/08/27/23w5106-group-picture/"/>
        <updated>2023-08-26T22:23:25.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[English accents]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60344</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60344&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=english-accents"/>
        <updated>2023-08-26T13:13:05.000Z</updated>
        <summary type="html"><![CDATA[Entertaining and edifying. As I said recently (last paragraph of this post:  "Irish accents" [8/13/23]), I'm enchanted by all the accents that I've ever heard.  I find them to be endlessly fascinating.   Selected readings "What makes an accent 'good' or 'bad?" (11/17/20) "Dialect vs. accent (vs. language)" (11/30/18) "Mandarin with an English accent" (4/5/23) […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debate-night pronouns]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60372</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60372&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=debate-night-pronouns"/>
        <updated>2023-08-26T13:13:00.000Z</updated>
        <summary type="html"><![CDATA[In a comment on yesterday's "Debate words" post, I noted that Donald Trump's ratio of I-words to we-words was "off the charts" compared the other eight candidates, and several people have asked me to give all the numbers. There's an idea Out There that such numbers are related to issues of personality and mood, though […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calyampudi Radhakrishna Rao (1920-2023)]]></title>
        <id>http://xianblog.wordpress.com/?p=53899</id>
        <link href="https://xianblog.wordpress.com/2023/08/26/calyampudi-radhakrishna-rao-1920-2023/"/>
        <updated>2023-08-25T22:23:46.000Z</updated>
        <summary type="html"><![CDATA[Just heard that C.R. Rao had passed away on Wednesday. Above is a 1941 picture I photographed while attending the jubilee of the Department of Statistics of the University of Calcuta. Showing R.A. Fisher and P.C. Mahalanobis surrounded by faculty and students from the Department. Including a very young Rao who would a few years […]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Responsible AI at Google Research: Perception Fairness]]></title>
        <id>http://blog.research.google/2023/08/responsible-ai-at-google-research.html</id>
        <link href="http://blog.research.google/2023/08/responsible-ai-at-google-research.html"/>
        <updated>2023-08-25T17:38:00.003Z</updated>
        <summary type="html"><![CDATA[Posted by Susanna Ricco and Utsav Prabhu, co-leads, Perception Fairness Team, Google Research





Google’s Responsible AI research is built on a foundation of collaboration — between teams with diverse backgrounds and expertise, between researchers and product developers, and ultimately with the community at large. The Perception Fairness team drives progress by combining deep subject-matter expertise in both computer vision and machine learning (ML) fairness with direct connections to the researchers building the perception systems that power products across Google and beyond. Together, we are working to intentionally design our systems to be inclusive from the ground up, guided by Google’s AI Principles.






Perception Fairness research spans the design, development, and deployment of…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Responsible AI at Google Research: Perception Fairness]]></title>
        <id>http://ai.googleblog.com/2023/08/responsible-ai-at-google-research.html</id>
        <link href="http://ai.googleblog.com/2023/08/responsible-ai-at-google-research.html"/>
        <updated>2023-08-25T17:38:00.003Z</updated>
        <summary type="html"><![CDATA[Posted by Susanna Ricco and Utsav Prabhu, co-leads, Perception Fairness Team, Google Research





Google’s Responsible AI research is built on a foundation of collaboration — between teams with diverse backgrounds and expertise, between researchers and product developers, and ultimately with the community at large. The Perception Fairness team drives progress by combining deep subject-matter expertise in both computer vision and machine learning (ML) fairness with direct connections to the researchers building the perception systems that power products across Google and beyond. Together, we are working to intentionally design our systems to be inclusive from the ground up, guided by Google’s AI Principles.






Perception Fairness research spans the design, development, and deployment of…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debate words]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60351</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60351&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=debate-words-3"/>
        <updated>2023-08-25T12:41:38.000Z</updated>
        <summary type="html"><![CDATA[The Transcript Library at rev.com is a great resource — within 24 hours, they had transcripts of Wednesday's Fox News Republican presidential debate, and also of Tucker Carlson's debate night interview with Donald Trump on X. So this morning I downloaded the transcripts, and ran the code that I've used several times over the years […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calligraphic license]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60331</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60331&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=calligraphic-license"/>
        <updated>2023-08-25T03:07:39.000Z</updated>
        <summary type="html"><![CDATA[Shaing tai asked whether I recognized these characters: I didn't recognize the two characters, so I asked several Chinese and Japanese friends, and none of them recognized the characters either.  My curiosity piqued, I figured I might as well take a stab on my own to figure out what they were. The calligraphy was displayed […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StableDR: Stabilized Doubly Robust Learning for Recommendation on Data
  Missing Not at Random]]></title>
        <id>http://arxiv.org/abs/2205.04701</id>
        <link href="http://arxiv.org/abs/2205.04701"/>
        <updated>2023-08-25T02:20:22.385Z</updated>
        <summary type="html"><![CDATA[Haoxuan Li, Chunyuan Zheng, Peng Wu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Intentional Forgetting-Driven Self-Healing Method For Deep
  Reinforcement Learning Systems]]></title>
        <id>http://arxiv.org/abs/2308.12445</id>
        <link href="http://arxiv.org/abs/2308.12445"/>
        <updated>2023-08-25T02:20:22.378Z</updated>
        <summary type="html"><![CDATA[Ahmed Haj Yahmed, Rached Bouchoucha, Houssem Ben Braiek, Foutse Khomh]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single-shot Bayesian approximation for neural networks]]></title>
        <id>http://arxiv.org/abs/2308.12785</id>
        <link href="http://arxiv.org/abs/2308.12785"/>
        <updated>2023-08-25T02:20:22.204Z</updated>
        <summary type="html"><![CDATA[Kai Brach, Beate Sick, Oliver D\"urr]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interneurons accelerate learning dynamics in recurrent neural networks
  for statistical adaptation]]></title>
        <id>http://arxiv.org/abs/2209.10634</id>
        <link href="http://arxiv.org/abs/2209.10634"/>
        <updated>2023-08-25T02:20:22.197Z</updated>
        <summary type="html"><![CDATA[David Lipshutz, Cengiz Pehlevan, Dmitri B. Chklovskii]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[making the next meeting more productive]]></title>
        <id>http://xianblog.wordpress.com/?p=53619</id>
        <link href="https://xianblog.wordpress.com/2023/08/25/making-the-next-meeting-more-productive/"/>
        <updated>2023-08-24T22:23:56.000Z</updated>
        <summary type="html"><![CDATA[One of the students’ requests I almost invariably reject is code debugging (and they are warned about it from the start). Here is an illustration why, with an R code sent by a student working this summer on the standard estimators of a Cauchy location parameter, asking for debugging help in order “to make the […]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to compare a noisy quantum processor to a classical computer]]></title>
        <id>http://blog.research.google/2023/08/how-to-compare-noisy-quantum-processor.html</id>
        <link href="http://blog.research.google/2023/08/how-to-compare-noisy-quantum-processor.html"/>
        <updated>2023-08-24T22:10:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Sergio Boixo and Vadim Smelyanskiy, Principal Scientists, Google Quantum AI Team




A full-scale error-corrected quantum computer will be able to solve some problems that are impossible for classical computers, but building such a device is a huge endeavor. We are proud of the milestones that we have achieved toward a fully error-corrected quantum computer, but that large-scale computer is still some number of years away. Meanwhile, we are using our current noisy quantum processors as flexible platforms for quantum experiments. 



In contrast to an error-corrected quantum computer, experiments in noisy quantum processors are currently limited to a few thousand quantum operations or gates, before noise degrades the quantum state. In 2019 we implemented a specific computational t…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to compare a noisy quantum processor to a classical computer]]></title>
        <id>http://ai.googleblog.com/2023/08/how-to-compare-noisy-quantum-processor.html</id>
        <link href="http://ai.googleblog.com/2023/08/how-to-compare-noisy-quantum-processor.html"/>
        <updated>2023-08-24T22:10:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Sergio Boixo and Vadim Smelyanskiy, Principal Scientists, Google Quantum AI Team




A full-scale error-corrected quantum computer will be able to solve some problems that are impossible for classical computers, but building such a device is a huge endeavor. We are proud of the milestones that we have achieved toward a fully error-corrected quantum computer, but that large-scale computer is still some number of years away. Meanwhile, we are using our current noisy quantum processors as flexible platforms for quantum experiments. 



In contrast to an error-corrected quantum computer, experiments in noisy quantum processors are currently limited to a few thousand quantum operations or gates, before noise degrades the quantum state. In 2019 we implemented a specific computational t…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teaching language models to reason algorithmically]]></title>
        <id>http://blog.research.google/2023/08/teaching-language-models-to-reason.html</id>
        <link href="http://blog.research.google/2023/08/teaching-language-models-to-reason.html"/>
        <updated>2023-08-24T19:33:00.002Z</updated>
        <summary type="html"><![CDATA[Posted by Hattie Zhou, Graduate Student at MILA, Hanie Sedghi, Research Scientist, Google




Large language models (LLMs), such as GPT-3 and PaLM, have shown impressive progress in recent years, which have been driven by scaling up models and training data sizes. Nonetheless, a long standing debate has been whether LLMs can reason symbolically (i.e., manipulating symbols based on logical rules). For example, LLMs are able to perform simple arithmetic operations when numbers are small, but struggle to perform with large numbers. This suggests that LLMs have not learned the underlying rules needed to perform these arithmetic operations. 



While neural networks have powerful pattern matching capabilities, they are prone to overfitting to spurious statistical patterns in the data. This does…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teaching language models to reason algorithmically]]></title>
        <id>http://ai.googleblog.com/2023/08/teaching-language-models-to-reason.html</id>
        <link href="http://ai.googleblog.com/2023/08/teaching-language-models-to-reason.html"/>
        <updated>2023-08-24T19:33:00.002Z</updated>
        <summary type="html"><![CDATA[Posted by Hattie Zhou, Graduate Student at MILA, Hanie Sedghi, Research Scientist, Google




Large language models (LLMs), such as GPT-3 and PaLM, have shown impressive progress in recent years, which have been driven by scaling up models and training data sizes. Nonetheless, a long standing debate has been whether LLMs can reason symbolically (i.e., manipulating symbols based on logical rules). For example, LLMs are able to perform simple arithmetic operations when numbers are small, but struggle to perform with large numbers. This suggests that LLMs have not learned the underlying rules needed to perform these arithmetic operations. 



While neural networks have powerful pattern matching capabilities, they are prone to overfitting to spurious statistical patterns in the data. This does…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer升级之路：14、当HWFA遇见ReRoPE]]></title>
        <id>https://kexue.fm/archives/9731</id>
        <link href="https://kexue.fm/archives/9731"/>
        <updated>2023-08-24T10:24:00.000Z</updated>
        <summary type="html"><![CDATA[在上一篇文章《Transformer升级之路：13、逆用Leaky ReRoPE》中，笔者尝试通过在训练阶段逆用Leaky ReRoPE的思路，使得推理阶段的位置编码变为正常的RoPE，从而在达...]]></summary>
        <author>
            <name>苏剑林</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large Scale Training of Hugging Face Transformers on TPUs With PyTorch/XLA FSDP]]></title>
        <id>https://pytorch.org/blog/large-scale-training-hugging-face/</id>
        <link href="https://pytorch.org/blog/large-scale-training-hugging-face/"/>
        <updated>2023-08-24T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[AI is transforming many industries through advanced capabilities such as understanding and generating language, answering questions, and delivering accurate recommendations. These capabilities are fueled by ever-increasing size and complexity of AI models, which require vast amounts of computing power to train.]]></summary>
        <author>
            <name>PyTorch Website</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vulgar Cantonese elegantly displayed]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60319</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60319&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=vulgar-cantonese-elegantly-displayed"/>
        <updated>2023-08-24T03:03:11.000Z</updated>
        <summary type="html"><![CDATA[This curious Cantonese couplet appeared on Weibo today: Mandarin romanization, hanzi transcription, and glyph by glyph literal translation: gān jiāo jú lí luó yòu yàn jiù diāo lí shī fèi 柑蕉桔梨蘿柚 雁鷲雕狸獅狒 “mandarin orange*, banana / plantain, tangerine*, pear, turnip**, pomelo” “wild goose, vulture, eagle, fox***, lion, baboon****” *The Sinitic terminology for "mandarin orange", "tangerine", and other […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The car hit cheese bacon mushroom face, part 2]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60314</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60314&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-car-hit-cheese-bacon-mushroom-face-part-2"/>
        <updated>2023-08-24T02:57:17.000Z</updated>
        <summary type="html"><![CDATA[Todd Wilbur shared this menu item on Facebook: This is a veritable masterpiece of Chinglish, covering several of the usual types of mistranslation inherent in the genre.  To unpack the crash, let's begin with the standard Language Log romanization, transcription, and translation: chēdǎ zhīshì yānròu mógū miàn 车打芝士腌肉蘑菇面 "cheddar cheese bacon mushroom noodles" All the […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying degeneracy in singular models via the learning coefficient]]></title>
        <id>http://arxiv.org/abs/2308.12108</id>
        <link href="http://arxiv.org/abs/2308.12108"/>
        <updated>2023-08-24T02:19:09.913Z</updated>
        <summary type="html"><![CDATA[Edmund Lau, Daniel Murfet, Susan Wei]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Learn Financial Networks for Optimising Momentum Strategies]]></title>
        <id>http://arxiv.org/abs/2308.12212</id>
        <link href="http://arxiv.org/abs/2308.12212"/>
        <updated>2023-08-24T02:19:09.895Z</updated>
        <summary type="html"><![CDATA[Xingyue (Stacy) Pu, Stefan Zohren, Stephen Roberts, Xiaowen Dong]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A multiobjective continuation method to compute the regularization path
  of deep neural networks]]></title>
        <id>http://arxiv.org/abs/2308.12044</id>
        <link href="http://arxiv.org/abs/2308.12044"/>
        <updated>2023-08-24T02:19:09.890Z</updated>
        <summary type="html"><![CDATA[Augustina C. Amakor, Konstantin Sontag, Sebastian Peitz]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Will More Expressive Graph Neural Networks do Better on Generative
  Tasks?]]></title>
        <id>http://arxiv.org/abs/2308.11978</id>
        <link href="http://arxiv.org/abs/2308.11978"/>
        <updated>2023-08-24T02:19:09.858Z</updated>
        <summary type="html"><![CDATA[Xiandong Zou, Xiangyu Zhao, Pietro Li\`o, Yiren Zhao]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Critical Learning Periods Emerge Even in Deep Linear Networks]]></title>
        <id>http://arxiv.org/abs/2308.12221</id>
        <link href="http://arxiv.org/abs/2308.12221"/>
        <updated>2023-08-24T02:19:09.787Z</updated>
        <summary type="html"><![CDATA[Michael Kleinman, Alessandro Achille, Stefano Soatto]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[smoky sunset [jatp]]]></title>
        <id>http://xianblog.wordpress.com/?p=53842</id>
        <link href="https://xianblog.wordpress.com/2023/08/24/smoky-sunset-jatp/"/>
        <updated>2023-08-23T22:23:00.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Doom-lexing]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60323</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60323&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=doom-lexing"/>
        <updated>2023-08-23T20:33:58.000Z</updated>
        <summary type="html"><![CDATA[Yesterday, Randoh Sallihall from unscramblerer.com sent this note: Susie Dent has an ever growing Twitter following of 1,1 million unique word lovers to whom she shares her daily word of the day. Word search engine Unscramblerer.com went through Susie Dent's whole Twitter history and analyzed what are the most liked, shared and commented words of […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EvoPruneDeepTL: An Evolutionary Pruning Model for Transfer Learning
  based Deep Neural Networks]]></title>
        <id>http://arxiv.org/abs/2202.03844</id>
        <link href="http://arxiv.org/abs/2202.03844"/>
        <updated>2023-08-23T03:15:13.514Z</updated>
        <summary type="html"><![CDATA[Javier Poyatos, Daniel Molina, Aritz. D. Martinez, Javier Del Ser,
  Francisco Herrera]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Theory-Guided Heuristic Progressive Multi-View Coding]]></title>
        <id>http://arxiv.org/abs/2109.02344</id>
        <link href="http://arxiv.org/abs/2109.02344"/>
        <updated>2023-08-23T03:15:13.512Z</updated>
        <summary type="html"><![CDATA[Jiangmeng Li, Wenwen Qiang, Hang Gao, Bing Su, Farid Razzak, Jie Hu,
  Changwen Zheng, Hui Xiong]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibrating and Improving Graph Contrastive Learning]]></title>
        <id>http://arxiv.org/abs/2101.11525</id>
        <link href="http://arxiv.org/abs/2101.11525"/>
        <updated>2023-08-23T03:15:13.511Z</updated>
        <summary type="html"><![CDATA[Kaili Ma, Haochen Yang, Han Yang, Yongqiang Chen, James Cheng]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Learn Graph Topologies]]></title>
        <id>http://arxiv.org/abs/2110.09807</id>
        <link href="http://arxiv.org/abs/2110.09807"/>
        <updated>2023-08-23T03:15:13.510Z</updated>
        <summary type="html"><![CDATA[Xingyue Pu, Tianyue Cao, Xiaoyun Zhang, Xiaowen Dong, Siheng Chen]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FRAug: Tackling Federated Learning with Non-IID Features via
  Representation Augmentation]]></title>
        <id>http://arxiv.org/abs/2205.14900</id>
        <link href="http://arxiv.org/abs/2205.14900"/>
        <updated>2023-08-23T03:15:13.292Z</updated>
        <summary type="html"><![CDATA[Haokun Chen, Ahmed Frikha, Denis Krompass, Jindong Gu, Volker Tresp]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Waterless, emission-free toilet that Chairman Xi saw]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60288</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60288&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=waterless-emission-free-toilet-that-chairman-xi-saw"/>
        <updated>2023-08-23T01:31:20.000Z</updated>
        <summary type="html"><![CDATA[(see in particular the second item) If this isn't dictator status, I don't know what ishttps://t.co/A4guMzG4m1 — Bumboclott (@Bumboclott) June 29, 2023 The poster board on the bottom left shows Xi inspecting the hallowed hardware on an earlier visit.  It says: Zhè shì Xí Zhǔxí kànguò de mǎtǒng 这是习主席看过的马桶 "This is the toilet that Chairman […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metlakatla trail [jatp]]]></title>
        <id>http://xianblog.wordpress.com/?p=53811</id>
        <link href="https://xianblog.wordpress.com/2023/08/23/metlakatla-trail-jatp/"/>
        <updated>2023-08-22T22:21:13.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language to rewards for robotic skill synthesis]]></title>
        <id>http://blog.research.google/2023/08/language-to-rewards-for-robotic-skill.html</id>
        <link href="http://blog.research.google/2023/08/language-to-rewards-for-robotic-skill.html"/>
        <updated>2023-08-22T18:47:00.004Z</updated>
        <summary type="html"><![CDATA[Posted by Wenhao Yu and Fei Xia, Research Scientists, Google






Empowering end-users to interactively teach robots to perform novel tasks is a crucial capability for their successful integration into real-world applications. For example, a user may want to teach a robot dog to perform a new trick, or teach a manipulator robot how to organize a lunch box based on user preferences. The recent advancements in large language models (LLMs) pre-trained on extensive internet data have shown a promising path towards achieving this goal. Indeed, researchers have explored diverse ways of leveraging LLMs for robotics, from step-by-step planning and goal-oriented dialogue to robot-code-writing agents. 



While these methods impart new modes of compositional generalization, they focus on using lang…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language to rewards for robotic skill synthesis]]></title>
        <id>http://ai.googleblog.com/2023/08/language-to-rewards-for-robotic-skill.html</id>
        <link href="http://ai.googleblog.com/2023/08/language-to-rewards-for-robotic-skill.html"/>
        <updated>2023-08-22T18:47:00.003Z</updated>
        <summary type="html"><![CDATA[Posted by Wenhao Yu and Fei Xia, Research Scientists, Google






Empowering end-users to interactively teach robots to perform novel tasks is a crucial capability for their successful integration into real-world applications. For example, a user may want to teach a robot dog to perform a new trick, or teach a manipulator robot how to organize a lunch box based on user preferences. The recent advancements in large language models (LLMs) pre-trained on extensive internet data have shown a promising path towards achieving this goal. Indeed, researchers have explored diverse ways of leveraging LLMs for robotics, from step-by-step planning and goal-oriented dialogue to robot-code-writing agents. 



While these methods impart new modes of compositional generalization, they focus on using lang…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ron's Princibles]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60291</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60291&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rons-princibles"/>
        <updated>2023-08-22T17:28:36.000Z</updated>
        <summary type="html"><![CDATA[Sunday's post on "Listless vessels" opened with this clip: Your browser does not support the audio element. The movement has got to be about what are you trying to achieve on behalf of the American people and that's got to be based in principle uh because if you're not rooted in principle uh if all […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Parameter-Efficient Learning Approach to Arabic Dialect Identification
  with Pre-Trained General-Purpose Speech Model]]></title>
        <id>http://arxiv.org/abs/2305.11244</id>
        <link href="http://arxiv.org/abs/2305.11244"/>
        <updated>2023-08-22T02:19:22.867Z</updated>
        <summary type="html"><![CDATA[Srijith Radhakrishnan, Chao-Han Huck Yang, Sumeer Ahmad Khan, Narsis
  A. Kiani, David Gomez-Cabrero, Jesper N. Tegner]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Rate Schedules in the Presence of Distribution Shift]]></title>
        <id>http://arxiv.org/abs/2303.15634</id>
        <link href="http://arxiv.org/abs/2303.15634"/>
        <updated>2023-08-22T02:19:22.850Z</updated>
        <summary type="html"><![CDATA[Matthew Fahrbach, Adel Javanmard, Vahab Mirrokni, Pratik Worah]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Transparent AI: A Survey on Interpreting the Inner Structures of
  Deep Neural Networks]]></title>
        <id>http://arxiv.org/abs/2207.13243</id>
        <link href="http://arxiv.org/abs/2207.13243"/>
        <updated>2023-08-22T02:19:22.682Z</updated>
        <summary type="html"><![CDATA[Tilman R\"auker, Anson Ho, Stephen Casper, Dylan Hadfield-Menell]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-separable Covariance Kernels for Spatiotemporal Gaussian Processes
  based on a Hybrid Spectral Method and the Harmonic Oscillator]]></title>
        <id>http://arxiv.org/abs/2302.09580</id>
        <link href="http://arxiv.org/abs/2302.09580"/>
        <updated>2023-08-22T02:19:22.670Z</updated>
        <summary type="html"><![CDATA[Dionissios T.Hristopulos]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GradientCoin: A Peer-to-Peer Decentralized Large Language Models]]></title>
        <id>http://arxiv.org/abs/2308.10502</id>
        <link href="http://arxiv.org/abs/2308.10502"/>
        <updated>2023-08-22T02:19:22.473Z</updated>
        <summary type="html"><![CDATA[Yeqi Gao, Zhao Song, Junze Yin]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The President [jatp]]]></title>
        <id>http://xianblog.wordpress.com/?p=53836</id>
        <link href="https://xianblog.wordpress.com/2023/08/22/the-president-jatp/"/>
        <updated>2023-08-21T22:23:41.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MLIR — Defining a New Dialect]]></title>
        <id>https://jeremykun.com/?p=119602</id>
        <link href="https://jeremykun.com/2023/08/21/mlir-defining-a-new-dialect/"/>
        <updated>2023-08-21T15:00:00.000Z</updated>
        <summary type="html"><![CDATA[Table of Contents In the last article in the series, we migrated the passes we had written to use the tablegen code generation framework. That was a preface to using tablegen to define dialects. In this article we’ll define a dialect that represents arithmetic on single-variable polynomials, with coefficients in $\mathbb{Z} / 2^{32} \mathbb{Z}$ (32-bit […]]]></summary>
        <author>
            <name>Jeremy Kun</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Tocharian Trek:  PIE and migration across Eurasia]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60244</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60244&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-tocharian-trek-pie-and-migration-across-eurasia"/>
        <updated>2023-08-21T14:29:48.000Z</updated>
        <summary type="html"><![CDATA[In recent weeks and months, Language Log has been quite active in discussions on Tocharian and its relationship to other members of Indo-European.  Today's post takes a different approach from this post made just yesterday and many earlier posts. "Europe's ancient languages shed light on a great migration and weather vocabulary" by Ali Jones, Horizon: […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google at Interspeech 2023]]></title>
        <id>http://blog.research.google/2023/08/google-at-interspeech-2023.html</id>
        <link href="http://blog.research.google/2023/08/google-at-interspeech-2023.html"/>
        <updated>2023-08-21T07:33:00.002Z</updated>
        <summary type="html"><![CDATA[Posted by Catherine Armato, Program Manager, Google




This week, the 24th Annual Conference of the International Speech Communication Association (INTERSPEECH 2023) is being held in Dublin, Ireland, representing one of the world’s most extensive conferences on research and technology of spoken language understanding and processing. Experts in speech-related research fields gather to take part in oral presentations and poster sessions and to build collaborations across the globe.




We are excited to be a Platinum Sponsor of INTERSPEECH 2023, where we will be showcasing more than 20 research publications and supporting a number of workshops and special sessions. We welcome in-person attendees to drop by the Google Research booth to meet our researchers and participate in Q&As and demonst…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google at Interspeech 2023]]></title>
        <id>http://ai.googleblog.com/2023/08/google-at-interspeech-2023.html</id>
        <link href="http://ai.googleblog.com/2023/08/google-at-interspeech-2023.html"/>
        <updated>2023-08-21T07:33:00.002Z</updated>
        <summary type="html"><![CDATA[Posted by Catherine Armato, Program Manager, Google




This week, the 24th Annual Conference of the International Speech Communication Association (INTERSPEECH 2023) is being held in Dublin, Ireland, representing one of the world’s most extensive conferences on research and technology of spoken language understanding and processing. Experts in speech-related research fields gather to take part in oral presentations and poster sessions and to build collaborations across the globe.




We are excited to be a Platinum Sponsor of INTERSPEECH 2023, where we will be showcasing more than 20 research publications and supporting a number of workshops and special sessions. We welcome in-person attendees to drop by the Google Research booth to meet our researchers and participate in Q&As and demonst…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The rightness of the Riemann Hypothesis]]></title>
        <id>http://arxiv.org/abs/1608.03199</id>
        <link href="http://arxiv.org/abs/1608.03199"/>
        <updated>2023-08-21T05:13:37.677Z</updated>
        <summary type="html"><![CDATA[Shaoyong Lai]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Strong boundedness of simply connected split Chevalley groups defined
  over rings]]></title>
        <id>http://arxiv.org/abs/2004.05039</id>
        <link href="http://arxiv.org/abs/2004.05039"/>
        <updated>2023-08-21T05:13:37.670Z</updated>
        <summary type="html"><![CDATA[Alexander Alois Trost]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A certain structure of Artin groups and the isomorphism conjecture]]></title>
        <id>http://arxiv.org/abs/1811.11589</id>
        <link href="http://arxiv.org/abs/1811.11589"/>
        <updated>2023-08-21T05:13:37.663Z</updated>
        <summary type="html"><![CDATA[S. K. Roushon]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantisation of derived Poisson structures]]></title>
        <id>http://arxiv.org/abs/1708.00496</id>
        <link href="http://arxiv.org/abs/1708.00496"/>
        <updated>2023-08-21T05:13:37.655Z</updated>
        <summary type="html"><![CDATA[J.P.Pridham]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lie, associative and commutative quasi-isomorphism]]></title>
        <id>http://arxiv.org/abs/1904.03585</id>
        <link href="http://arxiv.org/abs/1904.03585"/>
        <updated>2023-08-21T05:13:37.458Z</updated>
        <summary type="html"><![CDATA[Ricardo Campos, Dan Petersen, Daniel Robert-Nicoud, Felix Wierstra]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FeDXL: Provable Federated Learning for Deep X-Risk Optimization]]></title>
        <id>http://arxiv.org/abs/2210.14396</id>
        <link href="http://arxiv.org/abs/2210.14396"/>
        <updated>2023-08-21T03:14:40.027Z</updated>
        <summary type="html"><![CDATA[Zhishuai Guo, Rong Jin, Jiebo Luo, Tianbao Yang]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient 1 Iteration Learning Algorithm for Gaussian Mixture Model
  And Gaussian Mixture Embedding For Neural Network]]></title>
        <id>http://arxiv.org/abs/2308.09444</id>
        <link href="http://arxiv.org/abs/2308.09444"/>
        <updated>2023-08-21T03:14:40.020Z</updated>
        <summary type="html"><![CDATA[Weiguo Lu, Xuan Wu, Deng Ding, Gangnan Yuan]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving PDEs on Spheres with Physics-Informed Convolutional Neural
  Networks]]></title>
        <id>http://arxiv.org/abs/2308.09605</id>
        <link href="http://arxiv.org/abs/2308.09605"/>
        <updated>2023-08-21T03:14:38.699Z</updated>
        <summary type="html"><![CDATA[Guanhang Lei, Zhen Lei, Lei Shi, Chenyu Zeng, Ding-Xuan Zhou]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hitting the High-Dimensional Notes: An ODE for SGD learning dynamics on
  GLMs and multi-index models]]></title>
        <id>http://arxiv.org/abs/2308.08977</id>
        <link href="http://arxiv.org/abs/2308.08977"/>
        <updated>2023-08-21T03:14:38.692Z</updated>
        <summary type="html"><![CDATA[Elizabeth Collins-Woodfin, Courtney Paquette, Elliot Paquette, Inbar
  Seroussi]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise Sensitivity and Stability of Deep Neural Networks for Binary
  Classification]]></title>
        <id>http://arxiv.org/abs/2308.09374</id>
        <link href="http://arxiv.org/abs/2308.09374"/>
        <updated>2023-08-21T03:14:38.671Z</updated>
        <summary type="html"><![CDATA[Johan Jonasson, Jeffrey E. Steif, Olof Zetterqvist]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[a journal of the conquest, war, famine, and death year]]></title>
        <id>http://xianblog.wordpress.com/?p=53509</id>
        <link href="https://xianblog.wordpress.com/2023/08/21/a-journal-of-the-conquest-war-famine-and-death-year-3/"/>
        <updated>2023-08-20T22:23:33.000Z</updated>
        <summary type="html"><![CDATA[Read at last the (disappointing) last tome of Christelle Dabos’ The Mirror Visitor, The Storm of Echoes, as I find the unravelling of the story unbearably slow and poorly constructed, contrasting with the earlier volumes where the universe building was central to the appeal of the series. Here it is collapsing, literally and figuratively. Without […]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Val de Montferrand [bergerie de l’Hortus]]]></title>
        <id>http://xianblog.wordpress.com/?p=53028</id>
        <link href="https://xianblog.wordpress.com/2023/08/20/val-de-montferrand-bergerie-de-lhortus/"/>
        <updated>2023-08-20T18:20:43.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Listless vessels]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60245</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60245&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=listless-vessels"/>
        <updated>2023-08-20T15:11:05.000Z</updated>
        <summary type="html"><![CDATA[In an interview on Friday ("DeSantis plans to do what Trump couldn't | Full Interview with Will Witt", The Florida Standard 8/18/2023), Ron DeSantis referred to (some of?) Donald Trump's followers as "listless vessels": Your browser does not support the audio element. The movement has got to be about what are you trying to achieve […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The origins and affinities of Tocharian]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60225</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60225&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-origins-and-affinities-of-tocharian"/>
        <updated>2023-08-20T13:23:38.000Z</updated>
        <summary type="html"><![CDATA[I asked several IEist colleagues: Of all the IE languages, which one is Tocharian closest to? Celtic? Germanic? Answers received: James P. Mallory: I can't answer that but since almost all the phylogenies have it leaving after Anatolian one could argue that it does not actually have any close relatives, i.e., it was never really […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A sign of the future?]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60221</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60221&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=a-sign-of-the-future"/>
        <updated>2023-08-19T12:03:58.000Z</updated>
        <summary type="html"><![CDATA[Anemona Hartocollis, "Slashing Its Budget, West Virginia University Asks, What Is Essential?", NYT 8/18/2023: The state’s flagship school will no longer teach world languages or creative writing — a sign, its president says, of the future at many public universities. Christian Adams wants to be an immigration or labor lawyer, so he planned to major […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Axing languages and linguistics at West Virginia University]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60207</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60207&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=axing-languages-and-linguistics-at-west-virginia-university"/>
        <updated>2023-08-19T11:39:22.000Z</updated>
        <summary type="html"><![CDATA[From M. Paul Shore: Article that appeared on the Washington Post website this morning (and is therefore likely to appear in tomorrow's print edition) about the recently proposed demise of, among other things, the Department of World Languages, Literatures and Linguistics at West Virginia University's flagship Morgantown campus (note that that department name really should be […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kelowna two weeks later]]></title>
        <id>http://xianblog.wordpress.com/?p=53819</id>
        <link href="https://xianblog.wordpress.com/2023/08/19/kelowna-two-weeks-later/"/>
        <updated>2023-08-18T22:23:14.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI hype #∞]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60209</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60209&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=ai-hype"/>
        <updated>2023-08-18T19:37:27.000Z</updated>
        <summary type="html"><![CDATA[In social and even mass media, you may have seen coverage of a recent paper by Joshua Harrison et al., "A Practical Deep Learning-Based Acoustic Side Channel Attack on Keyboards". Some samples of the clickbait: "A.I. can identify keystrokes by just the sound of your typing and steal information with 95% accuracy, new research shows", […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomous visual information seeking with large language models]]></title>
        <id>http://blog.research.google/2023/08/autonomous-visual-information-seeking.html</id>
        <link href="http://blog.research.google/2023/08/autonomous-visual-information-seeking.html"/>
        <updated>2023-08-18T18:28:00.004Z</updated>
        <summary type="html"><![CDATA[Posted by Ziniu Hu, Student Researcher, and Alireza Fathi, Research Scientist, Google Research, Perception Team




There has been great progress towards adapting large language models (LLMs) to accommodate multimodal inputs for tasks including image captioning, visual question answering (VQA), and open vocabulary recognition. Despite such achievements, current state-of-the-art visual language models (VLMs) perform inadequately on visual information seeking datasets, such as Infoseek and OK-VQA, where external knowledge is required to answer the questions. 
 






Examples of visual information seeking queries where external knowledge is required to answer the question. Images are taken from the OK-VQA dataset.

AVIS: Autonomous Visual Information Seeking with Large Language Models”, we i…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomous visual information seeking with large language models]]></title>
        <id>http://ai.googleblog.com/2023/08/autonomous-visual-information-seeking.html</id>
        <link href="http://ai.googleblog.com/2023/08/autonomous-visual-information-seeking.html"/>
        <updated>2023-08-18T18:28:00.004Z</updated>
        <summary type="html"><![CDATA[Posted by Ziniu Hu, Student Researcher, and Alireza Fathi, Research Scientist, Google Research, Perception Team




There has been great progress towards adapting large language models (LLMs) to accommodate multimodal inputs for tasks including image captioning, visual question answering (VQA), and open vocabulary recognition. Despite such achievements, current state-of-the-art visual language models (VLMs) perform inadequately on visual information seeking datasets, such as Infoseek and OK-VQA, where external knowledge is required to answer the questions. 
 






Examples of visual information seeking queries where external knowledge is required to answer the question. Images are taken from the OK-VQA dataset.

AVIS: Autonomous Visual Information Seeking with Large Language Models”, we i…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nasality]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60201</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60201&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=nasality"/>
        <updated>2023-08-18T08:58:13.000Z</updated>
        <summary type="html"><![CDATA[Acoustically and phonetically, how do you account for its similarity in music and linguistics?   Selected readings "'Sound' at the center, 'horn' at the periphery: the shawm and its eastern cousins, part 2" (11/19/22) "The shawm and its eastern cousins" (11/16/15) "Allergese" (4/30/15) "Drawl from all over? (9/20/12)]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Canmore Xing [jatp]]]></title>
        <id>http://xianblog.wordpress.com/?p=53806</id>
        <link href="https://xianblog.wordpress.com/2023/08/18/canmore-xing-jatp/"/>
        <updated>2023-08-17T22:23:32.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Max-affine regression via first-order methods]]></title>
        <id>http://arxiv.org/abs/2308.08070</id>
        <link href="http://arxiv.org/abs/2308.08070"/>
        <updated>2023-08-17T21:11:57.321Z</updated>
        <summary type="html"><![CDATA[Seonho Kim, Kiryung Lee]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple online learning with consistency oracle]]></title>
        <id>http://arxiv.org/abs/2308.08055</id>
        <link href="http://arxiv.org/abs/2308.08055"/>
        <updated>2023-08-17T21:11:56.583Z</updated>
        <summary type="html"><![CDATA[Alexander Kozachinskiy, Tomasz Steifer]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Data Generated by Gaussian Mixture Models Using Deep
  ReLU Networks]]></title>
        <id>http://arxiv.org/abs/2308.08030</id>
        <link href="http://arxiv.org/abs/2308.08030"/>
        <updated>2023-08-17T21:11:56.566Z</updated>
        <summary type="html"><![CDATA[Tian-Yi Zhou, Xiaoming Huo]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Ability of Interpolating Deep Convolutional Neural Networks]]></title>
        <id>http://arxiv.org/abs/2210.14184</id>
        <link href="http://arxiv.org/abs/2210.14184"/>
        <updated>2023-08-17T21:11:56.561Z</updated>
        <summary type="html"><![CDATA[Tian-Yi Zhou, Xiaoming Huo]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural network pruning with combinatorial optimization]]></title>
        <id>http://blog.research.google/2023/08/neural-network-pruning-with.html</id>
        <link href="http://blog.research.google/2023/08/neural-network-pruning-with.html"/>
        <updated>2023-08-17T18:08:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Hussein Hazimeh, Research Scientist, Athena Team, and Riade Benbaki, Graduate Student at MIT





Modern neural networks have achieved impressive performance across a variety of applications, such as language, mathematical reasoning, and vision. However, these networks often use large architectures that require lots of computational resources. This can make it impractical to serve such models to users, especially in resource-constrained environments like wearables and smartphones. A widely used approach to mitigate the inference costs of pre-trained networks is to prune them by removing some of their weights, in a way that doesn’t significantly affect utility. In standard neural networks, each weight defines a connection between two neurons. So after weights are pruned, the input…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural network pruning with combinatorial optimization]]></title>
        <id>http://ai.googleblog.com/2023/08/neural-network-pruning-with.html</id>
        <link href="http://ai.googleblog.com/2023/08/neural-network-pruning-with.html"/>
        <updated>2023-08-17T18:08:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Hussein Hazimeh, Research Scientist, Athena Team, and Riade Benbaki, Graduate Student at MIT





Modern neural networks have achieved impressive performance across a variety of applications, such as language, mathematical reasoning, and vision. However, these networks often use large architectures that require lots of computational resources. This can make it impractical to serve such models to users, especially in resource-constrained environments like wearables and smartphones. A widely used approach to mitigate the inference costs of pre-trained networks is to prune them by removing some of their weights, in a way that doesn’t significantly affect utility. In standard neural networks, each weight defines a connection between two neurons. So after weights are pruned, the input…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pork Lion Bone]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60183</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60183&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=pork-lion-bone"/>
        <updated>2023-08-17T15:12:12.000Z</updated>
        <summary type="html"><![CDATA[Seen by François Lang at the meat counter at The Great Wall in Rockville, MD: François remarks:  "It took me a minute to realize that the sign meant 'Loin Bone'!" The Chinese characters say: lónggǔ 龍骨 "dragon bone" In different contexts, that can also mean "keel (beam along the underside of a ship’s hull); fossil bones […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teaching Taiwanese in France]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60198</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60198&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=teaching-taiwanese-in-france"/>
        <updated>2023-08-17T13:07:58.000Z</updated>
        <summary type="html"><![CDATA[Taiwanese may be fading in Taiwan (see "Selected Readings" below; except for foreign diplomats and the like!), but in France it is thriving: "Language of our own: Fun Taiwanese classes gain popularity in France" By Tseng Ting-hsuan and James Lo, Focus Taiwan (8/10/2023) In a classroom in Paris, Taiwan's top envoy to France François Wu […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reflections on “Making the Atomic Bomb”]]></title>
        <id>http://windowsontheory.org/?p=8674</id>
        <link href="https://windowsontheory.org/2023/08/16/reflections-on-making-the-atomic-bomb/"/>
        <updated>2023-08-17T02:47:58.000Z</updated>
        <summary type="html"><![CDATA[[Cross posted on lesswrong; see here for my prior writings]  [it appears almost certain that in the immediate future, it would be] possible to set up a nuclear chain reaction in a large mass of uranium by which vast amounts of power and large quantities of new radium-like elements would be generated. Letter from Albert Einstein (prepared by … Continue reading Reflections on “Making the Atomic Bomb”]]></summary>
        <author>
            <name>Boaz Barak</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reflections on “Making the Atomic Bomb”]]></title>
        <id>http://windowsontheory.org/?p=8674</id>
        <link href="https://windowsontheory.org/2023/08/16/reflections-on-making-the-atomic-bomb/"/>
        <updated>2023-08-17T02:47:58.000Z</updated>
        <summary type="html"><![CDATA[[Cross posted on lesswrong; see here for my prior writings]  [it appears almost certain that in the immediate future, it would be] possible to set up a nuclear chain reaction in a large mass of uranium by which vast amounts of power and large quantities of new radium-like elements would be generated. Letter from Albert Einstein (prepared by … Continue reading Reflections on “Making the Atomic Bomb”]]></summary>
        <author>
            <name>Boaz Barak</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mike’s obituary in the IMS Bulletin]]></title>
        <id>http://xianblog.wordpress.com/?p=53703</id>
        <link href="https://xianblog.wordpress.com/2023/08/17/mikes-obituary-in-the-ims-bulletin/"/>
        <updated>2023-08-16T22:23:17.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stan Carey on "greenlit"]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60190</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60190&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=stan-carey-on-greenlit"/>
        <updated>2023-08-16T12:17:49.000Z</updated>
        <summary type="html"><![CDATA[From Stan Carey at Sentence First, a lucid and deeply empirical dive into the question "Has ‘greenlit’ been greenlighted?". Stan explores a wide variety of relevant corpus data, both historical and otherwise, providing a lovely illustration of how accessible such resources have become. And at the end, one of his commenters points him to an […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[misty Grassy Bay [jatp]]]></title>
        <id>http://xianblog.wordpress.com/?p=53800</id>
        <link href="https://xianblog.wordpress.com/2023/08/16/misty-grassy-bay-jatp/"/>
        <updated>2023-08-15T22:23:39.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STUDY: Socially aware temporally causal decoder recommender systems]]></title>
        <id>http://blog.research.google/2023/08/study-socially-aware-temporally-causal.html</id>
        <link href="http://blog.research.google/2023/08/study-socially-aware-temporally-causal.html"/>
        <updated>2023-08-15T19:59:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Eltayeb Ahmed, Research Engineer, and Subhrajit Roy, Senior Research Scientist, Google Research




Reading has many benefits for young students, such as better linguistic and life skills, and reading for pleasure has been shown to correlate with academic success. Furthermore students have reported improved emotional wellbeing from reading, as well as better general knowledge and better understanding of other cultures. With the vast amount of reading material both online and off, finding age-appropriate, relevant and engaging content can be a challenging task, but helping students do so is a necessary step to engage them in reading. Effective recommendations that present students with relevant reading material helps keep students reading, and this is where machine learning (ML) c…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STUDY: Socially aware temporally causal decoder recommender systems]]></title>
        <id>http://ai.googleblog.com/2023/08/study-socially-aware-temporally-causal.html</id>
        <link href="http://ai.googleblog.com/2023/08/study-socially-aware-temporally-causal.html"/>
        <updated>2023-08-15T19:59:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Eltayeb Ahmed, Research Engineer, and Subhrajit Roy, Senior Research Scientist, Google Research




Reading has many benefits for young students, such as better linguistic and life skills, and reading for pleasure has been shown to correlate with academic success. Furthermore students have reported improved emotional wellbeing from reading, as well as better general knowledge and better understanding of other cultures. With the vast amount of reading material both online and off, finding age-appropriate, relevant and engaging content can be a challenging task, but helping students do so is a necessary step to engage them in reading. Effective recommendations that present students with relevant reading material helps keep students reading, and this is where machine learning (ML) c…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bilingual wordplay on a Taipei sign]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60178</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60178&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=bilingual-wordplay-on-a-taipei-sign"/>
        <updated>2023-08-15T12:56:42.000Z</updated>
        <summary type="html"><![CDATA[From Tom Mazanac: I came across this sign on the subway recently: Tom explains: It's an ad for the waterpark (which my children love), but what struck me most was the bilingual wordplay in the middle: 夏天Want to 水, above which is written "1 2 3", which I think is supposed to sound like "want to […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Analysis of Edge Elimination for Euclidean TSP]]></title>
        <id>http://arxiv.org/abs/1809.10469</id>
        <link href="http://arxiv.org/abs/1809.10469"/>
        <updated>2023-08-15T03:14:32.337Z</updated>
        <summary type="html"><![CDATA[Xianghui Zhong]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothing finite group actions on three-manifolds]]></title>
        <id>http://arxiv.org/abs/1901.11127</id>
        <link href="http://arxiv.org/abs/1901.11127"/>
        <updated>2023-08-15T03:14:32.317Z</updated>
        <summary type="html"><![CDATA[John Pardon]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Generalised Colouring Numbers of Graphs that Exclude a Fixed
  Minor]]></title>
        <id>http://arxiv.org/abs/1602.09052</id>
        <link href="http://arxiv.org/abs/1602.09052"/>
        <updated>2023-08-15T03:14:32.300Z</updated>
        <summary type="html"><![CDATA[Jan van den Heuvel, Patrice Ossona de Mendez, Daniel Quiroz, Roman
  Rabinovich, Sebastian Siebertz]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proper equivariant stable homotopy theory]]></title>
        <id>http://arxiv.org/abs/1908.00779</id>
        <link href="http://arxiv.org/abs/1908.00779"/>
        <updated>2023-08-15T03:14:32.294Z</updated>
        <summary type="html"><![CDATA[Dieter Degrijse, Markus Hausmann, Wolfgang L\"uck, Irakli Patchkoria,
  Stefan Schwede]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The eight Cayley-Dickson doubling products]]></title>
        <id>http://arxiv.org/abs/1707.07318</id>
        <link href="http://arxiv.org/abs/1707.07318"/>
        <updated>2023-08-15T03:14:32.272Z</updated>
        <summary type="html"><![CDATA[John W. Bales]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximation and Non-parametric Estimation of ResNet-type Convolutional
  Neural Networks]]></title>
        <id>http://arxiv.org/abs/1903.10047</id>
        <link href="http://arxiv.org/abs/1903.10047"/>
        <updated>2023-08-15T02:18:45.906Z</updated>
        <summary type="html"><![CDATA[Kenta Oono, Taiji Suzuki]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAC-Bayesian Generalization Bounds for Adversarial Generative Models]]></title>
        <id>http://arxiv.org/abs/2302.08942</id>
        <link href="http://arxiv.org/abs/2302.08942"/>
        <updated>2023-08-15T02:18:45.900Z</updated>
        <summary type="html"><![CDATA[Sokhna Diarra Mbacke, Florence Clerc, Pascal Germain]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of
  Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation]]></title>
        <id>http://arxiv.org/abs/2308.06422</id>
        <link href="http://arxiv.org/abs/2308.06422"/>
        <updated>2023-08-15T02:18:45.879Z</updated>
        <summary type="html"><![CDATA[Seyedarmin Azizi, Mahdi Nazemi, Arash Fayyazi, Massoud Pedram]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via
  Mixed-Effect Models and Hierarchical Clustering]]></title>
        <id>http://arxiv.org/abs/2308.06399</id>
        <link href="http://arxiv.org/abs/2308.06399"/>
        <updated>2023-08-15T02:18:45.872Z</updated>
        <summary type="html"><![CDATA[Lorenzo Vallegi, Marco Scutari, Federico Mattia Stefanini]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prince Rupert murals]]></title>
        <id>http://xianblog.wordpress.com/?p=53793</id>
        <link href="https://xianblog.wordpress.com/2023/08/15/prince-rupert-murals/"/>
        <updated>2023-08-14T22:23:08.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The new, enhanced / advanced quiet luxury language of pèihuò 配货]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60159</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60159&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-new-enhanced-advanced-quiet-luxury-language-of-peihuo-%25e9%2585%258d%25e8%25b4%25a7"/>
        <updated>2023-08-14T17:57:54.000Z</updated>
        <summary type="html"><![CDATA["Pèihuò 配货" is not a new term to me.  I knew it quite a while ago as it is used in supply chain studies with the meaning of "distribution; prepare goods for delivery according to an order"), as in the expression "pèihuò zhōngxīn 配货中心" ("distribution center").  Now, though, it has morphed into something altogether different. […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DARPA/Dartmouth one/won …]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60168</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60168&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=darpa-dartmouth-one-won"/>
        <updated>2023-08-14T11:17:31.000Z</updated>
        <summary type="html"><![CDATA[Despite the evidence of my most recent relevant post, the best current speech-to-text systems still make mistakes that a literate and informed human wouldn't. In this recent YouTube video on the history of robotics research, the automatic closed-captioning system renders "DARPA" as "Dartmouth": The audio is clear enough: Your browser does not support the audio […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer升级之路：13、逆用Leaky ReRoPE]]></title>
        <id>https://kexue.fm/archives/9728</id>
        <link href="https://kexue.fm/archives/9728"/>
        <updated>2023-08-14T03:53:00.000Z</updated>
        <summary type="html"><![CDATA[上周在《Transformer升级之路：12、无限外推的ReRoPE？》中，笔者提出了ReRoPE和Leaky ReRoPE，诸多实验结果表明，它们能够在几乎不损失训练效果的情况下免微调地扩展L...]]></summary>
        <author>
            <name>苏剑林</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Application of Artificial Neural Networks for Investigation of Pressure
  Filtration Performance, a Zinc Leaching Filter Cake Moisture Modeling]]></title>
        <id>http://arxiv.org/abs/2308.06138</id>
        <link href="http://arxiv.org/abs/2308.06138"/>
        <updated>2023-08-14T02:19:10.155Z</updated>
        <summary type="html"><![CDATA[Masoume Kazemi, Davood Moradkhani, Alireza A. Alipour]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trained Transformers Learn Linear Models In-Context]]></title>
        <id>http://arxiv.org/abs/2306.09927</id>
        <link href="http://arxiv.org/abs/2306.09927"/>
        <updated>2023-08-14T02:19:10.147Z</updated>
        <summary type="html"><![CDATA[Ruiqi Zhang, Spencer Frei, Peter L. Bartlett]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative Learning with a Drone Orchestrator]]></title>
        <id>http://arxiv.org/abs/2303.02266</id>
        <link href="http://arxiv.org/abs/2303.02266"/>
        <updated>2023-08-14T02:19:10.125Z</updated>
        <summary type="html"><![CDATA[Mahdi Boloursaz Mashhadi, Mahnoosh Mahdavimoghadam, Rahim Tafazolli,
  Walid Saad]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Law of Data Separation in Deep Learning]]></title>
        <id>http://arxiv.org/abs/2210.17020</id>
        <link href="http://arxiv.org/abs/2210.17020"/>
        <updated>2023-08-14T02:19:10.119Z</updated>
        <summary type="html"><![CDATA[Hangfeng He, Weijie J. Su]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[a storm is coming [jatp]]]></title>
        <id>http://xianblog.wordpress.com/?p=53787</id>
        <link href="https://xianblog.wordpress.com/2023/08/14/a-storm-is-coming-jatp/"/>
        <updated>2023-08-13T22:23:40.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language and politics: The use of English "OR" in Chinese official propaganda]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60134</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60134&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=language-and-politics-the-use-of-english-or-in-chinese-official-propaganda"/>
        <updated>2023-08-13T15:37:15.000Z</updated>
        <summary type="html"><![CDATA[From the weibo of People's Daily  (Rénmín rìbào 人民日報): Here are all four pairs of questions separated by English "OR": 1. jūnshì àihàozhě ？OR jiāndié guāncháshào ？ 军事爱好者？ OR 间谍观察哨？ "Military enthusiasts? OR Spy observation sentinel?" 2. wǎngluò jiāoyǒu ？OR wǎngluò gōulián ? 网络交友？ OR 网络勾连？ "Online dating / friend-making? OR Online colluding?" 3. xuéshù jiāoliú ？OR qièmì […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elevator etiquette and rules (lots of 'em)]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60113</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60113&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=elevator-etiquette-and-rules-lots-of-em"/>
        <updated>2023-08-13T15:32:32.000Z</updated>
        <summary type="html"><![CDATA[On the inside (N.B.) doors of a lift in Wuhan (yes that [in]famous Wuhan): I will first romanize, transcribe, and translate all six of the signs, from top left to bottom right, then I will provide annotations and explanations: 1. yánjìn pāidǎ 严禁拍打 "slapping / swatting / flapping / patting / tapping / thwacking / beating / […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Irish accents]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60156</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60156&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=irish-accents"/>
        <updated>2023-08-13T13:15:17.000Z</updated>
        <summary type="html"><![CDATA["Lost in translation — navigating accents in a changing world" Joe Horgan, Irish Post (8/7/23) An engaging story: WHEN I first started associating with English people I had to translate when my father spoke to them. I’d grown up in a very large Irish community in an immigrant area in an English city and it […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structural and Optical Properties of Spin-Coated Mn$_3$O$_4$ Thin Films
  of Different Coating Layers]]></title>
        <id>http://arxiv.org/abs/2308.05728</id>
        <link href="http://arxiv.org/abs/2308.05728"/>
        <updated>2023-08-13T01:04:49.952Z</updated>
        <summary type="html"><![CDATA[Vidit Pandey, Mohd Salman Siddiqui, Sandeep Munjal, Tufail Ahmad]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preemptive Detection of Fake Accounts on Social Networks via Multi-Class
  Preferential Attachment Classifiers]]></title>
        <id>http://arxiv.org/abs/2308.05353</id>
        <link href="http://arxiv.org/abs/2308.05353"/>
        <updated>2023-08-13T01:04:49.930Z</updated>
        <summary type="html"><![CDATA[Adam Breuer, Nazanin Khosravani, Michael Tingley, Bradford Cottel]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gauged $SU(3)_F$ and loop induced quark and lepton masses]]></title>
        <id>http://arxiv.org/abs/2308.05642</id>
        <link href="http://arxiv.org/abs/2308.05642"/>
        <updated>2023-08-13T01:04:49.923Z</updated>
        <summary type="html"><![CDATA[Gurucharan Mohanta, Ketan M. Patel]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[classic postcard [jatp]]]></title>
        <id>http://xianblog.wordpress.com/?p=53769</id>
        <link href="https://xianblog.wordpress.com/2023/08/13/classic-postcard-jatp/"/>
        <updated>2023-08-12T22:23:02.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text orientation ambiguity]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60150</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60150&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=text-orientation-ambiguity"/>
        <updated>2023-08-12T19:47:25.000Z</updated>
        <summary type="html"><![CDATA[Perhaps Victor can point us to an analogous ambiguity in Chinese poetico-political history: The source is this tweet. (oO this X?). Although I don't think I've seen it before, this pattern should be pretty common: BAD NAME1 NAME2 GOOD]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More on LLMs' current problem-solving abilities]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60141</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60141&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=more-on-llms-current-problem-solving-abilities"/>
        <updated>2023-08-12T16:46:45.000Z</updated>
        <summary type="html"><![CDATA[It's hard to keep up with the waves of hype and anti-hype in the LLM space these days. Here's something from a few weeks ago that I missed — Xiaoxuan Wang et al., "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models", arxiv.org 7/20/2023: Abstract: Recent advances in large language models (LLMs) have demonstrated […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Voilà!]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60120</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60120&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=voila"/>
        <updated>2023-08-12T14:38:39.000Z</updated>
        <summary type="html"><![CDATA[I've always been fond of this pretty, little word, but I seldom use it in my own speech (maybe once every five or ten years), because it seems too triumphant.  This morning, however, after a long, numerical list of steps that some colleagues and I need to take, followed by a conclusion we wished to […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Golden dawn [jatp]]]></title>
        <id>http://xianblog.wordpress.com/?p=53760</id>
        <link href="https://xianblog.wordpress.com/2023/08/12/golden-dawn-jatp/"/>
        <updated>2023-08-11T22:23:46.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The state of speech-to-text]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60122</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60122&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-state-of-speech-to-text"/>
        <updated>2023-08-11T21:05:39.000Z</updated>
        <summary type="html"><![CDATA[…if you haven't noticed, is good. There are many applications, from conversing with Siri and Alexa and Google Assistant, to getting voicemail in textual form, to automatically generated subtitles, and so on. For linguists, one parochial (but important) application is accurate automatic transcription of speech corpora, and the example that motivates this post comes from […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cheerful news in difficult times: David Conlon and Jeck Lim settled Kupitz’s planar discrepancy problem for pseudolines.]]></title>
        <id>http://gilkalai.wordpress.com/?p=24665</id>
        <link href="https://gilkalai.wordpress.com/2023/08/11/cheerful-news-in-difficult-times-david-conlon-and-jeck-lim-settled-kupitzs-planar-discrepancy-problem-for-pseudolines/"/>
        <updated>2023-08-11T10:56:48.000Z</updated>
        <summary type="html"><![CDATA[Small updates. I wrote a post about the overwhelming mathematical activities in the lat week of April 2023 and the following weeks were as exciting. It was difficult to follow all the activities (not to mention blogging about them). Let … Continue reading →]]></summary>
        <author>
            <name>Gil Kalai</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cheerful news in difficult times: David Conlon and Jeck Lim settled Kupitz’s planar discrepancy problem for pseudolines.]]></title>
        <id>http://gilkalai.wordpress.com/?p=24665</id>
        <link href="https://gilkalai.wordpress.com/2023/08/11/cheerful-news-in-difficult-times-david-conlon-and-jeck-lim-settled-kupitzs-planar-discrepancy-problem-for-pseudolines/"/>
        <updated>2023-08-11T10:56:48.000Z</updated>
        <summary type="html"><![CDATA[Small updates. I wrote a post about the overwhelming mathematical activities in the lat week of April 2023 and the following weeks were as exciting. It was difficult to follow all the activities (not to mention blogging about them). Let … Continue reading →]]></summary>
        <author>
            <name>Gil Kalai</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning ground states of gapped quantum Hamiltonians with Kernel
  Methods]]></title>
        <id>http://arxiv.org/abs/2303.08902</id>
        <link href="http://arxiv.org/abs/2303.08902"/>
        <updated>2023-08-11T02:18:06.059Z</updated>
        <summary type="html"><![CDATA[Clemens Giuliani, Filippo Vicentini, Riccardo Rossi, Giuseppe Carleo]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Networks for Scalar Input and Functional Output]]></title>
        <id>http://arxiv.org/abs/2208.05776</id>
        <link href="http://arxiv.org/abs/2208.05776"/>
        <updated>2023-08-11T02:18:05.885Z</updated>
        <summary type="html"><![CDATA[Sidi Wu, C\'edric Beaulac, Jiguo Cao]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Updating Clinical Risk Stratification Models Using Rank-Based
  Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team
  Performance]]></title>
        <id>http://arxiv.org/abs/2308.05619</id>
        <link href="http://arxiv.org/abs/2308.05619"/>
        <updated>2023-08-11T02:18:05.874Z</updated>
        <summary type="html"><![CDATA[Erkin \"Otle\c{s}, Brian T. Denton, Jenna Wiens]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ta Mother Noodle]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60106</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60106&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=ta-mother-noodle"/>
        <updated>2023-08-10T23:25:27.000Z</updated>
        <summary type="html"><![CDATA[Sign on a noodle shop in Xindan, Taiwan: (Via Google Street View) Mark Swofford has written a nice post about this on Pinyin News:  "Mother-effing noodles". As anyone who has studied more than a few months of Chinese or is a regular Language Log reader knows, "his mother's" (tāmā de) is China's "national swear" (see […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[wood and orphan bears]]></title>
        <id>http://xianblog.wordpress.com/?p=53780</id>
        <link href="https://xianblog.wordpress.com/2023/08/11/wood-and-orphan-bears/"/>
        <updated>2023-08-10T22:23:35.000Z</updated>
        <summary type="html"><![CDATA[On the trail to Burgess Pass,we passed this wooden structure that had something bearish in it (at least in my short-sighted view!). And the day after, saw this 21 year old grizzly bear in an orphan bear refuge where he had spent most of his life after his mother was killed by a hunter. Magnificent […]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MLIR — Using Tablegen for Passes]]></title>
        <id>https://jeremykun.com/?p=119631</id>
        <link href="https://jeremykun.com/2023/08/10/mlir-using-tablegen-for-passes/"/>
        <updated>2023-08-10T21:41:45.000Z</updated>
        <summary type="html"><![CDATA[Table of Contents In the last article in this series, we defined some custom lowering passes that modified an MLIR program. Notably, we accomplished that by implementing the required interfaces of the MLIR API directly. This is not the way that most MLIR developers work. Instead, they use a code generation tool called tablegen to […]]]></summary>
        <author>
            <name>Jeremy Kun</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MLIR — Writing Our First Pass]]></title>
        <id>https://jeremykun.com/?p=119490</id>
        <link href="https://jeremykun.com/2023/08/10/mlir-writing-our-first-pass/"/>
        <updated>2023-08-10T21:39:42.000Z</updated>
        <summary type="html"><![CDATA[Table of Contents This series is an introduction to MLIR and an onboarding tutorial for the HEIR project. Last time we saw how to run and test a basic lowering. This time we will write some simple passes to illustrate the various parts of the MLIR API and the pass infrastructure. As mentioned previously, the […]]]></summary>
        <author>
            <name>Jeremy Kun</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MLIR — Running and Testing a Lowering]]></title>
        <id>https://jeremykun.com/?p=119430</id>
        <link href="https://jeremykun.com/2023/08/10/mlir-running-and-testing-a-lowering/"/>
        <updated>2023-08-10T21:36:13.000Z</updated>
        <summary type="html"><![CDATA[Table of Contents Last time, we covered a Bazel build system setup for an MLIR project. This time we’ll give an overview of a simple lowering and show how end-to-end tests work in MLIR. All of the code for this article is contained in this pull request on GitHub, and the commits are nicely organized […]]]></summary>
        <author>
            <name>Jeremy Kun</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MLIR — Getting Started]]></title>
        <id>https://jeremykun.com/?p=119363</id>
        <link href="https://jeremykun.com/2023/08/10/mlir-getting-started/"/>
        <updated>2023-08-10T21:32:33.000Z</updated>
        <summary type="html"><![CDATA[Table of Contents As we announced recently, my team at Google has started a new effort to build production-worthy engineering tools for Fully Homomorphic Encryption (FHE). One focal point of this, and one which I’ll be focusing on as long as Google is willing to pay me to do so, is building out a compiler […]]]></summary>
        <author>
            <name>Jeremy Kun</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chinese and Japanese Terms for Food Textures]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60110</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60110&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=chinese-and-japanese-terms-for-food-textures"/>
        <updated>2023-08-10T20:19:31.000Z</updated>
        <summary type="html"><![CDATA[Catching up on some oldish e-mail, I came upon this interesting one from Francois Lang dated 5/9/23: According to an article in yesterday's NYT, "A 2008 report in the Journal of Texture Studies lists 144 Chinese terms for food texture".   The NYT article also says "In Japan, such terms number more than 400. 'Too […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google’s Recent FHE work, and starting HEIR]]></title>
        <id>https://jeremykun.com/?p=119694</id>
        <link href="https://jeremykun.com/2023/08/10/googles-recent-fhe-work-and-starting-heir/"/>
        <updated>2023-08-10T19:22:29.000Z</updated>
        <summary type="html"><![CDATA[Today my team at Google published an article on Google’s Developers Blog with some updates on what we’ve been doing with fully homomorphic encryption (FHE). There’s fun stuff in there, including work on video processing FHE, compiling ML models to FHE, an FHE implementation for TPUs, and improvements to the compiler I wrote about earlier […]]]></summary>
        <author>
            <name>Jeremy Kun</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PIE *gene- *gwen-]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60080</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60080&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=pie-gene-gwen"/>
        <updated>2023-08-10T08:40:20.000Z</updated>
        <summary type="html"><![CDATA[I asked several Indo-Europeanist colleagues: In Hittite, Tocharian, Indo-Iranian (Indic and Persian), Greek, Albanian, Germanic, Armenian, Celtic, Anatolian, Italic, Lithuanian, Balto-Slavic, Macedonian, Phrygian, and other IE languages, do you ever find reflexes (derivatives) of these two PIE roots in close association / linkage with each other? PIE root *gene- "give birth, beget," with derivatives referring […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intel Joins the PyTorch Foundation as a Premier Member]]></title>
        <id>https://pytorch.org/blog/intel-joins-pytorch/</id>
        <link href="https://pytorch.org/blog/intel-joins-pytorch/"/>
        <updated>2023-08-10T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[<div type="html"/>]]></summary>
        <author>
            <name>PyTorch Website</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SLEM: Machine Learning for Path Modeling and Causal Inference with Super
  Learner Equation Modeling]]></title>
        <id>http://arxiv.org/abs/2308.04365</id>
        <link href="http://arxiv.org/abs/2308.04365"/>
        <updated>2023-08-10T01:16:17.288Z</updated>
        <summary type="html"><![CDATA[Matthew J. Vowels]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Doubly Robust Estimator for Off-Policy Evaluation with Large Action
  Spaces]]></title>
        <id>http://arxiv.org/abs/2308.03443</id>
        <link href="http://arxiv.org/abs/2308.03443"/>
        <updated>2023-08-10T01:16:16.944Z</updated>
        <summary type="html"><![CDATA[Tatsuhiro Shimizu, Laura Forastiere]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Site-Agnostic Multimodal Deep Learning Model to Identify
  Pro-Eating Disorder Content on Social Media]]></title>
        <id>http://arxiv.org/abs/2307.06775</id>
        <link href="http://arxiv.org/abs/2307.06775"/>
        <updated>2023-08-10T01:16:16.712Z</updated>
        <summary type="html"><![CDATA[Jonathan Feldman]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Critical Review of Physics-Informed Machine Learning Applications in
  Subsurface Energy Systems]]></title>
        <id>http://arxiv.org/abs/2308.04457</id>
        <link href="http://arxiv.org/abs/2308.04457"/>
        <updated>2023-08-10T01:16:16.706Z</updated>
        <summary type="html"><![CDATA[Abdeldjalil Latrach, Mohamed Lamine Malki, Misael Morales, Mohamed
  Mehana, Minou Rabiei]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[trở về từ thành phố hồ chí minh]]></title>
        <id>http://xianblog.wordpress.com/?p=53586</id>
        <link href="https://xianblog.wordpress.com/2023/08/10/tro-ve-tu-thanh-pho-ho-chi-minh/"/>
        <updated>2023-08-09T22:23:27.000Z</updated>
        <summary type="html"><![CDATA[While fast and furious (!), the days I spent in Ho Chi Minh City (at UEH, the university of economics of HCMC) were most enjoyable, from running very early in the relatively cool morning, by the nearby canal, often with others, to the group breakfast at a small food stall, or another, with a huge […]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Multi-hyphenate spaces"]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60101</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60101&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=multi-hyphenate-spaces"/>
        <updated>2023-08-09T20:36:53.000Z</updated>
        <summary type="html"><![CDATA[Alex Bauman sent in this real-estate ad from Singapore: For the fully hyper-hyphenated experience, click here… I've managed somehow to live many decades in this world without ever registering the existence of the word multi-hyphenate. In fairness to me, it hasn't had its OED Word Induction Ceremony yet. And Merriam-Websters is similarly uninformed. But Wiktionary […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advances in document understanding]]></title>
        <id>http://blog.research.google/2023/08/advances-in-document-understanding.html</id>
        <link href="http://blog.research.google/2023/08/advances-in-document-understanding.html"/>
        <updated>2023-08-09T18:32:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Sandeep Tata, Software Engineer, Google Research, Athena Team




The last few years have seen rapid progress in systems that can automatically process complex business documents and turn them into structured objects. A system that can automatically extract data from documents, e.g., receipts, insurance quotes, and financial statements, has the potential to dramatically improve the efficiency of business workflows by avoiding error-prone, manual work. Recent models, based on the Transformer architecture, have shown impressive gains in accuracy. Larger models, such as PaLM 2, are also being leveraged to further streamline these business workflows. However, the datasets used in academic literature fail to capture the challenges seen in real-world use cases. Consequently, academic b…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advances in document understanding]]></title>
        <id>http://ai.googleblog.com/2023/08/advances-in-document-understanding.html</id>
        <link href="http://ai.googleblog.com/2023/08/advances-in-document-understanding.html"/>
        <updated>2023-08-09T18:32:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Sandeep Tata, Software Engineer, Google Research, Athena Team




The last few years have seen rapid progress in systems that can automatically process complex business documents and turn them into structured objects. A system that can automatically extract data from documents, e.g., receipts, insurance quotes, and financial statements, has the potential to dramatically improve the efficiency of business workflows by avoiding error-prone, manual work. Recent models, based on the Transformer architecture, have shown impressive gains in accuracy. Larger models, such as PaLM 2, are also being leveraged to further streamline these business workflows. However, the datasets used in academic literature fail to capture the challenges seen in real-world use cases. Consequently, academic b…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Don't blindly save yourself"]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60068</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60068&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=dont-blindly-save-yourself"/>
        <updated>2023-08-09T05:03:33.000Z</updated>
        <summary type="html"><![CDATA[The following photo is from Guanghzhou and was taken recently by David Lobina's partner who’s there now.  The Chinese says: rúyǒu gùzhàng ， búyào mángmù zìjiù 如有故障，不要盲目自救 "If there is a malfunction, do not blindly rescue / save yourself" Though the second clause may initially strike one as being in error, it is actually a […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[It's so hard to say "goodbye" in Chinese]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60050</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60050&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=its-so-hard-to-say-goodbye-in-chinese"/>
        <updated>2023-08-09T05:00:17.000Z</updated>
        <summary type="html"><![CDATA[From a photo sharing group on Facebook: The Chinese says: fēnshǒu 分手 "separate; (bid) farewell; part(ing); part company; break up (with); split up; end a relationship") Cf. fēnshǒu xìn 分手信 ("break up letter; Dear John letter"). Selected readings "Luv u" (4/29/17) "'Waiving goodbye'" (9/21/20) "Goodbye to Hello" (10/3/22) "'Just ghost'" (7/6/20) "Have a good / […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inferring the finest pattern of mutual independence from data]]></title>
        <id>http://arxiv.org/abs/2306.12984</id>
        <link href="http://arxiv.org/abs/2306.12984"/>
        <updated>2023-08-09T02:29:07.753Z</updated>
        <summary type="html"><![CDATA[G. Marrelec, A. Giron]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shuffle SGD is Always Better than SGD: Improved Analysis of SGD with
  Arbitrary Data Orders]]></title>
        <id>http://arxiv.org/abs/2305.19259</id>
        <link href="http://arxiv.org/abs/2305.19259"/>
        <updated>2023-08-09T02:29:07.722Z</updated>
        <summary type="html"><![CDATA[Anastasia Koloskova, Nikita Doikov, Sebastian U. Stich, Martin Jaggi]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can We Trust Race Prediction?]]></title>
        <id>http://arxiv.org/abs/2307.08496</id>
        <link href="http://arxiv.org/abs/2307.08496"/>
        <updated>2023-08-09T02:29:07.710Z</updated>
        <summary type="html"><![CDATA[Cangyuan Li]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated extraction of mutual independence patterns using Bayesian
  comparison of partition models]]></title>
        <id>http://arxiv.org/abs/2001.05407</id>
        <link href="http://arxiv.org/abs/2001.05407"/>
        <updated>2023-08-09T01:15:34.313Z</updated>
        <summary type="html"><![CDATA[Guillaume Marrelec, Alain Giron]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical and Rigorous Uncertainty Bounds for Gaussian Process
  Regression]]></title>
        <id>http://arxiv.org/abs/2105.02796</id>
        <link href="http://arxiv.org/abs/2105.02796"/>
        <updated>2023-08-09T01:15:34.116Z</updated>
        <summary type="html"><![CDATA[Christian Fiedler, Carsten W. Scherer, Sebastian Trimpe]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Bayesian Networks with Annealing Machine]]></title>
        <id>http://arxiv.org/abs/2006.06926</id>
        <link href="http://arxiv.org/abs/2006.06926"/>
        <updated>2023-08-09T01:15:34.109Z</updated>
        <summary type="html"><![CDATA[Yuta Shikuri]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminator optimal transport]]></title>
        <id>http://arxiv.org/abs/1910.06832</id>
        <link href="http://arxiv.org/abs/1910.06832"/>
        <updated>2023-08-09T01:15:34.102Z</updated>
        <summary type="html"><![CDATA[Akinori Tanaka]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composite Goodness-of-fit Tests with Kernels]]></title>
        <id>http://arxiv.org/abs/2111.10275</id>
        <link href="http://arxiv.org/abs/2111.10275"/>
        <updated>2023-08-09T01:15:34.087Z</updated>
        <summary type="html"><![CDATA[Oscar Key, Arthur Gretton, Fran\c{c}ois-Xavier Briol, Tamara Fernandez]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[exact yet private MCMC]]></title>
        <id>http://xianblog.wordpress.com/?p=53674</id>
        <link href="https://xianblog.wordpress.com/2023/08/09/exact-yet-private-mcmc/"/>
        <updated>2023-08-08T22:23:46.000Z</updated>
        <summary type="html"><![CDATA[“at each iteration, DP-fast MH first samples a minibatch size and checks if it uses a minibatch of data or full-batch data. Then it checks whether to require additional Gaussian noise. If so, it will instantiate the Gaussian mechanism which adds Gaussian noise to the energy difference function. Finally, it chooses accept or reject θ′ […]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdaTape: Foundation model with adaptive computation and dynamic read-and-write]]></title>
        <id>http://blog.research.google/2023/08/adatape-foundation-model-with-adaptive.html</id>
        <link href="http://blog.research.google/2023/08/adatape-foundation-model-with-adaptive.html"/>
        <updated>2023-08-08T21:02:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Fuzhao Xue, Research Intern, and Mostafa Dehghani, Research Scientist, Google







Adaptive computation refers to the ability of a machine learning system to adjust its behavior in response to changes in the environment. While conventional neural networks have a fixed function and computation capacity, i.e., they spend the same number of FLOPs for processing different inputs, a model with adaptive and dynamic computation modulates the computational budget it dedicates to processing each input, depending on the complexity of the input.


Adaptive computation in neural networks is appealing for two key reasons. First, the mechanism that introduces adaptivity provides an inductive bias that can play a key role in solving some challenging tasks. For instance, enabling different num…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdaTape: Foundation model with adaptive computation and dynamic read-and-write]]></title>
        <id>http://ai.googleblog.com/2023/08/adatape-foundation-model-with-adaptive.html</id>
        <link href="http://ai.googleblog.com/2023/08/adatape-foundation-model-with-adaptive.html"/>
        <updated>2023-08-08T21:02:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Fuzhao Xue, Research Intern, and Mostafa Dehghani, Research Scientist, Google







Adaptive computation refers to the ability of a machine learning system to adjust its behavior in response to changes in the environment. While conventional neural networks have a fixed function and computation capacity, i.e., they spend the same number of FLOPs for processing different inputs, a model with adaptive and dynamic computation modulates the computational budget it dedicates to processing each input, depending on the complexity of the input.


Adaptive computation in neural networks is appealing for two key reasons. First, the mechanism that introduces adaptivity provides an inductive bias that can play a key role in solving some challenging tasks. For instance, enabling different num…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLMs can't reason?]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60081</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60081&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=llms-cant-reason"/>
        <updated>2023-08-08T18:27:54.000Z</updated>
        <summary type="html"><![CDATA[…though they often do a credible job of faking it.  An interesting (preprint) paper by Konstantine Arkoudas, "GPT-4 Can't Reason", brings the receipts. The abstract: GPT-4 was released in March 2023 to wide acclaim, marking a very substantial improvement across the board over GPT-3.5 (OpenAI’s previously best model, which had powered the initial release of […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Eyeworm]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60072</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60072&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=eyeworm"/>
        <updated>2023-08-08T18:05:52.000Z</updated>
        <summary type="html"><![CDATA[No, I'm not talking about the eye parasite called Loa loa (a filarial nematode), which is also called eyeworm.  I'm talking about an image that gets stuck in your brain the same way an earworm (also called brainworm, sticky music, or stuck song syndrome) gets stuck in your head.  We've talked about earworms a lot […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tackling the Curse of Dimensionality with Physics-Informed Neural
  Networks]]></title>
        <id>http://arxiv.org/abs/2307.12306</id>
        <link href="http://arxiv.org/abs/2307.12306"/>
        <updated>2023-08-08T02:28:56.881Z</updated>
        <summary type="html"><![CDATA[Zheyuan Hu, Khemraj Shukla, George Em Karniadakis, Kenji Kawaguchi]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Incentivize Information Acquisition: Proper Scoring Rules
  Meet Principal-Agent Model]]></title>
        <id>http://arxiv.org/abs/2303.08613</id>
        <link href="http://arxiv.org/abs/2303.08613"/>
        <updated>2023-08-08T02:28:56.865Z</updated>
        <summary type="html"><![CDATA[Siyu Chen, Jibang Wu, Yifan Wu, Zhuoran Yang]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Studying Large Language Model Generalization with Influence Functions]]></title>
        <id>http://arxiv.org/abs/2308.03296</id>
        <link href="http://arxiv.org/abs/2308.03296"/>
        <updated>2023-08-08T02:28:56.846Z</updated>
        <summary type="html"><![CDATA[Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin,
  Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez,
  Evan Hubinger, Kamil\.e Luko\v{s}i\=ut\.e, Karina Nguyen, Nicholas Joseph,
  Sam McCandlish, Jared Kaplan, Samuel R. Bowman]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Fine-Tuning of Deep Neural Networks with Hessian-based
  Generalization Guarantees]]></title>
        <id>http://arxiv.org/abs/2206.02659</id>
        <link href="http://arxiv.org/abs/2206.02659"/>
        <updated>2023-08-08T02:28:56.839Z</updated>
        <summary type="html"><![CDATA[Haotian Ju, Dongyue Li, Hongyang R. Zhang]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SurvBeX: An explanation method of the machine learning survival models
  based on the Beran estimator]]></title>
        <id>http://arxiv.org/abs/2308.03730</id>
        <link href="http://arxiv.org/abs/2308.03730"/>
        <updated>2023-08-08T02:28:56.725Z</updated>
        <summary type="html"><![CDATA[Lev V. Utkin, Danila Y. Eremenko, Andrei V. Konstantinov]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[sunrise and moonset on Lake Okanagan [jatp]]]></title>
        <id>http://xianblog.wordpress.com/?p=53746</id>
        <link href="https://xianblog.wordpress.com/2023/08/08/sunrise-and-moonset-on-lake-okanagan-jatp/"/>
        <updated>2023-08-07T22:23:41.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autodidact]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60065</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60065&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=autodidact"/>
        <updated>2023-08-07T12:25:18.000Z</updated>
        <summary type="html"><![CDATA[[This is a guest post by Don Keyser in response to "Sinitic semiliteracy" (6/5/23)] This one takes me back.  In the late 1980s, I served my second assignment at our embassy in Tokyo.  The chief of the American Citizens Services unit in the Consular Section, a white lady in her early 50s, asked my assistance.  […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sergeant Dickason's Blend]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60058</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60058&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=sergeant-dickasons-blend"/>
        <updated>2023-08-07T10:43:28.000Z</updated>
        <summary type="html"><![CDATA[Over brunch the other day, a question came up that I've wondered about in the past: Who was the "Major Dickason" of Major Dickason's Blend? Skipping my imaginary histories, here's the real story. According to "A coffee revolution: Peet’s’ first ever employee spills the beans", Berkeleyside 7/25/2023: Long before the pandemic, the widespread adoption of […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[INT8 Quantization for x86 CPU in PyTorch]]></title>
        <id>https://pytorch.org/blog/int8-quantization/</id>
        <link href="https://pytorch.org/blog/int8-quantization/"/>
        <updated>2023-08-07T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[Overview]]></summary>
        <author>
            <name>PyTorch Website</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable PAC-Bayesian Meta-Learning via the PAC-Optimal Hyper-Posterior:
  From Theory to Practice]]></title>
        <id>http://arxiv.org/abs/2211.07206</id>
        <link href="http://arxiv.org/abs/2211.07206"/>
        <updated>2023-08-07T02:30:08.547Z</updated>
        <summary type="html"><![CDATA[Jonas Rothfuss, Martin Josifoski, Vincent Fortuin, Andreas Krause]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Multi-Task Learning with Recursive Least Squares and Recursive
  Kernel Methods]]></title>
        <id>http://arxiv.org/abs/2308.01938</id>
        <link href="http://arxiv.org/abs/2308.01938"/>
        <updated>2023-08-07T02:30:08.541Z</updated>
        <summary type="html"><![CDATA[Gabriel R. Lencione, Fernando J. Von Zuben]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Networks from Gaussian Graphical Models and Gaussian Free
  Fields]]></title>
        <id>http://arxiv.org/abs/2308.02344</id>
        <link href="http://arxiv.org/abs/2308.02344"/>
        <updated>2023-08-07T02:30:08.527Z</updated>
        <summary type="html"><![CDATA[Subhro Ghosh, Soumendu Sundar Mukherjee, Hoang-Son Tran, Ujan
  Gangopadhyay]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A stochastic optimization approach to train non-linear neural networks
  with regularization of higher-order total variation]]></title>
        <id>http://arxiv.org/abs/2308.02293</id>
        <link href="http://arxiv.org/abs/2308.02293"/>
        <updated>2023-08-07T02:30:08.521Z</updated>
        <summary type="html"><![CDATA[Akifumi Okuno]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Accurate Reduced-Order Modeling of a MOOSE-based Additive
  Manufacturing Model with Operator Learning]]></title>
        <id>http://arxiv.org/abs/2308.02462</id>
        <link href="http://arxiv.org/abs/2308.02462"/>
        <updated>2023-08-07T02:30:08.516Z</updated>
        <summary type="html"><![CDATA[Mahmoud Yaseen, Dewen Yushu, Peter German, Xu Wu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer升级之路：12、无限外推的ReRoPE？]]></title>
        <id>https://kexue.fm/archives/9708</id>
        <link href="https://kexue.fm/archives/9708"/>
        <updated>2023-08-07T02:08:00.000Z</updated>
        <summary type="html"><![CDATA[自从在《Transformer升级之路：11、将β进制位置进行到底》中引入混合进制的思路进一步推广了NTK-aware Scaled RoPE后，笔者感觉类似思路的效果已经达到了上限，想要更大幅...]]></summary>
        <author>
            <name>苏剑林</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[the unapologetic brilliance of Sinéad O’Connor]]></title>
        <id>http://xianblog.wordpress.com/?p=53649</id>
        <link href="https://xianblog.wordpress.com/2023/08/07/the-unapologetic-brilliance-of-sinead-oconnor/"/>
        <updated>2023-08-06T22:23:45.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cartesian Cafe podcast interviews me on cryptography]]></title>
        <id>http://windowsontheory.org/?p=8662</id>
        <link href="https://windowsontheory.org/2023/08/06/cartesian-cafe-podcast-interviews-me-on-cryptography/"/>
        <updated>2023-08-06T09:29:39.000Z</updated>
        <summary type="html"><![CDATA[[Unrelated announcement: Yael Kalai, Ran Raz, Salil Vadhan, Nisheeth Vishnoi and I recently completed our survey of Avi Wigderson’s work for the volume on Abel prize winners. Given the breadth and depth of Avi’s work, our survey could only cover a small sample of it, but we still hope it can. be a useful resource … Continue reading Cartesian Cafe podcast interviews me on cryptography]]></summary>
        <author>
            <name>Boaz Barak</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cartesian Cafe podcast interviews me on cryptography]]></title>
        <id>http://windowsontheory.org/?p=8662</id>
        <link href="https://windowsontheory.org/2023/08/06/cartesian-cafe-podcast-interviews-me-on-cryptography/"/>
        <updated>2023-08-06T09:29:39.000Z</updated>
        <summary type="html"><![CDATA[[Unrelated announcement: Yael Kalai, Ran Raz, Salil Vadhan, Nisheeth Vishnoi and I recently completed our survey of Avi Wigderson’s work for the volume on Abel prize winners. Given the breadth and depth of Avi’s work, our survey could only cover a small sample of it, but we still hope it can. be a useful resource … Continue reading Cartesian Cafe podcast interviews me on cryptography]]></summary>
        <author>
            <name>Boaz Barak</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coherent control of a high-orbital hole in a semiconductor quantum dot]]></title>
        <id>http://arxiv.org/abs/2212.10749</id>
        <link href="http://arxiv.org/abs/2212.10749"/>
        <updated>2023-08-06T01:08:44.930Z</updated>
        <summary type="html"><![CDATA[Jun-Yong Yan, Chen Chen, Xiao-Dong Zhang, Yu-Tong Wang, Hans-Georg
  Babin, Andreas D. Wieck, Arne Ludwig, Yun Meng, Xiaolong Hu, Huali Duan,
  Wenchao Chen, Wei Fang, Moritz Cygorek, Xing Lin, Da-Wei Wang, Chao-Yuan Jin,
  Feng Liu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[position at the Technical University of Munich (TUM)]]></title>
        <id>http://xianblog.wordpress.com/?p=53743</id>
        <link href="https://xianblog.wordpress.com/2023/08/06/position-at-the-technical-university-of-munich-tum/"/>
        <updated>2023-08-05T22:23:52.000Z</updated>
        <summary type="html"><![CDATA[Mathias Drton from TUM pointed out this opening in his department: The Technical University of Munich (TUM) invites applications for the prestigious Rudolf Mößbauer Tenure Track Assistant Professorships to be appointed in spring 2024. The position is a W2 fixed-term (6 year) tenure-track professorship with the possibility for promotion to a tenured W3. Within its […]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Something for nothing]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60008</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60008&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=something-for-nothing"/>
        <updated>2023-08-05T04:42:08.000Z</updated>
        <summary type="html"><![CDATA[The following is a Facebook advertisement for how people from Hong Kong can readily gain permanent residency in Canada.  Check out the unusual Sinoglyph inside the red bubble. The disyllabic expression is báihāo 白薅, where bái 白 is a very common character meaning "white; blank; plain; gratuitous", while many people, even those who are moderately literate, are […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Skunk stunk]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60047</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60047&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=skunk-stunk"/>
        <updated>2023-08-05T01:19:07.000Z</updated>
        <summary type="html"><![CDATA[Two nights ago, it was raining heavily, with lightning and thunder every so often.  As I was peering out into the blackness of my backyard, all of a sudden, a bright light flashed on.  At first I thought it was lightning, but then I realized that someone or something had set off the light.  It […]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextual Integrity for Differential Privacy #4 [23w5106]]]></title>
        <id>http://xianblog.wordpress.com/?p=53733</id>
        <link href="https://xianblog.wordpress.com/2023/08/05/contextual-integrity-for-differential-privacy-4-23w5106/"/>
        <updated>2023-08-04T22:23:40.000Z</updated>
        <summary type="html"><![CDATA[Mostly short talks. First talk by Thomas Seinke (Google) on interpreting ε, with a side wondering of mine on the relation between exp(ε) and the uncertainty that comes with Monte Carlo outcome. Which may relate to this 2022 paper by Ruobin Gong. Second talk by Gautam Kamath (U Waterloo) on large language models under privacy […]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Just sayin']]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=60023</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=60023&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=just-sayin-2"/>
        <updated>2023-08-04T13:53:15.000Z</updated>
        <summary type="html"><![CDATA[The third verse of Ben Sidran's song Can We Talk (track 5 on the 2013 album Don't Cry For No Hipster) repeats the couplet "I'm not sayin'; I'm just sayin'": Your browser does not support the audio element. This reminded me of a LLOG Post of Yore: "Just sayin'", 1/11/2012, which tried to answer a […]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextual Integrity for Differential Privacy #3 [23w5106]]]></title>
        <id>http://xianblog.wordpress.com/?p=53718</id>
        <link href="https://xianblog.wordpress.com/2023/08/04/contextual-integrity-for-differential-privacy-3-23w5106/"/>
        <updated>2023-08-03T22:23:23.000Z</updated>
        <summary type="html"><![CDATA[Morning of diverse short talks. First talk by Bei Jiang (Edmonton) on locally processed privacy for quantile estimation, which relates very much to our ongoing research with Stan, who is starting his ERC funded PhD on privacy. Randomised response, in having a positive probability of replacing indicators in the empirical cdf by a random or […]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLMs4OL: Large Language Models for Ontology Learning]]></title>
        <id>http://arxiv.org/abs/2307.16648</id>
        <link href="http://arxiv.org/abs/2307.16648"/>
        <updated>2023-08-03T21:12:13.214Z</updated>
        <summary type="html"><![CDATA[Hamed Babaei Giglou, Jennifer D'Souza, S\"oren Auer]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Hierarchical Neural Networks using Hierarchical Softmax]]></title>
        <id>http://arxiv.org/abs/2308.01210</id>
        <link href="http://arxiv.org/abs/2308.01210"/>
        <updated>2023-08-03T21:12:13.193Z</updated>
        <summary type="html"><![CDATA[Jetze Schuurmans, Flavius Frasincar]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Emergency Dust-Free solution on-board International Space
  Station with Bi-GRU (AED-ISS)]]></title>
        <id>http://arxiv.org/abs/2210.08549</id>
        <link href="http://arxiv.org/abs/2210.08549"/>
        <updated>2023-08-03T21:12:13.187Z</updated>
        <summary type="html"><![CDATA[Po-Han Hou, Wei-Chih Lin, Hong-Chun Hou, Yu-Hao Huang, Jih-Hong Shue]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating the Robustness of Test Selection Methods for Deep Neural
  Networks]]></title>
        <id>http://arxiv.org/abs/2308.01314</id>
        <link href="http://arxiv.org/abs/2308.01314"/>
        <updated>2023-08-03T21:12:13.064Z</updated>
        <summary type="html"><![CDATA[Qiang Hu, Yuejun Guo, Xiaofei Xie, Maxime Cordy, Wei Ma, Mike
  Papadakis, Yves Le Traon]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal medical AI]]></title>
        <id>http://blog.research.google/2023/08/multimodal-medical-ai.html</id>
        <link href="http://blog.research.google/2023/08/multimodal-medical-ai.html"/>
        <updated>2023-08-03T18:24:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Greg Corrado, Head of Health AI, Google Research, and Yossi Matias, VP, Engineering and Research, Google Research






Medicine is an inherently multimodal discipline. When providing care, clinicians routinely interpret data from a wide range of modalities including medical images, clinical notes, lab tests, electronic health records, genomics, and more.  Over the last decade or so, AI systems have achieved expert-level performance on specific tasks within specific modalities — some AI systems processing CT scans, while others analyzing high magnification pathology slides, and still others hunting for rare genetic variations. The inputs to these systems tend to be complex data such as images, and they typically provide structured outputs, whether in the form of discrete grades o…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal medical AI]]></title>
        <id>http://ai.googleblog.com/2023/08/multimodal-medical-ai.html</id>
        <link href="http://ai.googleblog.com/2023/08/multimodal-medical-ai.html"/>
        <updated>2023-08-03T18:24:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Greg Corrado, Head of Health AI, Google Research, and Yossi Matias, VP, Engineering and Research, Google Research






Medicine is an inherently multimodal discipline. When providing care, clinicians routinely interpret data from a wide range of modalities including medical images, clinical notes, lab tests, electronic health records, genomics, and more.  Over the last decade or so, AI systems have achieved expert-level performance on specific tasks within specific modalities — some AI systems processing CT scans, while others analyzing high magnification pathology slides, and still others hunting for rare genetic variations. The inputs to these systems tend to be complex data such as images, and they typically provide structured outputs, whether in the form of discrete grades o…]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
</feed>