<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://Furyton.github.io/misc-feed/index.html</id>
    <title>osmos::feed</title>
    <updated>2023-05-01T09:15:05.802Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="https://Furyton.github.io/misc-feed/index.html"/>
    <link rel="self" href="https://Furyton.github.io/misc-feed/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[A feature selection method based on Shapley values robust to concept
  shift in regression]]></title>
        <id>http://arxiv.org/abs/2304.14774</id>
        <link href="http://arxiv.org/abs/2304.14774"/>
        <updated>2023-05-01T07:14:17.709Z</updated>
        <summary type="html"><![CDATA[Carlos Sebasti\'an, Carlos E. Gonz\'alez-Guill\'en]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimalistic Unsupervised Learning with the Sparse Manifold Transform]]></title>
        <id>http://arxiv.org/abs/2209.15261</id>
        <link href="http://arxiv.org/abs/2209.15261"/>
        <updated>2023-05-01T07:14:17.666Z</updated>
        <summary type="html"><![CDATA[Yubei Chen, Zeyu Yun, Yi Ma, Bruno Olshausen, Yann LeCun]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Learning of Large-Scale Neural Networks Behaves Like Convex
  Optimization]]></title>
        <id>http://arxiv.org/abs/1903.02140</id>
        <link href="http://arxiv.org/abs/1903.02140"/>
        <updated>2023-05-01T07:14:17.661Z</updated>
        <summary type="html"><![CDATA[Hui Jiang]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Stable and Scalable Method for Solving Initial Value PDEs with Neural
  Networks]]></title>
        <id>http://arxiv.org/abs/2304.14994</id>
        <link href="http://arxiv.org/abs/2304.14994"/>
        <updated>2023-05-01T07:14:17.647Z</updated>
        <summary type="html"><![CDATA[Marc Finzi, Andres Potapczynski, Matthew Choptuik, Andrew Gordon
  Wilson]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Neural Networks for Sequential Change-point Detection]]></title>
        <id>http://arxiv.org/abs/2210.17312</id>
        <link href="http://arxiv.org/abs/2210.17312"/>
        <updated>2023-05-01T07:14:17.642Z</updated>
        <summary type="html"><![CDATA[Junghwan Lee, Tingnan Gong, Xiuyuan Cheng, Yao Xie]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google at ICLR 2023]]></title>
        <id>http://ai.googleblog.com/2023/04/google-at-iclr-2023.html</id>
        <link href="http://ai.googleblog.com/2023/04/google-at-iclr-2023.html"/>
        <updated>2023-05-01T06:37:00.002Z</updated>
        <summary type="html"><![CDATA[Posted by Catherine Armato, Program Manager, Google




The Eleventh International Conference on Learning Representations (ICLR 2023) is being held this week as a hybrid event in Kigali, Rwanda. We are proud to be a Diamond Sponsor of ICLR 2023, a premier conference on deep learning, where Google researchers contribute at all levels. This year we are presenting over 100 papers and are actively involved in organizing and hosting a number of different events, including workshops and interactive sessions.

 

If you‚Äôre registered for ICLR 2023, we hope you‚Äôll visit the Google booth to learn more about the exciting work we‚Äôre doing across topics spanning representation and reinforcement learning, theory and optimization, social impact, safety and privacy, and applications from generative AI to‚Ä¶]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["The age of Socratic AI"?]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58607</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58607&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-age-of-socratic-ai"/>
        <updated>2023-04-30T23:20:50.000Z</updated>
        <summary type="html"><![CDATA[Or should we call it "Delphic AI"? Alexy Khrabrov suggested both possibilities a few days ago, in "Reasonable AI ‚Äî the Golden Age of AI Programming": The emerging techniques are all around the way you construct the prompts and also chain them. Effectively, we‚Äôre plotting dialogues. I call it the Age of Socratic AI, or [‚Ä¶]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[a particularly unhelpful graph [about UN Water Conferences]]]></title>
        <id>http://xianblog.wordpress.com/?p=52648</id>
        <link href="https://xianblog.wordpress.com/2023/05/01/a-particularly-unhelpful-graph-about-un-water-conferences/"/>
        <updated>2023-04-30T22:23:54.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[5 worlds of AI]]></title>
        <id>http://windowsontheory.org/?p=8605</id>
        <link href="https://windowsontheory.org/2023/04/30/5-worlds-of-ai/"/>
        <updated>2023-04-30T21:39:21.000Z</updated>
        <summary type="html"><![CDATA[Scott Aaronson and I wrote a post about 5 possible worlds for (the progress of) Artificial Intelligence. See Scott‚Äôs blog for the post itself and discussions. The post was, of course, inspired by the classic essay on the 5 worlds of computational complexity by Russell Impagliazzo who will be turning 60 soon ‚Äì Happy birthday!]]></summary>
        <author>
            <name>Boaz Barak</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[5 worlds of AI]]></title>
        <id>http://windowsontheory.org/?p=8605</id>
        <link href="https://windowsontheory.org/2023/04/30/5-worlds-of-ai/"/>
        <updated>2023-04-30T21:39:21.000Z</updated>
        <summary type="html"><![CDATA[Scott Aaronson and I wrote a post about 5 possible worlds for (the progress of) Artificial Intelligence. See Scott‚Äôs blog for the post itself and discussions. The post was, of course, inspired by the classic essay on the 5 worlds of computational complexity by Russell Impagliazzo who will be turning 60 soon ‚Äì Happy birthday!]]></summary>
        <author>
            <name>Boaz Barak</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The interplay between Cantonese and Mandarin as an index of sociopolitical tensions in Hong Kong]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58594</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58594&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-interplay-between-cantonese-and-mandarin-as-an-index-of-sociopolitical-tensions-in-hong-kong"/>
        <updated>2023-04-30T11:22:18.000Z</updated>
        <summary type="html"><![CDATA[First it was the British from afar, and now it is the Chinese from the north who are imposing themselves on the people of Hong Kong.¬† In both cases, the imposition has been not merely political and economic, but has had important cultural and linguistic implications.¬† Language-wise, under which master have the Hongkongers (also known [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[operation impossible]]></title>
        <id>http://xianblog.wordpress.com/?p=52850</id>
        <link href="https://xianblog.wordpress.com/2023/04/30/operation-impossible/"/>
        <updated>2023-04-29T22:23:36.000Z</updated>
        <summary type="html"><![CDATA[A riddle from The Riddler on how many different numbers one could at most produce from six initial values and the four basic operations. In other words, how many values could the terms in a‚àÖ(b‚àÖ{c‚àÖ[d‚àÖ(e‚àÖf)]}) could take? (With each ‚àÖ being one of the four operations and a,‚Ä¶,f the initial values or a permutation of [‚Ä¶]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calling Benjamin Lee Whorf]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58597</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58597&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=calling-benjamin-lee-whorf"/>
        <updated>2023-04-29T14:56:58.000Z</updated>
        <summary type="html"><![CDATA[What do a baker, a shepherd, and a drummer have in common? You can add an orchestra conductor, Harry Potter, and a drill sergeant. Hint: this is in French. Answer: A central focus of their lives is one or more baguettes ‚Äî which in English variously translates as "stick", "wand", "rod", "baton", "staff", "chopstick", "drumstick", [‚Ä¶]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diabolo:  devil / yo-yo]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58591</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58591&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=diabolo-devil-yo-yo"/>
        <updated>2023-04-29T09:10:51.000Z</updated>
        <summary type="html"><![CDATA[The diabolo, sometimes called a Chinese yo-yo, is a two-headed top controlled by a string manipulated by two sticks, one attached to each end.¬† It is popular among jugglers. Diabolo, commonly misspelled as diablo, was formerly also known as "the devil on two sticks" (Juggling Wiki). In this post, I am concerned primarily with language [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[jeune fille √† la perle [street art]]]></title>
        <id>http://xianblog.wordpress.com/?p=52487</id>
        <link href="https://xianblog.wordpress.com/2023/04/29/jeune-fille-a-la-perle-street-art/"/>
        <updated>2023-04-28T22:23:43.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Test if the sample mean is statistically significantly different from zero]]></title>
        <id>https://parsiad.ca/blog/2023/test_if_the_sample_mean_is_statistically_significantly_different_from_zero/</id>
        <link href="https://parsiad.ca/blog/2023/test_if_the_sample_mean_is_statistically_significantly_different_from_zero/"/>
        <updated>2023-04-28T20:00:00.000Z</updated>
        <summary type="html"><![CDATA[Let $X_{1},\ldots,X_{N}$ be IID. The plug-in estimator for the mean is the sample mean $\bar{X}=(X_{1}+\cdots+X_{N})/N$. The standard error of this estimator is $\sqrt{\operatorname{Var}(X_{1})/N}$. Therefore, a normal confidence interval for the mean is $$ \bar{X}\pm c\sqrt{\frac{\operatorname{Var}(X_{1})}{N}}. $$ In short, if we want to test if the mean is statistically significantly different from zero, the number of samples needs to satisfy $$ N&gt;\frac{c^{2}\operatorname{Var}(X_{1})}{\left|\bar{X}\right|^{2}}. $$]]></summary>
        <author>
            <name>Parsiad Azimzadeh</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Learning Theory for Control: A Finite Sample Perspective]]></title>
        <id>http://arxiv.org/abs/2209.05423</id>
        <link href="http://arxiv.org/abs/2209.05423"/>
        <updated>2023-04-28T07:14:00.845Z</updated>
        <summary type="html"><![CDATA[Anastasios Tsiamis, Ingvar Ziemann, Nikolai Matni, George J. Pappas]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond calibration: estimating the grouping loss of modern neural
  networks]]></title>
        <id>http://arxiv.org/abs/2210.16315</id>
        <link href="http://arxiv.org/abs/2210.16315"/>
        <updated>2023-04-28T07:13:54.882Z</updated>
        <summary type="html"><![CDATA[Alexandre Perez-Lebel (SODA), Marine Le Morvan (SODA), Ga\"el
  Varoquaux (SODA)]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Neural-Symbolic Concept Reasoning]]></title>
        <id>http://arxiv.org/abs/2304.14068</id>
        <link href="http://arxiv.org/abs/2304.14068"/>
        <updated>2023-04-28T07:13:54.803Z</updated>
        <summary type="html"><![CDATA[Pietro Barbiero, Gabriele Ciravegna, Francesco Giannini, Mateo
  Espinosa Zarlenga, Lucie Charlotte Magister, Alberto Tonda, Pietro Lio',
  Frederic Precioso, Mateja Jamnik, Giuseppe Marra]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DORA: Exploring outlier representations in Deep Neural Networks]]></title>
        <id>http://arxiv.org/abs/2206.04530</id>
        <link href="http://arxiv.org/abs/2206.04530"/>
        <updated>2023-04-28T07:13:54.797Z</updated>
        <summary type="html"><![CDATA[Kirill Bykov, Mayukh Deb, Dennis Grinwald, Klaus-Robert M\"uller,
  Marina M.-C. H\"ohne]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Over-Squashing in Message Passing Neural Networks: The Impact of
  Width, Depth, and Topology]]></title>
        <id>http://arxiv.org/abs/2302.02941</id>
        <link href="http://arxiv.org/abs/2302.02941"/>
        <updated>2023-04-28T07:13:54.779Z</updated>
        <summary type="html"><![CDATA[Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise,
  Pietro Lio', Michael Bronstein]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Êñ∞Êµ™Âº†‰øäÊûóÔºöÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊ∂åÁé∞ËÉΩÂäõ‚Äî‚ÄîÁé∞Ë±°‰∏éËß£Èáä]]></title>
        <id>https://www.52nlp.cn/?p=13467</id>
        <link href="https://www.52nlp.cn/%e6%96%b0%e6%b5%aa%e5%bc%a0%e4%bf%8a%e6%9e%97%ef%bc%9a%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%b6%8c%e7%8e%b0%e8%83%bd%e5%8a%9b-%e7%8e%b0%e8%b1%a1%e4%b8%8e%e8%a7%a3"/>
        <updated>2023-04-28T06:22:49.000Z</updated>
        <summary type="html"><![CDATA[ÂÜÖÂÆπÊù•Ê∫êÔºöChatGPT ÂèäÂ§ßÊ®°Âûã‰∏ìÈ¢òÁ†îËÆ®‰ºö¬† ÂàÜ‰∫´ÂòâÂÆæÔºöÊñ∞Êµ™Êñ∞ÊäÄÊúØÁ†îÂèëË¥üË¥£‰∫∫„ÄÅ‰∏≠ÂõΩ‰∏≠Êñá‰ø°ÊÅØÂ≠¶‰ºöÁêÜ‰∫ã [‚Ä¶]]]></summary>
        <author>
            <name>ËææËßÇÊï∞ÊçÆ</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[validated by rigorous peer review with the author‚Äôs benefit]]></title>
        <id>http://xianblog.wordpress.com/?p=52838</id>
        <link href="https://xianblog.wordpress.com/2023/04/28/52838/"/>
        <updated>2023-04-27T22:23:53.000Z</updated>
        <summary type="html"><![CDATA[The other day my co-authors and I received an email addressed to Dear Prof. Hugo Mariva (sic), offering to publish our importance Markov chain paper in the Journal of Pure and Applied Mathematics which is very much likely a predatory journal, given the approximate English of the email, the journal acceptance window, its editorial board, [‚Ä¶]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An ML-based approach to better characterize lung diseases]]></title>
        <id>http://ai.googleblog.com/2023/04/an-ml-based-approach-to-better.html</id>
        <link href="http://ai.googleblog.com/2023/04/an-ml-based-approach-to-better.html"/>
        <updated>2023-04-27T20:24:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Babak Behsaz, Software Engineer, and Andrew Carroll, Product Lead, Genomics




The combination of the environment an individual experiences and their genetic predispositions determines the majority of their risk for various diseases. Large national efforts, such as the UK Biobank, have created large, public resources to better understand the links between environment, genetics, and disease. This has the potential to help individuals better understand how to stay healthy, clinicians to treat illnesses, and scientists to develop new medicines.


One challenge in this process is how we make sense of the vast amount of clinical measurements ‚Äî the UK Biobank has many petabytes of imaging, metabolic tests, and medical records spanning 500,000 individuals. To best use this data, we nee‚Ä¶]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Laowai (the Old Furriner) trolls the CCP]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58578</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58578&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=laowai-the-old-furriner-trolls-the-ccp"/>
        <updated>2023-04-27T19:47:36.000Z</updated>
        <summary type="html"><![CDATA[Laowai is "an informal term or slang for 'foreigner' and/or non-Chinese national".¬† One of the tens of thousands of commenters on the above video is the renowned SerpentZA, who teamed up with his partner, the dry, wry Laowhy86, to form the quintessential Laowai China travel and commentarial vloggers of the second decade of the 21st [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Causal Representation Learning for Out-of-Distribution Motion
  Forecasting]]></title>
        <id>http://arxiv.org/abs/2302.08635</id>
        <link href="http://arxiv.org/abs/2302.08635"/>
        <updated>2023-04-27T07:14:20.356Z</updated>
        <summary type="html"><![CDATA[Shayan Shirahmad Gale Bagi, Zahra Gharaee, Oliver Schulte, Mark
  Crowley]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integration of Radiomics and Tumor Biomarkers in Interpretable Machine
  Learning Models]]></title>
        <id>http://arxiv.org/abs/2303.11177</id>
        <link href="http://arxiv.org/abs/2303.11177"/>
        <updated>2023-04-27T07:14:19.960Z</updated>
        <summary type="html"><![CDATA[Lennart Brocki, Neo Christopher Chung]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Plasticity Neural Network Based on Memory Generation, Memory
  Consolidation and Synaptic Strength Rebalance by Current and Memory Brain
  Plasticity and Synapse Formation-to simulate Artificial Brain]]></title>
        <id>http://arxiv.org/abs/2203.11740</id>
        <link href="http://arxiv.org/abs/2203.11740"/>
        <updated>2023-04-27T07:14:19.960Z</updated>
        <summary type="html"><![CDATA[Jun-Bo Tao, Bai-Qing Sun, Wei-Dong Zhu, Shi-You Qu, Ling-Kun Chen,
  Jia-Qiang Li, Guo-Qi Li, Chong Wu, Yu Xiong, Jiaxuan Zhou]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-vs-the-Rest Loss to Focus on Important Samples in Adversarial
  Training]]></title>
        <id>http://arxiv.org/abs/2207.10283</id>
        <link href="http://arxiv.org/abs/2207.10283"/>
        <updated>2023-04-27T07:14:19.960Z</updated>
        <summary type="html"><![CDATA[Sekitoshi Kanai, Shin'ya Yamaguchi, Masanori Yamada, Hiroshi
  Takahashi, Kentaro Ohno, Yasutoshi Ida]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Methods are Competitive for Operator Learning]]></title>
        <id>http://arxiv.org/abs/2304.13202</id>
        <link href="http://arxiv.org/abs/2304.13202"/>
        <updated>2023-04-27T07:14:19.959Z</updated>
        <summary type="html"><![CDATA[Pau Batlle, Matthieu Darcy, Bamdad Hosseini, Houman Owhadi]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing Hidet: A Deep Learning Compiler for Efficient Model Serving]]></title>
        <id>https://pytorch.org/blog/introducing-hidet/</id>
        <link href="https://pytorch.org/blog/introducing-hidet/"/>
        <updated>2023-04-27T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[Hidet is a powerful deep learning compiler that simplifies the process of implementing high-performing deep learning operators on modern accelerators (e.g., NVIDIA GPUs). With the new feature of torch.compile(...) in PyTorch 2.0, integrating a novel compiler into PyTorch is easier than ever - Hidet now can be used as a torch.compile(...) backend to accelerate PyTorch models, making it an attractive option for PyTorch users who want to improve the inference performance of their models, especially for those who also need to implement extremely optimized custom operators.]]></summary>
        <author>
            <name>PyTorch Website</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepMind‚Äôs latest research at ICLR 2023]]></title>
        <id>https://www.deepmind.com/blog/deepminds-latest-research-at-iclr-2023</id>
        <link href="https://www.deepmind.com/blog/deepminds-latest-research-at-iclr-2023"/>
        <updated>2023-04-27T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Next week marks the start of the 11th International Conference on Learning Representations (ICLR), taking place 1-5 May in Kigali, Rwanda. This will be the first major artificial intelligence (AI) conference to be hosted in Africa and the first in-person event since the start of the pandemic. Researchers from around the world will gather to share their cutting-edge work in deep learning spanning the fields of AI, statistics and data science, and applications including machine vision, gaming and robotics. We‚Äôre proud to support the conference as a Diamond sponsor and DEI champion.]]></summary>
        <author>
            <name>DeepMind Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[postdoctoral research position]]></title>
        <id>http://xianblog.wordpress.com/?p=52863</id>
        <link href="https://xianblog.wordpress.com/2023/04/27/postdoctoral-research-position/"/>
        <updated>2023-04-26T22:23:48.000Z</updated>
        <summary type="html"><![CDATA[Through the ERC Synergy grant OCEAN (On intelligenCE And Networks: Synergistic research in Bayesian Statistics, Microeconomics and Computer¬†Sciences), I am seeking one postdoctoral researcher with an interest in Bayesian federated learning, distributed MCMC, approximate Bayesian inference, and data privacy. The project is based at Universit√© Paris Dauphine, on the new PariSant√© Campus. ¬†The postdoc will [‚Ä¶]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust and efficient medical imaging with self-supervision]]></title>
        <id>http://ai.googleblog.com/2023/04/robust-and-efficient-medical-imaging.html</id>
        <link href="http://ai.googleblog.com/2023/04/robust-and-efficient-medical-imaging.html"/>
        <updated>2023-04-26T20:32:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Shekoofeh Azizi, Senior Research Scientist, and Laura Culp, Senior Research Engineer, Google Research
  
  
  
Despite recent progress in the field of medical artificial intelligence (AI), most existing models are narrow, single-task systems that require large quantities of labeled data to train. Moreover, these models cannot be easily reused in new clinical contexts as they often require the collection, de-identification and annotation of site-specific data for every new deployment environment, which is both laborious and expensive. This problem of data-efficient generalization (a model‚Äôs ability to generalize to new settings using minimal new data) continues to be a key translational challenge for medical machine learning (ML) models and has in turn, prevented their broad uptak‚Ä¶]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fake F*ck]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58574</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58574&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=fake-fck"/>
        <updated>2023-04-26T13:36:52.000Z</updated>
        <summary type="html"><![CDATA[This is also linguistically fascinating because it shows ‚Äòfuck‚Äô being used as a loanword (fake Ê≥ïÂÖã) in Mandarin! The loanword is so entrenched nowadays that it has it‚Äòs own baidupedia entry‚Ä¶ üòÖ 1/ https://t.co/TVxEmev6iA pic.twitter.com/UKBx3CiO26 ‚Äî Egas Moniz-Bandeira ·†°·°§·††·†∞ ·†Æ·†£·†®·†¢·†∞ ·†™·††·†®·°©·†ã·††·†∂·†¢·†∑·†† (@egasmb) April 25, 2023 Usual LL romanization, transcription, and translation of the Chinese text: [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simplifying Momentum-based Positive-definite Submanifold Optimization
  with Applications to Deep Learning]]></title>
        <id>http://arxiv.org/abs/2302.09738</id>
        <link href="http://arxiv.org/abs/2302.09738"/>
        <updated>2023-04-26T07:14:18.633Z</updated>
        <summary type="html"><![CDATA[Wu Lin, Valentin Duruisseaux, Melvin Leok, Frank Nielsen, Mohammad
  Emtiyaz Khan, Mark Schmidt]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Ability of Graph Neural Networks to Model Interactions Between
  Vertices]]></title>
        <id>http://arxiv.org/abs/2211.16494</id>
        <link href="http://arxiv.org/abs/2211.16494"/>
        <updated>2023-04-26T07:14:18.577Z</updated>
        <summary type="html"><![CDATA[Noam Razin, Tom Verbin, Nadav Cohen]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proto-Value Networks: Scaling Representation Learning with Auxiliary
  Tasks]]></title>
        <id>http://arxiv.org/abs/2304.12567</id>
        <link href="http://arxiv.org/abs/2304.12567"/>
        <updated>2023-04-26T07:14:18.550Z</updated>
        <summary type="html"><![CDATA[Jesse Farebrother, Joshua Greaves, Rishabh Agarwal, Charline Le Lan,
  Ross Goroshin, Pablo Samuel Castro, Marc G. Bellemare]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing BERT against traditional machine learning text classification]]></title>
        <id>http://arxiv.org/abs/2005.13012</id>
        <link href="http://arxiv.org/abs/2005.13012"/>
        <updated>2023-04-26T07:14:18.544Z</updated>
        <summary type="html"><![CDATA[Santiago Gonz\'alez-Carvajal, Eduardo C. Garrido-Merch\'an]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free Probability for predicting the performance of feed-forward fully
  connected neural networks]]></title>
        <id>http://arxiv.org/abs/2111.00841</id>
        <link href="http://arxiv.org/abs/2111.00841"/>
        <updated>2023-04-26T07:14:18.539Z</updated>
        <summary type="html"><![CDATA[Reda Chhaibi, Tariq Daouda, Ezechiel Kahn]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[simulated annealing and logistic regression to the max]]></title>
        <id>http://xianblog.wordpress.com/?p=52791</id>
        <link href="https://xianblog.wordpress.com/2023/04/26/simulated-annealing-and-logistic-regression-to-the-max/"/>
        <updated>2023-04-25T22:23:39.000Z</updated>
        <summary type="html"><![CDATA[A Riddler puzzle on the three binary and sequential questions one should ask three players hiding their respective U(0,1) realisation, U, V, and W, to best guess which player holds the largest number, max{U,V,W}. Assuming questions of the type Is U<b¬π, &tc., the challenge boils down to selecting seven bounds (one for U, two for [‚Ä¶]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LayerNAS: Neural Architecture Search in Polynomial Complexity]]></title>
        <id>http://ai.googleblog.com/2023/04/layernas-neural-architecture-search-in.html</id>
        <link href="http://ai.googleblog.com/2023/04/layernas-neural-architecture-search-in.html"/>
        <updated>2023-04-25T17:20:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Yicheng Fan and Dana Alon, Software Engineers, Google Research




Every byte and every operation matters when trying to build a faster model, especially if the model is to run on-device. Neural architecture search (NAS) algorithms design sophisticated model architectures by searching through a larger model-space than what is possible manually. Different NAS algorithms, such as MNasNet and TuNAS, have been proposed and have discovered several efficient model architectures, including MobileNetV3, EfficientNet. 

 


Here we present LayerNAS, an approach that reformulates the multi-objective NAS problem within the framework of combinatorial optimization to greatly reduce the complexity, which results in an order of magnitude reduction in the number of model candidates that must be ‚Ä¶]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparative dialectology and romanizations for North and South Korea]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58561</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58561&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=comparative-dialectology-and-romanizations-for-north-and-south-korea"/>
        <updated>2023-04-25T10:49:48.000Z</updated>
        <summary type="html"><![CDATA[[This is a guest post by S. Robert Ramsey] Your Language Log coverage of the North Korean news item was chilling, but pretty much what we've come to expect of that outrageous regime. If ever there was a clearer contrast between the two worlds in conflict, I've never heard of it. South Korea is now [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ê≥®ÊÑèÂäõÂíåSoftmaxÁöÑ‰∏§ÁÇπÊúâË∂£ÂèëÁé∞ÔºöÈ≤ÅÊ£íÊÄßÂíå‰ø°ÊÅØÈáè]]></title>
        <id>https://kexue.fm/archives/9593</id>
        <link href="https://kexue.fm/archives/9593"/>
        <updated>2023-04-25T09:00:00.000Z</updated>
        <summary type="html"><![CDATA[ÊúÄËøëÂá†Âë®Á¨îËÄÖ‰∏ÄÁõ¥ÈÉΩÂú®ÊÄùËÄÉÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÁõ∏ÂÖ≥ÊÄßË¥®ÔºåÂú®Ëøô‰∏™ËøáÁ®ã‰∏≠ÂØπÊ≥®ÊÑèÂäõÂèäSoftmaxÊúâ‰∫ÜÊõ¥Ê∑±ÂàªÁöÑÁêÜËß£„ÄÇÂú®ËøôÁØáÊñáÁ´†‰∏≠ÔºåÁ¨îËÄÖÁÆÄÂçïÂàÜ‰∫´ÂÖ∂‰∏≠ÁöÑ‰∏§ÁÇπÔºö1„ÄÅSoftmaxÊ≥®ÊÑèÂäõÂ§©ÁÑ∂ËÉΩÂ§üÊäµÂæ°‰∏ÄÂÆöÁöÑÂô™Â£∞Êâ∞Âä®Ôºõ2„ÄÅ...]]></summary>
        <author>
            <name>ËãèÂâëÊûó</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A theory of representation learning in deep neural networks gives a deep
  generalisation of kernel methods]]></title>
        <id>http://arxiv.org/abs/2108.13097</id>
        <link href="http://arxiv.org/abs/2108.13097"/>
        <updated>2023-04-25T07:14:05.616Z</updated>
        <summary type="html"><![CDATA[Adam X. Yang, Maxime Robeyns, Edward Milsom, Nandi Schoots, Laurence
  Aitchison]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Model Selection and Nearly-Proper Learning for GMMs]]></title>
        <id>http://arxiv.org/abs/2106.02774</id>
        <link href="http://arxiv.org/abs/2106.02774"/>
        <updated>2023-04-25T07:14:05.603Z</updated>
        <summary type="html"><![CDATA[Jerry Li, Allen Liu, Ankur Moitra]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conformal Risk Control]]></title>
        <id>http://arxiv.org/abs/2208.02814</id>
        <link href="http://arxiv.org/abs/2208.02814"/>
        <updated>2023-04-25T07:14:05.597Z</updated>
        <summary type="html"><![CDATA[Anastasios N. Angelopoulos, Stephen Bates, Adam Fisch, Lihua
  Lei, Tal Schuster]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmentation and Optimal Region Selection of Physiological Signals using
  Deep Neural Networks and Combinatorial Optimization]]></title>
        <id>http://arxiv.org/abs/2003.07981</id>
        <link href="http://arxiv.org/abs/2003.07981"/>
        <updated>2023-04-25T07:14:05.592Z</updated>
        <summary type="html"><![CDATA[Jorge Oliveira, Margarida Carvalho, Diogo Marcelo Nogueira, Miguel
  Coimbra]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Bias of Gradient Descent for Mean Squared Error Regression with
  Two-Layer Wide Neural Networks]]></title>
        <id>http://arxiv.org/abs/2006.07356</id>
        <link href="http://arxiv.org/abs/2006.07356"/>
        <updated>2023-04-25T07:14:05.568Z</updated>
        <summary type="html"><![CDATA[Hui Jin, Guido Mont\'ufar]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[top off‚Ä¶]]></title>
        <id>http://xianblog.wordpress.com/?p=52717</id>
        <link href="https://xianblog.wordpress.com/2023/04/25/top-off/"/>
        <updated>2023-04-24T22:23:43.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Correcting Bayesian Optimization through Bayesian Active Learning]]></title>
        <id>http://arxiv.org/abs/2304.11005</id>
        <link href="http://arxiv.org/abs/2304.11005"/>
        <updated>2023-04-24T07:14:24.151Z</updated>
        <summary type="html"><![CDATA[Carl Hvarfner, Erik Hellsten, Frank Hutter, Luigi Nardi]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implications of sparsity and high triangle density for graph
  representation learning]]></title>
        <id>http://arxiv.org/abs/2210.15277</id>
        <link href="http://arxiv.org/abs/2210.15277"/>
        <updated>2023-04-24T07:14:24.146Z</updated>
        <summary type="html"><![CDATA[Hannah Sansford, Alexander Modell, Nick Whiteley, Patrick
  Rubin-Delanchy]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpolation property of shallow neural networks]]></title>
        <id>http://arxiv.org/abs/2304.10552</id>
        <link href="http://arxiv.org/abs/2304.10552"/>
        <updated>2023-04-24T07:14:23.797Z</updated>
        <summary type="html"><![CDATA[Vlad-Raul Constantinescu, Ionel Popescu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence of Message Passing Graph Neural Networks with Generic
  Aggregation On Large Random Graphs]]></title>
        <id>http://arxiv.org/abs/2304.11140</id>
        <link href="http://arxiv.org/abs/2304.11140"/>
        <updated>2023-04-24T07:14:23.510Z</updated>
        <summary type="html"><![CDATA[Matthieu Cordonnier, Nicolas Keriven, Nicolas Tremblay, Samuel Vaiter]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Common Misassumption in Online Experiments with Machine Learning
  Models]]></title>
        <id>http://arxiv.org/abs/2304.10900</id>
        <link href="http://arxiv.org/abs/2304.10900"/>
        <updated>2023-04-24T07:14:23.454Z</updated>
        <summary type="html"><![CDATA[Olivier Jeunen]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[‰∏äÊµ∑È¶ñ‰∏™Â§ßÊ®°Âûã‰∫ß‰∏öÂèëÂ±ïÁ†îËÆ®‰ºöÂúÜÊª°ËêΩÂπïÔºå‰ºóÈ°∂Â∞ñ‰∏ìÂÆ∂ÂÖ±Êé¢Â§ßÊ®°Âûã‰∫ß‰∏ö]]></title>
        <id>https://www.52nlp.cn/?p=13461</id>
        <link href="https://www.52nlp.cn/%e4%b8%8a%e6%b5%b7%e9%a6%96%e4%b8%aa%e5%a4%a7%e6%a8%a1%e5%9e%8b%e4%ba%a7%e4%b8%9a%e5%8f%91%e5%b1%95%e7%a0%94%e8%ae%a8%e4%bc%9a%e5%9c%86%e6%bb%a1%e8%90%bd%e5%b9%95%ef%bc%8c%e4%bc%97%e9%a1%b6%e5%b0%96"/>
        <updated>2023-04-24T06:32:06.000Z</updated>
        <summary type="html"><![CDATA[4Êúà21Êó•ÔºåÁî±‰∏äÊµ∑Â∏ÇÊï∞ÊçÆÁßëÂ≠¶ÈáçÁÇπÂÆûÈ™åÂÆ§Êê∫ÊâãËææËßÇÊï∞ÊçÆ„ÄÅ‰∏äÊµ∑Êµ¶‰∏úËΩØ‰ª∂Âõ≠ËÅîÂêà‰∏æÂäûÁöÑ‚ÄúÂ§ßÊ®°Âûã‰∫ß‰∏öÂèëÂ±ïÁ†îËÆ®‰ºö‚ÄùÂúÜÊª°ËêΩÂπï„ÄÇ [‚Ä¶]]]></summary>
        <author>
            <name>ËææËßÇÊï∞ÊçÆ</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChatGPTÂèäÂ§ßÊ®°Âûã‰∏ìÈ¢òÁ†îËÆ®‰ºöÂú®ËìâÊàêÂäü‰∏æÂäûÂÖ±Êé¢ÂâçÊ≤øÊäÄÊúØ‰∏é‰∫ß‰∏öÂèëÂ±ï]]></title>
        <id>https://www.52nlp.cn/?p=13459</id>
        <link href="https://www.52nlp.cn/chatgpt%e5%8f%8a%e5%a4%a7%e6%a8%a1%e5%9e%8b%e4%b8%93%e9%a2%98%e7%a0%94%e8%ae%a8%e4%bc%9a%e5%9c%a8%e8%93%89%e6%88%90%e5%8a%9f%e4%b8%be%e5%8a%9e%e5%85%b1%e6%8e%a2%e5%89%8d%e6%b2%bf%e6%8a%80%e6%9c%af"/>
        <updated>2023-04-24T06:06:57.000Z</updated>
        <summary type="html"><![CDATA[4Êúà14Êó•ÔºåÁî±ÊàêÈÉΩÂ∏ÇÊñ∞ÁªèÊµéÂèëÂ±ïÂßîÂëò‰ºöÊåáÂØºÔºåÊàêÈÉΩÂ∏Ç‰∫∫Â∑•Êô∫ËÉΩ‰∫ß‰∏öÁîüÊÄÅËÅîÁõü‰∏ªÂäûÔºåËææËßÇÊï∞ÊçÆÊâøÂäûÔºåÊô∫ÊÖßËìâÂüéÁ†îÁ©∂Èô¢‰∏éË∂ãÂä®Áßë [‚Ä¶]]]></summary>
        <author>
            <name>ËææËßÇÊï∞ÊçÆ</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google at CHI 2023]]></title>
        <id>http://ai.googleblog.com/2023/04/google-at-chi-2023.html</id>
        <link href="http://ai.googleblog.com/2023/04/google-at-chi-2023.html"/>
        <updated>2023-04-24T02:26:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Malaya Jules, Program Manager, Google




This week, the Conference on Human Factors in Computing Systems (CHI 2023) is being held in Hamburg, Germany. We are proud to be a Hero Sponsor of CHI 2023, a premier conference on human-computer interaction, where Google researchers contribute at all levels. This year we are presenting over 30 papers and are actively involved in organizing and hosting a number of different events across workshops, courses, and interactive sessions.


 


If you‚Äôre registered for CHI 2023, we hope you‚Äôll visit the Google booth to learn more about the exciting work across various topics, including language interactions, causal inference, question answering and more. Take a look below to learn more about the Google research being presented at CHI 2023 (Goog‚Ä¶]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Japanese book formats]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58534</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58534&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=japanese-book-formats"/>
        <updated>2023-04-24T01:29:54.000Z</updated>
        <summary type="html"><![CDATA[Two days ago, a Penn freshman from China gifted me with a small format edition of the Gu«êg«îzi È¨ºË∞∑Â≠ê (Master of Ghost Valley), a text that has long intrigued me. Guiguzi (È¨ºË∞∑Â≠ê) is a collection of ancient Chinese texts compiled between the late Warring States period and the end of the Han Dynasty. The work, [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How can we build human values into AI?]]></title>
        <id>https://www.deepmind.com/blog/how-can-we-build-human-values-into-ai</id>
        <link href="https://www.deepmind.com/blog/how-can-we-build-human-values-into-ai"/>
        <updated>2023-04-24T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[As artificial intelligence (AI) becomes more powerful and more deeply integrated into our lives, the questions of how it is used and deployed are all the more important. What values guide AI? Whose values are they? And how are they selected?]]></summary>
        <author>
            <name>DeepMind Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[the Bayesian learning rule [One World ABC‚Äôminar, 27 April]]]></title>
        <id>http://xianblog.wordpress.com/?p=52845</id>
        <link href="https://xianblog.wordpress.com/2023/04/24/the-bayesian-learning-rule-one-world-abcminar-27-april/"/>
        <updated>2023-04-23T22:23:53.000Z</updated>
        <summary type="html"><![CDATA[The next One World ABC seminar is taking place (on-line, requiring pre-registration) on 27 April, 9:30am UK time, with Mohammad Emtiyaz Khan (RIKEN-AIP, Tokyo) speaking about the Bayesian learning rule: We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields [‚Ä¶]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combinatorial Algorithms for Matching Markets via Nash Bargaining:
  One-Sided, Two-Sided and Non-Bipartite]]></title>
        <id>http://arxiv.org/abs/2106.02024</id>
        <link href="http://arxiv.org/abs/2106.02024"/>
        <updated>2023-04-23T07:12:54.411Z</updated>
        <summary type="html"><![CDATA[Ioannis Panageas, Thorben Tr\"obst, Vijay V. Vazirani]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A new kanji for tapioca]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58524</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58524&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=a-new-kanji-for-tapioca"/>
        <updated>2023-04-23T03:33:54.000Z</updated>
        <summary type="html"><![CDATA[Remixed meme: The concocted kanji is¬†Âßè (Unicode U+59CF)¬†with the punctuation marks /„ÄÇsuperimposed upon it. I haven't met anyone who knows the underlying character, so it might as well have been made up by the memer. It is composed of n«ö Â•≥ ("female"),¬†Kangxi radical¬†#38, as presumed semantophore on the left and¬†gƒÅn¬†Áîò ("sweet"),¬†Kangxi radical #99, as presumed [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[art brut]]></title>
        <id>http://xianblog.wordpress.com/?p=52774</id>
        <link href="https://xianblog.wordpress.com/2023/04/23/art-brut-89/"/>
        <updated>2023-04-22T22:23:44.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Seacuterie"]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58540</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58540&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=seacuterie"/>
        <updated>2023-04-22T13:00:30.000Z</updated>
        <summary type="html"><![CDATA[Although Google now has "about 27,700 results" for seacuterie, this word doesn't seem to have made it into any of the standard dictionaries yet. But already in 2017, Fine Dining Lovers announced ("Seacuterie, When Salami Rhymes with 'Sea-lami'") that "today‚Äôs latest craze is 'seacuterie'", and went on to survey the gastronomical metaphors involved at greater [‚Ä¶]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[10th international Purdue symposium on Statistics]]></title>
        <id>http://xianblog.wordpress.com/?p=52779</id>
        <link href="https://xianblog.wordpress.com/2023/04/22/10th-international-purdue-symposium-on-statistics/"/>
        <updated>2023-04-21T22:23:59.000Z</updated>
        <summary type="html"><![CDATA[Thrice alas, I was dearly hoping to join the 10th international Purdue symposium on Statistics next June, but due to too many work- and home-front constraints in this period, I had to cancel my participation to this meeting and hence miss a rare opportunity to return for the first time since 1993 to a place [‚Ä¶]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Blocks for ML: Accelerating machine learning prototyping with interactive tools]]></title>
        <id>http://ai.googleblog.com/2023/04/visual-blocks-for-ml-accelerating.html</id>
        <link href="http://ai.googleblog.com/2023/04/visual-blocks-for-ml-accelerating.html"/>
        <updated>2023-04-21T19:01:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Ruofei Du, Interactive Perception & Graphics Lead, Google Augmented Reality, and Na Li, Tech Lead Manager, Google CoreML

  


Recent deep learning advances have enabled a plethora of high-performance, real-time multimedia applications based on machine learning (ML), such as human body segmentation for video and teleconferencing, depth estimation for 3D reconstruction, hand and body tracking for interaction, and audio processing for remote communication.



However, developing and iterating on these ML-based multimedia prototypes can be challenging and costly. It usually involves a cross-functional team of ML practitioners who fine-tune the models, evaluate robustness, characterize strengths and weaknesses, inspect performance in the end-use context, and develop the applications.‚Ä¶]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Jamie Morgenstern - Shifts in Distributions and Preferences in Response to Learning]]></title>
        <id>https://mlfoundations.org/talk/morgenstern/</id>
        <link href="https://mlfoundations.org/talk/morgenstern/"/>
        <updated>2023-04-21T14:00:00.000Z</updated>
        <summary type="html"><![CDATA[In this talk, I'll describe some recent work outlining how distribution shifts are fundamental to working with human-centric data. Some of these shifts come from attempting to "join" datasets gathered in different contexts,  others may be the result of people's preferences affecting which data they provide to which systems, and even more can arise when peoples' preferences themselves are shaped by ML systems' recommendations. Each of these types of shift require different modeling and analysis to more accurately predict the behavior of ML pipelines deployed in a way where they interact repeatedly with people who care about their predictions.]]></summary>
        <author>
            <name>Harvard ML Foundations</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[In North Korea, it's a dire crime to speak like a South Korean]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58531</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58531&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=in-north-korea-its-a-dire-crime-to-speak-like-a-south-korean"/>
        <updated>2023-04-21T12:25:23.000Z</updated>
        <summary type="html"><![CDATA[But almost everybody does it.¬† You can barely avoid it.¬† Especially if you're an athlete. "North Korea sentences 20 young athletes for ‚Äòspeaking like South Koreans‚Äô" Skaters and skiers were caught on video using banned words while playing a game during training. Jieun Kim, RFA (4/13/23) The first two paragraphs of the article: About 20 [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boundary Graph Neural Networks for 3D Simulations]]></title>
        <id>http://arxiv.org/abs/2106.11299</id>
        <link href="http://arxiv.org/abs/2106.11299"/>
        <updated>2023-04-21T07:14:22.400Z</updated>
        <summary type="html"><![CDATA[Andreas Mayr, Sebastian Lehner, Arno Mayrhofer, Christoph Kloss, Sepp
  Hochreiter, Johannes Brandstetter]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication-Efficient Adaptive Federated Learning]]></title>
        <id>http://arxiv.org/abs/2205.02719</id>
        <link href="http://arxiv.org/abs/2205.02719"/>
        <updated>2023-04-21T07:14:22.395Z</updated>
        <summary type="html"><![CDATA[Yujia Wang, Lu Lin, Jinghui Chen]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Narrow One-Hidden-Layer ReLU Networks]]></title>
        <id>http://arxiv.org/abs/2304.10524</id>
        <link href="http://arxiv.org/abs/2304.10524"/>
        <updated>2023-04-21T07:14:22.390Z</updated>
        <summary type="html"><![CDATA[Sitan Chen, Zehao Dou, Surbhi Goel, Adam R Klivans, Raghu Meka]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Learning Approach to Analyzing Continuous-Time Systems]]></title>
        <id>http://arxiv.org/abs/2209.12128</id>
        <link href="http://arxiv.org/abs/2209.12128"/>
        <updated>2023-04-21T07:14:21.826Z</updated>
        <summary type="html"><![CDATA[Cory Shain, William Schuler]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Manifold Two-Sample Test Study: Integral Probability Metric with
  Neural Networks]]></title>
        <id>http://arxiv.org/abs/2205.02043</id>
        <link href="http://arxiv.org/abs/2205.02043"/>
        <updated>2023-04-21T07:14:21.821Z</updated>
        <summary type="html"><![CDATA[Jie Wang, Minshuo Chen, Tuo Zhao, Wenjing Liao, Yao Xie]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[powering a probability [a Bernoulli factory tale]]]></title>
        <id>http://xianblog.wordpress.com/?p=52763</id>
        <link href="https://xianblog.wordpress.com/2023/04/21/powering-a-probability-a-bernoulli-factory-tale/"/>
        <updated>2023-04-20T22:23:52.000Z</updated>
        <summary type="html"><![CDATA[Starting from an X validated question on finding an unbiased estimator of an integral raised to a non-integer power, I came across a somewhat interesting Bernoulli factory solution! Thanks to Peter Occil‚Äôs encyclopedic record of cases, pointing out to Mendo‚Äôs (2019) solution for functions of œÅ that can be expressed as power series. Like œÅŒ≥ [‚Ä¶]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent advances in deep long-horizon forecasting]]></title>
        <id>http://ai.googleblog.com/2023/04/recent-advances-in-deep-long-horizon.html</id>
        <link href="http://ai.googleblog.com/2023/04/recent-advances-in-deep-long-horizon.html"/>
        <updated>2023-04-20T20:26:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Rajat Sen and Abhimanyu Das, Research Scientists, Google Research




Time-series forecasting is an important research area that is critical to several scientific and industrial applications, like retail supply chain optimization, energy and traffic prediction, and weather forecasting. In retail use cases, for example, it has been observed that improving demand forecasting accuracy can meaningfully reduce inventory costs and increase revenue. 
 

Modern time-series applications can involve forecasting hundreds of thousands of correlated time-series (e.g., demands of different products for a retailer) over long horizons (e.g., a quarter or year away at daily granularity). As such, time-series forecasting models need to satisfy the following key criterias:



Ability to handle auxi‚Ä¶]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domaine La [Nouvelle] France]]></title>
        <id>http://xianblog.wordpress.com/?p=52440</id>
        <link href="https://xianblog.wordpress.com/2023/04/20/domaine-la-nouvelle-france/"/>
        <updated>2023-04-20T18:20:12.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shanghainese under attack]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58515</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58515&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=shanghainese-under-attack"/>
        <updated>2023-04-20T13:46:43.000Z</updated>
        <summary type="html"><![CDATA[Headline in a Hong Kong Chinese newspaper, Bastille Post Â∑¥Â£´ÁöÑÂ†± (4/15/23): Sh√†ngh«éi X√∫jiƒÅhu√¨ sh≈´yu√†n y√¨m√≠ng zhƒ´ zhƒìng sh√¨f«íu g«éi y√≤ng H√†ny«î Pƒ´nyƒ´n zhuƒÅnjiƒÅ h√°ndi√© ‰∏äÊµ∑ÂæêÂÆ∂ÂåØÊõ∏Èô¢Ë≠ØÂêç‰πãÁà≠ ÊòØÂê¶ÊîπÁî®Êº¢Ë™ûÊãºÈü≥Â∞àÂÆ∂ÂíÅ‰è≤ "Controversy over the transcription of the name of the Xujiahui Library in Shanghai:¬† should it be changed to Hanyu Pinyin? Expert opinions" Currently the name of this library at [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speech error of the week]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58519</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58519&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=speech-error-of-the-week-2"/>
        <updated>2023-04-20T12:53:37.000Z</updated>
        <summary type="html"><![CDATA[Rep. Steube (R-Fla.) announces he‚Äôs supporting Trump pic.twitter.com/3bL2cBl9z2 ‚Äî Acyn (@Acyn) April 17, 2023 The context: Your browser does not support the audio element. yeah Rob I wanna thank you for giving me the opportunity to talk about this on your show and I'm happy and honored to endorse Donald J Chump uh for president [‚Ä¶]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some recent news and posts from Pinyin.info]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58509</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58509&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=some-recent-news-and-posts-from-pinyin-info"/>
        <updated>2023-04-20T10:45:54.000Z</updated>
        <summary type="html"><![CDATA[OMG, it‚Äôs nougat (4/15/23) ‚Äî "OMG" borrowed into Mandarin A long post on puns, multiscriptal writing, and the difficulties of Hanzi. Puns piled upon puns. Microsoft Translator and Pinyin (4/15/23) Microsoft's not very good character-to-Pinyin conversion. They have the resources and could surely do better. Japan likely to regulate pronunciations of personal names (4/14/23) ‚Äî [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Data Driven Sequential Learning Framework to Accelerate and Optimize
  Multi-Objective Manufacturing Decisions]]></title>
        <id>http://arxiv.org/abs/2304.09278</id>
        <link href="http://arxiv.org/abs/2304.09278"/>
        <updated>2023-04-20T07:14:16.082Z</updated>
        <summary type="html"><![CDATA[Hamed Khosravi, Taofeeq Olajire, Ahmed Shoyeb Raihan, Imtiaz Ahmed]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization and Estimation Error Bounds for Model-based Neural
  Networks]]></title>
        <id>http://arxiv.org/abs/2304.09802</id>
        <link href="http://arxiv.org/abs/2304.09802"/>
        <updated>2023-04-20T07:14:16.060Z</updated>
        <summary type="html"><![CDATA[Avner Shultzman, Eyar Azar, Miguel R. D. Rodrigues, Yonina C. Eldar]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical inference for transfer learning with high-dimensional
  quantile regression]]></title>
        <id>http://arxiv.org/abs/2211.14578</id>
        <link href="http://arxiv.org/abs/2211.14578"/>
        <updated>2023-04-20T07:14:15.185Z</updated>
        <summary type="html"><![CDATA[Jiayu Huang, Mingqiu Wang, Yuanshan Wu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Loss minimization yields multicalibration for large neural networks]]></title>
        <id>http://arxiv.org/abs/2304.09424</id>
        <link href="http://arxiv.org/abs/2304.09424"/>
        <updated>2023-04-20T07:14:15.174Z</updated>
        <summary type="html"><![CDATA[Jaros{\l}aw B{\l}asiok, Parikshit Gopalan, Lunjia Hu, Adam Tauman
  Kalai, Preetum Nakkiran]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence of stochastic gradient descent under a local Lajasiewicz
  condition for deep neural networks]]></title>
        <id>http://arxiv.org/abs/2304.09221</id>
        <link href="http://arxiv.org/abs/2304.09221"/>
        <updated>2023-04-20T07:14:15.163Z</updated>
        <summary type="html"><![CDATA[Jing An, Jianfeng Lu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Announcing Google DeepMind]]></title>
        <id>https://www.deepmind.com/blog/announcing-google-deepmind</id>
        <link href="https://www.deepmind.com/blog/announcing-google-deepmind"/>
        <updated>2023-04-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[DeepMind and the Brain team from Google Research will join forces to accelerate progress towards a world in which AI helps solve the biggest challenges facing humanity.]]></summary>
        <author>
            <name>DeepMind Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Responsible AI at Google Research: Technology, AI, Society and Culture]]></title>
        <id>http://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html</id>
        <link href="http://ai.googleblog.com/2023/04/responsible-ai-at-google-research.html"/>
        <updated>2023-04-19T18:12:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Lauren Wilcox, Senior Staff Research Scientist, on behalf of the Technology, AI, Society and Culture Team




Google sees AI as a foundational and transformational technology, with recent advances in generative AI technologies, such as LaMDA, PaLM, Imagen, Parti, MusicLM, and similar machine learning (ML) models, some of which are now being incorporated into our products. This transformative potential requires us to be responsible not only in how we advance our technology, but also in how we envision which technologies to build, and how we assess the social impact AI and ML-enabled technologies have on the world. This endeavor necessitates fundamental and applied research with an interdisciplinary lens that engages with ‚Äî and accounts for ‚Äî the social, cultural, economic, and oth‚Ä¶]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The verbal and visual in traditional prosimetric literature]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58483</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58483&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-verbal-and-visual-in-traditional-prosimetric-literature"/>
        <updated>2023-04-19T12:43:01.000Z</updated>
        <summary type="html"><![CDATA[In my trilogy of books and dozens of articles about medieval picture storytelling in South, Central, East, and Southeast Asia, I stressed the alternation of sung and spoken passages as performed by the narrator: Tun-huang Popular Narratives (Cambridge University Press, 1983) Painting and Performance:¬† Chinese Picture Recitation and Its Indian Genesis (University of Hawai'i Press, [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Learning in Multivariate Big Data Analysis for Network
  Monitoring]]></title>
        <id>http://arxiv.org/abs/1907.02677</id>
        <link href="http://arxiv.org/abs/1907.02677"/>
        <updated>2023-04-19T07:14:19.589Z</updated>
        <summary type="html"><![CDATA[Jos\'e Camacho, Katarzyna Wasielewska, Rasmus Bro, David Kotz]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impossibility of Characterizing Distribution Learning -- a simple
  solution to a long-standing problem]]></title>
        <id>http://arxiv.org/abs/2304.08712</id>
        <link href="http://arxiv.org/abs/2304.08712"/>
        <updated>2023-04-19T07:14:19.571Z</updated>
        <summary type="html"><![CDATA[Tosca Lechner, Shai-Ben-David]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Likelihood Learning of Unnormalized Models for Simulation-Based
  Inference]]></title>
        <id>http://arxiv.org/abs/2210.14756</id>
        <link href="http://arxiv.org/abs/2210.14756"/>
        <updated>2023-04-19T07:14:19.516Z</updated>
        <summary type="html"><![CDATA[Pierre Glaser, Michael Arbel, Samo Hromadka, Arnaud Doucet, Arthur
  Gretton]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural networks for geospatial data]]></title>
        <id>http://arxiv.org/abs/2304.09157</id>
        <link href="http://arxiv.org/abs/2304.09157"/>
        <updated>2023-04-19T07:14:19.487Z</updated>
        <summary type="html"><![CDATA[Wentao Zhan, Abhirup Datta]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Large Language Models with Accelerated Transformers]]></title>
        <id>https://pytorch.org/blog/accelerating-large-language-models/</id>
        <link href="https://pytorch.org/blog/accelerating-large-language-models/"/>
        <updated>2023-04-19T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[TL;DR. We show how to use Accelerated PyTorch 2.0 Transformers and the newly introduced torch.compile() method to accelerate Large Language Models on the example of nanoGPT, a compact open-source implementation of the GPT model from Andrej Karpathy. Using the new scaled dot product attention operator introduced with Accelerated PT2 Transformers, we select the flash_attention custom kernel and achieve faster training time per batch (measured with Nvidia A100 GPUs), going from a ~143ms/batch baseline to ~113 ms/batch. In addition, the enhanced implementation using the SDPA operator offers better numerical stability. Finally, further optimizations are achieved using padded inputs, which when combined with flash attention lead to ~87ms/batch.]]></summary>
        <author>
            <name>PyTorch Website</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mike Titterington [in memoriam¬≤]]]></title>
        <id>http://xianblog.wordpress.com/?p=52832</id>
        <link href="https://xianblog.wordpress.com/2023/04/19/mike-titterington-in-memoriam%c2%b2/"/>
        <updated>2023-04-18T22:23:12.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially private heatmaps]]></title>
        <id>http://ai.googleblog.com/2023/04/differentially-private-heatmaps.html</id>
        <link href="http://ai.googleblog.com/2023/04/differentially-private-heatmaps.html"/>
        <updated>2023-04-18T20:19:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Badih Ghazi, Staff Research Scientist, and Nachiappan Valliappan, Staff Software Engineer, Google Research






Recently, differential privacy (DP) has emerged as a mathematically robust notion of user privacy for data aggregation and machine learning (ML), with practical deployments including the 2022 US Census and in industry. Over the last few years, we have open-sourced libraries for privacy-preserving analytics and ML and have been constantly enhancing their capabilities. Meanwhile, new algorithms have been developed by the research community for several analytic tasks involving private aggregation of data. 
 

One such important data aggregation method is the heatmap. Heatmaps are popular for visualizing aggregated data in two or more dimensions. They are widely used in ma‚Ä¶]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Good bad]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58488</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58488&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=good-bad"/>
        <updated>2023-04-18T14:05:10.000Z</updated>
        <summary type="html"><![CDATA[Benito Antonio Mart√≠nez Ocasio, known as Bad Bunny, has been big in the media recently, from the first-ever Spanish cover of Time Magazine, to headlining Coachella ‚Äî against the background of literally millions of pages featuring his fashion choices and his sayings. According to a 2019 All Things Considered piece ("How Bad Bunny Skipped Categories [‚Ä¶]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open-set learning with augmented category by exploiting unlabeled data
  (Open-LACU)]]></title>
        <id>http://arxiv.org/abs/2002.01368</id>
        <link href="http://arxiv.org/abs/2002.01368"/>
        <updated>2023-04-18T07:14:03.275Z</updated>
        <summary type="html"><![CDATA[Emile R. Engelbrecht, Johan A. du Preez]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Compositional Policy Learning via Language Grounding]]></title>
        <id>http://arxiv.org/abs/2004.07200</id>
        <link href="http://arxiv.org/abs/2004.07200"/>
        <updated>2023-04-18T07:14:03.263Z</updated>
        <summary type="html"><![CDATA[Tianshi Cao, Jingkang Wang, Yining Zhang, Sivabalan Manivasagam]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proof-of-Learning is Currently More Broken Than You Think]]></title>
        <id>http://arxiv.org/abs/2208.03567</id>
        <link href="http://arxiv.org/abs/2208.03567"/>
        <updated>2023-04-18T07:14:03.258Z</updated>
        <summary type="html"><![CDATA[Congyu Fang, Hengrui Jia, Anvith Thudi, Mohammad Yaghini, Christopher
  A. Choquette-Choo, Natalie Dullerud, Varun Chandrasekaran, Nicolas Papernot]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Empirical Bregman Divergence for Uncertain Distance
  Representation]]></title>
        <id>http://arxiv.org/abs/2304.07689</id>
        <link href="http://arxiv.org/abs/2304.07689"/>
        <updated>2023-04-18T07:14:03.253Z</updated>
        <summary type="html"><![CDATA[Zhiyuan Li, Ziru Liu, Anna Zou, Anca L. Ralescu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Learning in Bayesian Neural Networks with Balanced Entropy
  Learning Principle]]></title>
        <id>http://arxiv.org/abs/2105.14559</id>
        <link href="http://arxiv.org/abs/2105.14559"/>
        <updated>2023-04-18T07:14:03.248Z</updated>
        <summary type="html"><![CDATA[Jae Oh Woo]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fatality-Free Dill Sauce]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58480</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58480&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=fatality-free-dill-sauce"/>
        <updated>2023-04-18T02:15:19.000Z</updated>
        <summary type="html"><![CDATA[Amazon screenshot¬†from an anonymous contributor: Remarks by the anonymous contributor: I accidentally reached Amazon's USA website when looking for a source for this excellent dill sauce other than visiting Ikea, 35 miles from here. How could this have arisen? Is it random, or is it something to do with the the pot being made of [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ABC with privacy]]></title>
        <id>http://xianblog.wordpress.com/?p=52746</id>
        <link href="https://xianblog.wordpress.com/2023/04/18/abc-with-privacy/"/>
        <updated>2023-04-17T22:23:10.000Z</updated>
        <summary type="html"><![CDATA[I very recently read a¬† 2021 paper by Mijung Park, Margarita Vinaroz, and Wittawat Jitkrittum on running ABC while ensuring data privacy (published in Entropy). ‚Äú‚Ä¶adding noise to the distance computed on the real observations and pseudo-data suffices the privacy guarantee of the resulting¬† posterior samples‚Äù For ABC tolerance, they use maximum mean discrepancy (MMD) [‚Ä¶]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The perils of AI (Artificial Intelligence) in the PRC]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58453</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58453&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-perils-of-ai-artificial-intelligence-in-the-prc"/>
        <updated>2023-04-17T11:49:28.000Z</updated>
        <summary type="html"><![CDATA[Here at Language Log, for the last couple months, we've been having long, intense discussions about ChatGPT and other AI chatbots and LLM (Large Language Model) applications.¬† Now, it seems that the battle over such AI programs has reached the level of ideological warfare. "America, China and a Crisis of Trust" Opinion | The New [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning-Based Multi-Objective Design Exploration Of Flexible
  Disc Elements]]></title>
        <id>http://arxiv.org/abs/2304.07245</id>
        <link href="http://arxiv.org/abs/2304.07245"/>
        <updated>2023-04-17T09:15:55.375Z</updated>
        <summary type="html"><![CDATA[Gehendra Sharma, Sungkwang Mun, Nayeon Lee, Luke Peterson, Daniela
  Tellkamp, Anand Balu Nellippallil]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ê¢ØÂ∫¶ËßÜËßí‰∏ãÁöÑLoRAÔºöÁÆÄ‰ªã„ÄÅÂàÜÊûê„ÄÅÁåúÊµãÂèäÊé®Âπø]]></title>
        <id>https://kexue.fm/archives/9590</id>
        <link href="https://kexue.fm/archives/9590"/>
        <updated>2023-04-17T08:46:00.000Z</updated>
        <summary type="html"><![CDATA[ÈöèÁùÄChatGPTÂèäÂÖ∂Âπ≥ÊõøÁöÑÁÅ´ÁÉ≠ÔºåÂêÑÁßçÂèÇÊï∞È´òÊïàÔºàParameter-EfficientÔºâÁöÑÂæÆË∞ÉÊñπÊ≥ï‰πü‚ÄúÊ∞¥Ê∂®ËàπÈ´ò‚ÄùÔºåÂÖ∂‰∏≠ÊúÄÊµÅË°åÁöÑÊñπÊ°à‰πã‰∏ÄÂ∞±ÊòØÊú¨ÊñáÁöÑ‰∏ªËßíLoRA‰∫ÜÔºåÂÆÉÂá∫Ëá™ËÆ∫Êñá„ÄäLoRA: Low-R...]]></summary>
        <author>
            <name>ËãèÂâëÊûó</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inference and dynamic decision-making for deteriorating systems with
  probabilistic dependencies through Bayesian networks and deep reinforcement
  learning]]></title>
        <id>http://arxiv.org/abs/2209.01092</id>
        <link href="http://arxiv.org/abs/2209.01092"/>
        <updated>2023-04-17T08:20:11.339Z</updated>
        <summary type="html"><![CDATA[Pablo G. Morato, Charalampos P. Andriotis, Konstantinos G.
  Papakonstantinou, Philippe Rigo]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monitoring machine learning (ML)-based risk prediction algorithms in the
  presence of confounding medical interventions]]></title>
        <id>http://arxiv.org/abs/2211.09781</id>
        <link href="http://arxiv.org/abs/2211.09781"/>
        <updated>2023-04-17T07:14:30.447Z</updated>
        <summary type="html"><![CDATA[Jean Feng, Alexej Gossmann, Gene Pennello, Nicholas Petrick, Berkman
  Sahiner, Romain Pirracchio]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixed moving average field guided learning for spatio-temporal data]]></title>
        <id>http://arxiv.org/abs/2301.00736</id>
        <link href="http://arxiv.org/abs/2301.00736"/>
        <updated>2023-04-17T07:14:30.438Z</updated>
        <summary type="html"><![CDATA[Imma Valentina Curato, Orkun Furat, Bennet Stroeh]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment]]></title>
        <id>http://arxiv.org/abs/2304.06767</id>
        <link href="http://arxiv.org/abs/2304.06767"/>
        <updated>2023-04-17T07:14:29.414Z</updated>
        <summary type="html"><![CDATA[Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng
  Zhang, Kashun Shum, Tong Zhang]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deformed semicircle law and concentration of nonlinear random matrices
  for ultra-wide neural networks]]></title>
        <id>http://arxiv.org/abs/2109.09304</id>
        <link href="http://arxiv.org/abs/2109.09304"/>
        <updated>2023-04-17T07:14:29.106Z</updated>
        <summary type="html"><![CDATA[Zhichao Wang, Yizhe Zhu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SurvSHAP(t): Time-dependent explanations of machine learning survival
  models]]></title>
        <id>http://arxiv.org/abs/2208.11080</id>
        <link href="http://arxiv.org/abs/2208.11080"/>
        <updated>2023-04-17T07:14:29.081Z</updated>
        <summary type="html"><![CDATA[Mateusz Krzyzi\'nski, Miko{\l}aj Spytek, Hubert Baniecki,
  Przemys{\l}aw Biecek]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[irreverent Mike [in memoriam]]]></title>
        <id>http://xianblog.wordpress.com/?p=52818</id>
        <link href="https://xianblog.wordpress.com/2023/04/17/irreverent-mike-in-memoriam/"/>
        <updated>2023-04-16T22:23:12.000Z</updated>
        <summary type="html"><![CDATA[While I could not find an on-line picture of Mike Titterington, another testimony to his modesty and selflessness, I remembered this series of sketches on priors he made for the Bulletin in Applied Statistics in 1982, under the title Irreverent Bayes!]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thailish, part 2]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58439</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58439&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=thailish-part-2"/>
        <updated>2023-04-16T17:23:27.000Z</updated>
        <summary type="html"><![CDATA[The Thai on the sign says: t…ïut sin sut rot¬†k ∞en ‡∏à‡∏∏‡∏î‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏£‡∏ñ‡πÄ‡∏Ç‡πá‡∏ô "end of wheelchair [pathway]" The "pathway" is omitted in the Thai sign, so the English version is the literal translation of the sign. More grammatically, the Chinese would say: l√∫ny«ê d√†ol√π de j√¨nt√≥u ËΩÆÊ§ÖÈÅìË∑ØÁöÑÂ∞ΩÂ§¥ "end of the wheelchair pathway" or, more naturally: l√∫ny«ê j√¨nzh«ê [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The hand of GOD GPT]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58457</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58457&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-hand-of-god-gpt"/>
        <updated>2023-04-16T12:25:38.000Z</updated>
        <summary type="html"><![CDATA[A VentureBeat story by Michael Kerner, "Cohere expands enterprise LLM efforts with LivePerson partnership" (4/11/2023), leads with this image: ‚Ä¶memetically referencing a widely-reproduced detail from Michelangelo's Sistine Chapel fresco Creazione di Adamo: Is this taking current AI hype a step too far, portraying cohere.ai's latest Large Language Model as the creator? Or is it a [‚Ä¶]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An example of ChatGPT "hallucinating"?]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58450</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58450&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=an-example-of-chatgpt-hallucinating"/>
        <updated>2023-04-16T11:20:33.000Z</updated>
        <summary type="html"><![CDATA[Definition In artificial intelligence (AI), a hallucination or artificial hallucination (also occasionally called delusion) is a confident response by an AI that does not seem to be justified by its training data. (source) I had mentioned such AI hallucinating in a previous post once or twice (see "Selected readings"), so it's good to have a [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[instantanea di Venezia¬≤ [jatp]]]></title>
        <id>http://xianblog.wordpress.com/?p=52725</id>
        <link href="https://xianblog.wordpress.com/2023/04/16/instantanea-di-venezia%c2%b2-jatp/"/>
        <updated>2023-04-15T22:23:15.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Digraphs in the gossip column]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58446</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58446&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=digraphs-in-the-gossip-column"/>
        <updated>2023-04-15T13:51:50.000Z</updated>
        <summary type="html"><![CDATA[Today's xkcd ‚Äî "Linguistics Gossip": The mouseover title: "The E's wedding invitation definitely used the word LOVE more times than was strictly necessary." As the explain xkcd wiki explains, AR and VE are not (?) actual typographical ligatures, but just potential couplings.]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curiously curious]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58435</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58435&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=curiously-curious"/>
        <updated>2023-04-15T11:44:48.000Z</updated>
        <summary type="html"><![CDATA[Victoria Bisset, "What to know about Arcturus, a new coronavirus subvariant the WHO is tracking", WaPo 4/14/2023: According to the WHO, Arcturus is similar to the prevalent XBB. 1.5 variant, but has ‚Äúone additional mutational mutation in the spike protein, which in lab studies shows increased infectivity, as well as potential increased pathogenicity.‚Äù Daniel Deutsch, [‚Ä¶]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data, information, knowledge, insight, wisdom, and Conspiracy Theory, part 2]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58430</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58430&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=data-information-knowledge-insight-wisdom-and-conspiracy-theory-part-2"/>
        <updated>2023-04-15T11:27:11.000Z</updated>
        <summary type="html"><![CDATA[From Phillip Remaker: Loved your deep dive on finding the provenance of the "conspiracy theory" image. The one that claimed authorship clipped¬†the edge of the unicorn tail. ¬† The only version I have found that doesn't clip the edge of the unicorn tail is this one from farhan ¬† I don't know if that means [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Experience the power of PyTorch 2.0 on AMD Solutions]]></title>
        <id>https://pytorch.org/blog/experience-power-pytorch-2.0/</id>
        <link href="https://pytorch.org/blog/experience-power-pytorch-2.0/"/>
        <updated>2023-04-15T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[PyTorch 2.0 represents a significant step forward for the PyTorch machine learning framework. The stable release of PyTorch 2.0 brings new features that unlock even higher performance, while remaining backward compatible with prior releases and retaining the Pythonic focus which has helped to make PyTorch so enthusiastically adopted by the AI/ML community. AMD has long been a strong proponent of PyTorch, and we are delighted that the PyTorch 2.0 stable release includes support for AMD Instinct‚Ñ¢ and Radeon‚Ñ¢ GPUs that are supported by the ROCm‚Ñ¢ software platform.]]></summary>
        <author>
            <name>PyTorch Website</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[instantanea di Venezia¬π [jatp]]]></title>
        <id>http://xianblog.wordpress.com/?p=52722</id>
        <link href="https://xianblog.wordpress.com/2023/04/15/instantanea-di-venezia%c2%b9-jatp/"/>
        <updated>2023-04-14T22:23:38.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond automatic differentiation]]></title>
        <id>http://ai.googleblog.com/2023/04/beyond-automatic-differentiation.html</id>
        <link href="http://ai.googleblog.com/2023/04/beyond-automatic-differentiation.html"/>
        <updated>2023-04-14T15:00:00.001Z</updated>
        <summary type="html"><![CDATA[Posted by Matthew Streeter, Software Engineer, Google Research




Derivatives play a central role in optimization and machine learning. By locally approximating a training loss, derivatives guide an optimizer toward lower values of the loss. Automatic differentiation frameworks such as TensorFlow, PyTorch, and JAX are an essential part of modern machine learning, making it feasible to use gradient-based optimizers to train very complex models.


 


But are derivatives all we need? By themselves, derivatives only tell us how a function behaves on an infinitesimal scale. To use derivatives effectively, we often need to know more than that. For example, to choose a learning rate for gradient descent, we need to know something about how the loss function behaves over a small but finite windo‚Ä¶]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uri Hasson - Deep language models as a cognitive model for natural language processing in the human brain]]></title>
        <id>https://mlfoundations.org/talk/hasson/</id>
        <link href="https://mlfoundations.org/talk/hasson/"/>
        <updated>2023-04-14T14:00:00.000Z</updated>
        <summary type="html"><![CDATA[Naturalistic experimental paradigms in cognitive neuroscience arose from a pressure to test, in real-world contexts, the validity of models we derive from highly controlled laboratory experiments. In many cases, however, such efforts led to the realization that models (i.e., explanatory principles) developed under particular experimental manipulations fail to capture many aspects of reality (variance) in the real world. Recent advances in artificial neural networks provide an alternative computational framework for modeling cognition in natural contexts. In contrast to the simplified and interpretable hypotheses we test in the lab, these models learn how to act in the world from massive amounts of real-world examples (big data) by optimizing big models with millions to billions of parameters. Surprisingly, such models' performance matches human performances on many cognitive tasks, including visual perceptions, language processing, and motor control. At the same time, these models sacrifice understanding in favor of competence by being able to act without knowing why their choices are optimal or preferable in a given context. In this talk, I will ask whether the human brain's underlying computations are similar or different from the underlying computations in deep neural networks. The ability to think and reason using natural language separates us from other animals and machines. In the talk, I will focus on the underlying neural process that supports natural language processing. Our study aims to model natural language processing in the wild. To that end, we recorded intracranial EEG data from epileptic patients while they engaged in open-ended free conversations with their doctors, friends, and family members during a week-long stay in the hospital. I will provide evidence that our neural code shares some computational principles with deep language models. This indicates that, to some extent, the brain relies on overparameterized optimization methods to comprehend and produce language. At the same time, I will present evidence that the brain differs from deep language models as speakers try to convey new ideas and thoughts. Together, our findings expose some unexpected similarities to deep neural networks while pointing to crucial human-centric missing properties in these machines.]]></summary>
        <author>
            <name>Harvard ML Foundations</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KL-divergence Based Deep Learning for Discrete Time Model]]></title>
        <id>http://arxiv.org/abs/2208.05100</id>
        <link href="http://arxiv.org/abs/2208.05100"/>
        <updated>2023-04-14T07:13:41.099Z</updated>
        <summary type="html"><![CDATA[Li Liu, Xiangeng Fang, Di Wang, Weijing Tang, Kevin He]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Inferential Reproducibility of Machine Learning Research]]></title>
        <id>http://arxiv.org/abs/2302.04054</id>
        <link href="http://arxiv.org/abs/2302.04054"/>
        <updated>2023-04-14T07:13:40.808Z</updated>
        <summary type="html"><![CDATA[Michael Hagmann, Philipp Meier, Stefan Riezler]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Bayes Inference in Neural Networks through Adaptive Importance
  Sampling]]></title>
        <id>http://arxiv.org/abs/2210.00993</id>
        <link href="http://arxiv.org/abs/2210.00993"/>
        <updated>2023-04-14T07:13:40.713Z</updated>
        <summary type="html"><![CDATA[Yunshi Huang, Emilie Chouzenoux, Victor Elvira and
  Jean-Christophe Pesquet]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying and Explaining Machine Learning Uncertainty in Predictive
  Process Monitoring: An Operations Research Perspective]]></title>
        <id>http://arxiv.org/abs/2304.06412</id>
        <link href="http://arxiv.org/abs/2304.06412"/>
        <updated>2023-04-14T07:13:40.068Z</updated>
        <summary type="html"><![CDATA[Nijat Mehdiyev, Maxim Majlatow, Peter Fettke]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do deep neural networks have an inbuilt Occam's razor?]]></title>
        <id>http://arxiv.org/abs/2304.06670</id>
        <link href="http://arxiv.org/abs/2304.06670"/>
        <updated>2023-04-14T07:13:40.062Z</updated>
        <summary type="html"><![CDATA[Chris Mingard, Henry Rees, Guillermo Valle-P\'erez, Ard A.
  Louis]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerated Generative Diffusion Models with PyTorch 2]]></title>
        <id>https://pytorch.org/blog/accelerated-generative-diffusion-models/</id>
        <link href="https://pytorch.org/blog/accelerated-generative-diffusion-models/"/>
        <updated>2023-04-14T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[TL;DR: PyTorch 2.0 nightly offers out-of-the-box performance improvement for Generative Diffusion models by using the new torch.compile() compiler and optimized implementations of Multihead Attention integrated with PyTorch 2.]]></summary>
        <author>
            <name>PyTorch Website</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hai-t'ao Tang (1931-2023)]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58427</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58427&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=hai-tao-tang-1931-2023"/>
        <updated>2023-04-14T03:28:24.000Z</updated>
        <summary type="html"><![CDATA[From the Princeton University Department of East Asian Studies newsletter (3/26/23): Passing of Emeritus lecturer Hai-t‚Äôao Tang Emeritus lecturer, Hai-t‚Äôao Tang passed away at his Princeton home on Sunday, March 26, 2023. He was born August 27, 1931, in Shanghai, China and completed his master‚Äôs degree in Chinese Literature at National Taiwan University. He joined [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mike Titterington (1945-2023)]]></title>
        <id>http://xianblog.wordpress.com/?p=52812</id>
        <link href="https://xianblog.wordpress.com/2023/04/14/mike-titterington-1945-2023/"/>
        <updated>2023-04-13T22:23:43.000Z</updated>
        <summary type="html"><![CDATA[Most sadly, I just heard from Glasgow that my friend and coauthor Mike Titterington passed away last weekend. While a significant figure in the field and a precursor in many ways, from mixtures to machine learning, Mike was one of the kindest persons ever, tolerant to a fault and generous with his time, and I [‚Ä¶]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robotic deep RL at scale: Sorting waste and recyclables with a fleet of robots]]></title>
        <id>http://ai.googleblog.com/2023/04/robotic-deep-rl-at-scale-sorting-waste.html</id>
        <link href="http://ai.googleblog.com/2023/04/robotic-deep-rl-at-scale-sorting-waste.html"/>
        <updated>2023-04-13T19:31:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Sergey Levine, Research Scientist, and Alexander Herzog, Staff Research Software Engineer, Google Research, Brain Team
  
  
Reinforcement learning (RL) can enable robots to learn complex behaviors through trial-and-error interaction, getting better and better over time. Several of our prior works explored how RL can enable intricate robotic skills, such as robotic grasping, multi-task learning, and even playing table tennis. Although robotic RL has come a long way, we still don't see RL-enabled robots in everyday settings. The real world is complex, diverse, and changes over time, presenting a major challenge for robotic systems. However, we believe that RL should offer us an excellent tool for tackling precisely these challenges: by continually practicing, getting better, and l‚Ä¶]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The "socialite" phenomenon in China]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58423</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58423&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-socialite-phenomenon-in-china"/>
        <updated>2023-04-13T11:12:42.000Z</updated>
        <summary type="html"><![CDATA[Source: China Media Project (12/7/2022)THE CMP DICTIONARY: Socialite Â™õBy¬†XINYU DENG Once signifying graceful women of a distinguished background, the term ‚Äúsocialite,‚Äù or yuan (Â™õ), has in recent years become a misogynistic umbrella term used on digital platforms in China to disparage women who advertise fancy lifestyles. The term has also been used by state-run media [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SoK: Certified Robustness for Deep Neural Networks]]></title>
        <id>http://arxiv.org/abs/2009.04131</id>
        <link href="http://arxiv.org/abs/2009.04131"/>
        <updated>2023-04-13T07:13:53.212Z</updated>
        <summary type="html"><![CDATA[Linyi Li, Tao Xie, Bo Li]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Certified Robustness via Statistical Learning with Logical
  Reasoning]]></title>
        <id>http://arxiv.org/abs/2003.00120</id>
        <link href="http://arxiv.org/abs/2003.00120"/>
        <updated>2023-04-13T07:13:53.194Z</updated>
        <summary type="html"><![CDATA[Zhuolin Yang, Zhikuan Zhao, Boxin Wang, Jiawei Zhang, Linyi Li,
  Hengzhi Pei, Bojan Karlas, Ji Liu, Heng Guo, Ce Zhang, Bo Li]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Criticality versus uniformity in deep neural networks]]></title>
        <id>http://arxiv.org/abs/2304.04784</id>
        <link href="http://arxiv.org/abs/2304.04784"/>
        <updated>2023-04-13T07:13:53.186Z</updated>
        <summary type="html"><![CDATA[Aleksandar Bukva, Jurriaan de Gier, Kevin T. Grosvenor, Ro Jefferson,
  Koenraad Schalm, Eliot Schwander]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selecting Robust Features for Machine Learning Applications using
  Multidata Causal Discovery]]></title>
        <id>http://arxiv.org/abs/2304.05294</id>
        <link href="http://arxiv.org/abs/2304.05294"/>
        <updated>2023-04-13T07:13:52.489Z</updated>
        <summary type="html"><![CDATA[Saranya Ganesh S., Tom Beucler, Frederick Iat-Hin Tam, Milton S.
  Gomez, Jakob Runge, Andreas Gerhardus]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StatMathAppli, Fr√©jus, France [18-22 September 2023]]]></title>
        <id>http://xianblog.wordpress.com/?p=52493</id>
        <link href="https://xianblog.wordpress.com/2023/04/13/statmathappli-frejus-france-18-22-september-2023/"/>
        <updated>2023-04-12T22:23:22.000Z</updated>
        <summary type="html"><![CDATA[The bi-yearly StatMathAppli conference will take place next September in Fr√©jus, France, with guest lecturers Marco Cuturi (ENSAE & Apple ML Research) who will a lecture on ‚ÄúOptimal Transport: From Theory to Tweaks, Computations and Applications in Machine Learning‚Äù, and Gareth O. Roberts (University of Warwick) who will give a lecture on ‚ÄúTopics in Retrospective [‚Ä¶]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniPi: Learning universal policies via text-guided video generation]]></title>
        <id>http://ai.googleblog.com/2023/04/unipi-learning-universal-policies-via.html</id>
        <link href="http://ai.googleblog.com/2023/04/unipi-learning-universal-policies-via.html"/>
        <updated>2023-04-12T20:04:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Sherry Yang, Research Scientist, and Yilun Du, Student Researcher, Google Research, Brain Team
Building models that solve a diverse set of tasks has become a dominant paradigm in the domains of vision and language. In natural language processing, large pre-trained models, such as PaLM, GPT-3 and Gopher, have demonstrated remarkable zero-shot learning of new language tasks. Similarly, in computer vision, models like CLIP and Flamingo have shown robust performance on zero-shot classification and object recognition. A natural next step is to use such tools to construct agents that can complete different decision-making tasks across many environments.



However, training such agents faces the inherent challenge of environmental diversity, since different environments operate with di‚Ä¶]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Sutradhar and the Ringgit: A Study of Terms Related to the Early Puppet Theatres]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58410</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58410&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-sutradhar-and-the-ringgit-a-study-of-terms-related-to-the-early-puppet-theatres"/>
        <updated>2023-04-12T16:17:25.000Z</updated>
        <summary type="html"><![CDATA[Sino-Platonic Papers is pleased to announce the publication of its three-hundred-and-thirty-second issue: ‚ÄúThe Sutradhar and the Ringgit: A Study of Terms Related to the Early Puppet Theatres,‚Äù by Keith Rawlings. ABSTRACT Certain words in Sanskrit, Old Javanese, and Ancient Greek that appear in centuries-old texts are thought by many scholars to be early references to [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thoughts on AI safety]]></title>
        <id>http://windowsontheory.org/?p=8591</id>
        <link href="https://windowsontheory.org/2023/04/12/thoughts-on-ai-safety/"/>
        <updated>2023-04-12T15:25:45.000Z</updated>
        <summary type="html"><![CDATA[Last week, I gave a lecture on AI safety as part of my deep learning foundations course. In this post, I‚Äôll try to write down a few of my thoughts on this topic. (The lecture was three hours, and this blog post will not cover all of what we discussed or all the points that ‚Ä¶ Continue reading Thoughts on AI¬†safety]]></summary>
        <author>
            <name>Boaz Barak</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thoughts on AI safety]]></title>
        <id>http://windowsontheory.org/?p=8591</id>
        <link href="https://windowsontheory.org/2023/04/12/thoughts-on-ai-safety/"/>
        <updated>2023-04-12T15:25:45.000Z</updated>
        <summary type="html"><![CDATA[Last week, I gave a lecture on AI safety as part of my deep learning foundations course. In this post, I‚Äôll try to write down a few of my thoughts on this topic. (The lecture was three hours, and this blog post will not cover all of what we discussed or all the points that ‚Ä¶ Continue reading Thoughts on AI¬†safety]]></summary>
        <author>
            <name>Boaz Barak</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Êä•ÂêçÂºÄÂêØÔºÅÊàêÈÉΩÈ¶ñ‰∏™ChatGPTÂíåÂ§ßÊ®°Âûã‰∏ìÈ¢òÁ†îËÆ®‰ºöÔºåÊúüÂæÖÊÇ®ÁöÑÂèÇ‰∏éÔºÅ]]></title>
        <id>https://www.52nlp.cn/?p=13454</id>
        <link href="https://www.52nlp.cn/%e6%8a%a5%e5%90%8d%e5%bc%80%e5%90%af%ef%bc%81%e6%88%90%e9%83%bd%e9%a6%96%e4%b8%aachatgpt%e5%92%8c%e5%a4%a7%e6%a8%a1%e5%9e%8b%e4%b8%93%e9%a2%98%e7%a0%94%e8%ae%a8%e4%bc%9a%ef%bc%8c%e6%9c%9f%e5%be%85"/>
        <updated>2023-04-12T08:17:15.000Z</updated>
        <summary type="html"><![CDATA[Âú®ÂΩì‰ªäÁöÑÊï∞Â≠óÊó∂‰ª£Ôºå‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÂ∑≤Êàê‰∏∫Êé®Âä®Á§æ‰ºöÂèëÂ±ïÁöÑÈáçË¶ÅÈ©±Âä®Âäõ„ÄÇ‰Ωú‰∏∫‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüüÁöÑÁÉ≠ÁÇπ‰πã‰∏ÄÔºåChatGPTÂíåÂ§ßÊ®° [‚Ä¶]]]></summary>
        <author>
            <name>ËææËßÇÊï∞ÊçÆ</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Âπ¥Â∫¶AIÂØπËØù|ËææËßÇÈôàËøêÊñáÔºöÂÅöÁßÅÊúâÈÉ®ÁΩ≤ÁöÑÂ§ßÊ®°ÂûãÔºåÊï∞ÊçÆÂÆâÂÖ®Êõ¥ÂÖ∑‰ºòÂäø]]></title>
        <id>https://www.52nlp.cn/?p=13451</id>
        <link href="https://www.52nlp.cn/%e5%b9%b4%e5%ba%a6ai%e5%af%b9%e8%af%9d%e8%be%be%e8%a7%82%e9%99%88%e8%bf%90%e6%96%87%ef%bc%9a%e5%81%9a%e7%a7%81%e6%9c%89%e9%83%a8%e7%bd%b2%e7%9a%84%e5%a4%a7%e6%a8%a1%e5%9e%8b%ef%bc%8c%e6%95%b0"/>
        <updated>2023-04-12T07:59:23.000Z</updated>
        <summary type="html"><![CDATA[Êú¨ÊñáÂÜÖÂÆπÊù•Ê∫ê‰∫éËææËßÇÊï∞ÊçÆËë£‰∫ãÈïøÂÖºCEOÈôàËøêÊñáÂèóÈÇÄÂá∫Â∏≠36Ê∞™Êï∞Â≠óÊó∂Ê∞™ÁöÑ„ÄäÂπ¥Â∫¶AIÂØπËØù„Äã‰∏ìÊ†èÈááËÆøÂÜÖÂÆπ Â§ßÊ®°ÂûãÊó¢Â∏¶Êù•‰∫Ü [‚Ä¶]]]></summary>
        <author>
            <name>ËææËßÇÊï∞ÊçÆ</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ËææËßÇÂä©ÊâãAIÊô∫ËÉΩÂÜô‰ΩúÔºåÂÖ®Êñπ‰ΩçÊô∫ËÉΩÂÜô‰ΩúÊñ∞‰ΩìÈ™å]]></title>
        <id>https://www.52nlp.cn/?p=13449</id>
        <link href="https://www.52nlp.cn/%e8%be%be%e8%a7%82%e5%8a%a9%e6%89%8bai%e6%99%ba%e8%83%bd%e5%86%99%e4%bd%9c%ef%bc%8c%e5%85%a8%e6%96%b9%e4%bd%8d%e6%99%ba%e8%83%bd%e5%86%99%e4%bd%9c%e6%96%b0%e4%bd%93%e9%aa%8c"/>
        <updated>2023-04-12T07:44:40.000Z</updated>
        <summary type="html"><![CDATA[ËææËßÇÂä©ÊâãÊòØ‰∏ÄÊ¨æÊô∫ËÉΩËæÖÂä©ÂÜô‰ΩúÁöÑwpsÊèí‰ª∂Â∑•ÂÖ∑ÔºåÈááÁî®‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÂºÄÂèëÔºåÊã•ÊúâÂ§öÈ°πÂÆûÁî®ÂäüËÉΩÔºåÂ¶ÇAIÁª≠ÂÜô„ÄÅAIÊ∂¶Ëâ≤„ÄÅÊô∫ËÉΩ [‚Ä¶]]]></summary>
        <author>
            <name>ËææËßÇÊï∞ÊçÆ</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ÂÖ®ÂõΩÈ¶ñÂ±äÂ§ßÊ®°ÂûãÂàõÊñ∞ÂàõÊÑèÂ∫îÁî®Â§ßËµõÂºÄÂêØÔºåÁ≠â‰Ω†Êù•ËµõÔºÅ]]></title>
        <id>https://www.52nlp.cn/?p=13447</id>
        <link href="https://www.52nlp.cn/%e5%85%a8%e5%9b%bd%e9%a6%96%e5%b1%8a%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%88%9b%e6%96%b0%e5%88%9b%e6%84%8f%e5%ba%94%e7%94%a8%e5%a4%a7%e8%b5%9b%e5%bc%80%e5%90%af%ef%bc%8c%e7%ad%89%e4%bd%a0%e6%9d%a5%e8%b5%9b"/>
        <updated>2023-04-12T07:28:06.000Z</updated>
        <summary type="html"><![CDATA[ËøëÊúüÔºå‰ª•ChatGPTÂíåGPT4‰∏∫‰ª£Ë°®ÁöÑÂ§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÈôÜÁª≠ÂèëÂ∏ÉÔºåÂ∏¶Âä®‰∫Ü‰∫∫Â∑•Êô∫ËÉΩ‰∫ß‰∏öÁöÑÂø´ÈÄüÂèëÂ±ï„ÄÇÂ§ßÊ®°ÂûãÂ∑≤ÁªèÊàê‰∏∫ÂÆû [‚Ä¶]]]></summary>
        <author>
            <name>ËææËßÇÊï∞ÊçÆ</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Learning and Signal Propagation in Deep Neural Networks]]></title>
        <id>http://arxiv.org/abs/2110.11749</id>
        <link href="http://arxiv.org/abs/2110.11749"/>
        <updated>2023-04-12T07:13:41.804Z</updated>
        <summary type="html"><![CDATA[Yizhang Lou, Chris Mingard, Yoonsoo Nam, Soufiane Hayou]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An analytic theory for the dynamics of wide quantum neural networks]]></title>
        <id>http://arxiv.org/abs/2203.16711</id>
        <link href="http://arxiv.org/abs/2203.16711"/>
        <updated>2023-04-12T07:13:41.797Z</updated>
        <summary type="html"><![CDATA[Junyu Liu, Khadijeh Najafi, Kunal Sharma, Francesco Tacchino, Liang
  Jiang, Antonio Mezzacapo]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient-based Uncertainty Attribution for Explainable Bayesian Deep
  Learning]]></title>
        <id>http://arxiv.org/abs/2304.04824</id>
        <link href="http://arxiv.org/abs/2304.04824"/>
        <updated>2023-04-12T07:13:41.790Z</updated>
        <summary type="html"><![CDATA[Hanjing Wang, Dhiraj Joshi, Shiqiang Wang, Qiang Ji]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On a continuous time model of gradient descent dynamics and instability
  in deep learning]]></title>
        <id>http://arxiv.org/abs/2302.01952</id>
        <link href="http://arxiv.org/abs/2302.01952"/>
        <updated>2023-04-12T07:13:41.771Z</updated>
        <summary type="html"><![CDATA[Mihaela Rosca, Yan Wu, Chongli Qin, Benoit Dherin]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Gradient Descent: Deep Learning without Hyperparameters]]></title>
        <id>http://arxiv.org/abs/2304.05187</id>
        <link href="http://arxiv.org/abs/2304.05187"/>
        <updated>2023-04-12T07:13:40.006Z</updated>
        <summary type="html"><![CDATA[Jeremy Bernstein, Chris Mingard, Kevin Huang, Navid Azizan
 , Yisong Yue]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ËææËßÇÊï∞ÊçÆÁ†îÂèë‚ÄúÊõπÊ§ç‚ÄùÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºåËá¥Âäõ‰∫éÂõΩ‰∫ßGPTÊ®°Âûã]]></title>
        <id>https://www.52nlp.cn/?p=13445</id>
        <link href="https://www.52nlp.cn/%e8%be%be%e8%a7%82%e6%95%b0%e6%8d%ae%e7%a0%94%e5%8f%91%e6%9b%b9%e6%a4%8d%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%ef%bc%8c%e8%87%b4%e5%8a%9b%e4%ba%8e%e5%9b%bd%e4%ba%a7gpt"/>
        <updated>2023-04-12T07:01:37.000Z</updated>
        <summary type="html"><![CDATA[Âú®ÈÄöÁî®È¢ÜÂüüÔºå‰ª•ChatGPT‰∏∫‰ª£Ë°®ÁöÑÁîüÊàêÂºèÂ§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÂ±ïÁé∞Âá∫ÂçìË∂äÁöÑÁü•ËØÜÂ≠¶‰π†ÂíåÊñáÂ≠óÂàõ‰ΩúËÉΩÂäõÔºåÂèóÂà∞ÂõΩÂÜÖÂ§ñÁöÑÂπøÊ≥õÂÖ≥ [‚Ä¶]]]></summary>
        <author>
            <name>ËææËßÇÊï∞ÊçÆ</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixed Thai, English, and Chinese sign]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58404</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58404&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=mixed-thai-english-and-chinese-sign"/>
        <updated>2023-04-12T04:28:39.000Z</updated>
        <summary type="html"><![CDATA[Photograph taken at a park in Chiang Mai, Thailand: In brief (and without marking the Thai tones here), the Thai text comprises two words [sa: la:] ‚Äòpavilion‚Äô and [phak pho:n] ‚Äòto rest‚Äô, i.e., ‚Äòpavilion for resting‚Äô. Complete transcription and explication of the Thai text by Robert S. Bauer (author of the monumental ABC Cantonese-English dictionary [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The genius and logic of French and English]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58399</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58399&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=the-genius-and-logic-of-french-and-english"/>
        <updated>2023-04-12T04:16:46.000Z</updated>
        <summary type="html"><![CDATA[Here are the warning labels on the sun visors in my Toyota Tacoma: In almost every line, the French is longer longer than the English ‚Äî demonstrably and conspicuously so. Why does the French feel more comfortable going on at length, while the English is happy being more concise ‚Äî consistently so in both cases? [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Five stars over China:  Central Kingdom in Central Asia]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58407</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58407&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=five-stars-over-china-central-kingdom-in-central-asia"/>
        <updated>2023-04-12T02:13:44.000Z</updated>
        <summary type="html"><![CDATA[Êñ∞Êó∂‰ª£Á••ÁëûÂ±ÇÂá∫‰∏çÁ©∑ pic.twitter.com/bVm5Vn4XC4 ‚Äî ÊñπËàüÂ≠ê (@fangshimin) April 9, 2023 "Five stars out of the east to benefit the Central Kingdom" Here is the original Weibo post: ‚ÄúW«îxƒ´ng ch≈´ d≈çngfƒÅng l√¨ zh≈çnggu√≥‚Äù H√†nd√†i zhƒ´j«ên h√π b√¨, fƒÅju√© y√∫ Xƒ´njiƒÅng T«ék√®lƒÅm«ég√†n shƒÅm√≤ f√πd√¨ de N√≠y«é y√≠zh«ê, b√®i y√π w√®i 20 sh√¨j√¨ Zh≈çnggu√≥ k«éog«î zu√¨ wƒõid√† de f«éxi√†n zhƒ´ [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[a journal of the plague, sword, and famine year [far North]]]></title>
        <id>http://xianblog.wordpress.com/?p=52652</id>
        <link href="https://xianblog.wordpress.com/2023/04/12/a-journal-of-the-plague-sword-and-famine-year-far-north/"/>
        <updated>2023-04-11T22:23:52.000Z</updated>
        <summary type="html"><![CDATA[Read Le sabre des Takeda (Furin kazan, È¢®ÊûóÁÅ´Â±±) by Inoue Yasushi, a very interesting book set between Japanese history and feudal novel, with surgical descriptions of battles and psychological tensions. This book reminded me in some aspects of a novel of Yoshimura Akira on the earthquake of Kant√¥ by its insistence on minutiae. Even though [‚Ä¶]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Developing an aging clock using deep learning on retinal images]]></title>
        <id>http://ai.googleblog.com/2023/04/developing-aging-clock-using-deep.html</id>
        <link href="http://ai.googleblog.com/2023/04/developing-aging-clock-using-deep.html"/>
        <updated>2023-04-11T18:22:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Sara Ahadi, Research Fellow, Applied Science, and Andrew Carroll, Product Lead, Genomics




Aging is a process that is characterized by physiological and molecular changes that increase an individual's risk of developing diseases and eventually dying. Being able to measure and estimate the biological signatures of aging can help researchers identify preventive measures to reduce disease risk and impact. Researchers have developed ‚Äúaging clocks‚Äù based on markers such as blood proteins or DNA methylation to measure individuals‚Äô biological age, which is distinct from one‚Äôs chronological age. These aging clocks help predict the risk of age-related diseases. But because protein and methylation markers require a blood draw, non-invasive ways to find similar measures could make aging i‚Ä¶]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[‰∏≠ÁßëÈô¢Âº†ÂÆ∂‰øäÔºöChatGPT‰∏≠ÁöÑÊèêÁ§∫‰∏éÊåá‰ª§Â≠¶‰π†]]></title>
        <id>https://www.52nlp.cn/?p=13443</id>
        <link href="https://www.52nlp.cn/%e4%b8%ad%e7%a7%91%e9%99%a2%e5%bc%a0%e5%ae%b6%e4%bf%8a%ef%bc%9achatgpt%e4%b8%ad%e7%9a%84%e6%8f%90%e7%a4%ba%e4%b8%8e%e6%8c%87%e4%bb%a4%e5%ad%a6%e4%b9%a0"/>
        <updated>2023-04-11T10:30:15.000Z</updated>
        <summary type="html"><![CDATA[ÂÜÖÂÆπÊù•Ê∫êÔºöChatGPT ÂèäÂ§ßÊ®°Âûã‰∏ìÈ¢òÁ†îËÆ®‰ºö ÂàÜ‰∫´ÂòâÂÆæÔºö‰∏≠ÂõΩÁßëÂ≠¶Èô¢Ëá™Âä®ÂåñÁ†îÁ©∂ÊâÄÁ†îÁ©∂Âëò Âº†ÂÆ∂‰øä ÂàÜ‰∫´‰∏ªÈ¢òÔºö„ÄäCh [‚Ä¶]]]></summary>
        <author>
            <name>ËææËßÇÊï∞ÊçÆ</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Generative Models of the Geometry and Topology of Tree-like 3D
  Objects]]></title>
        <id>http://arxiv.org/abs/2110.08693</id>
        <link href="http://arxiv.org/abs/2110.08693"/>
        <updated>2023-04-11T07:13:55.203Z</updated>
        <summary type="html"><![CDATA[Guan Wang, Hamid Laga, Anuj Srivastava]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weighted Ensemble Self-Supervised Learning]]></title>
        <id>http://arxiv.org/abs/2211.09981</id>
        <link href="http://arxiv.org/abs/2211.09981"/>
        <updated>2023-04-11T07:13:55.197Z</updated>
        <summary type="html"><![CDATA[Yangjun Ruan, Saurabh Singh, Warren Morningstar, Alexander A. Alemi,
  Sergey Ioffe, Ian Fischer, Joshua V. Dillon]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Theoretical Study of Inductive Biases in Contrastive Learning]]></title>
        <id>http://arxiv.org/abs/2211.14699</id>
        <link href="http://arxiv.org/abs/2211.14699"/>
        <updated>2023-04-11T07:13:55.192Z</updated>
        <summary type="html"><![CDATA[Jeff Z. HaoChen, Tengyu Ma]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Phase Diagram of Initial Condensation for Two-layer Neural Networks]]></title>
        <id>http://arxiv.org/abs/2303.06561</id>
        <link href="http://arxiv.org/abs/2303.06561"/>
        <updated>2023-04-11T07:13:55.186Z</updated>
        <summary type="html"><![CDATA[Zhengan Chen, Yuqing Li, Tao Luo, Zhangchen Zhou, Zhi-Qin John Xu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian learning of Causal Structure and Mechanisms with GFlowNets and
  Variational Bayes]]></title>
        <id>http://arxiv.org/abs/2211.02763</id>
        <link href="http://arxiv.org/abs/2211.02763"/>
        <updated>2023-04-11T07:13:55.180Z</updated>
        <summary type="html"><![CDATA[Mizu Nishikawa-Toomey, Tristan Deleu, Jithendaraa Subramanian, Yoshua
  Bengio, Laurent Charlin]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[flying static]]></title>
        <id>http://xianblog.wordpress.com/?p=52676</id>
        <link href="https://xianblog.wordpress.com/2023/04/11/flying-static/"/>
        <updated>2023-04-11T06:23:02.000Z</updated>
        <summary type="html"><![CDATA[[While flying to Venezia, two art professionals were sitting in front of me, talking the entirety of the flight in a rather affected way.] ‚ÄúLa Fenice, j‚Äôai pas du tout envie d‚Äôaller l√†. C‚Äôest pas int√©ressant, j‚Äôirais beaucoup plus vers le cin√©ma italien classique des ann√©es 60, avec une vision beaucoup plus sophistiqu√©e‚Ä¶‚Äù]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[state of the [des]Union]]></title>
        <id>http://xianblog.wordpress.com/?p=52784</id>
        <link href="https://xianblog.wordpress.com/2023/04/10/state-of-the-desunion/"/>
        <updated>2023-04-10T14:07:26.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[‰ªéJLÂºïÁêÜÁúãÁÜµ‰∏çÂèòÊÄßAttention]]></title>
        <id>https://kexue.fm/archives/9588</id>
        <link href="https://kexue.fm/archives/9588"/>
        <updated>2023-04-10T08:12:00.000Z</updated>
        <summary type="html"><![CDATA[Âú®„Ää‰ªéÁÜµ‰∏çÂèòÊÄßÁúãAttentionÁöÑScaleÊìç‰Ωú„Äã„ÄÅ„ÄäÁÜµ‰∏çÂèòÊÄßSoftmaxÁöÑ‰∏Ä‰∏™Âø´ÈÄüÊé®ÂØº„Äã‰∏≠Á¨îËÄÖÊèêÂá∫‰∫ÜÁÜµ‰∏çÂèòÊÄßSoftmaxÔºåÁÆÄÂçïÊù•ËØ¥Â∞±ÊòØÂæÄSoftmax‰πãÂâçÁöÑAttentionÁü©ÈòµÂ§ö‰πò‰∏ä‰∏Ä...]]></summary>
        <author>
            <name>ËãèÂâëÊûó</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean
  Field Neural Networks]]></title>
        <id>http://arxiv.org/abs/2304.03408</id>
        <link href="http://arxiv.org/abs/2304.03408"/>
        <updated>2023-04-10T07:13:50.234Z</updated>
        <summary type="html"><![CDATA[Blake Bordelon, Cengiz Pehlevan]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modality-Agnostic Variational Compression of Implicit Neural
  Representations]]></title>
        <id>http://arxiv.org/abs/2301.09479</id>
        <link href="http://arxiv.org/abs/2301.09479"/>
        <updated>2023-04-10T07:13:50.229Z</updated>
        <summary type="html"><![CDATA[Jonathan Richard Schwarz, Jihoon Tack, Yee Whye Teh, Jaeho
  Lee, Jinwoo Shin]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Replicability and stability in learning]]></title>
        <id>http://arxiv.org/abs/2304.03757</id>
        <link href="http://arxiv.org/abs/2304.03757"/>
        <updated>2023-04-10T07:13:50.223Z</updated>
        <summary type="html"><![CDATA[Zachary Chase, Shay Moran, Amir Yehudayoff]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supervised Contrastive Learning with Heterogeneous Similarity for
  Distribution Shifts]]></title>
        <id>http://arxiv.org/abs/2304.03440</id>
        <link href="http://arxiv.org/abs/2304.03440"/>
        <updated>2023-04-10T07:13:49.496Z</updated>
        <summary type="html"><![CDATA[Takuro Kutsuna]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Centuries-old Quran being treated in a Taiwan "book hospital"]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58393</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58393&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=centuries-old-quran-being-treated-in-a-taiwan-book-hospital"/>
        <updated>2023-04-10T03:42:57.000Z</updated>
        <summary type="html"><![CDATA[Article in¬†Taiwan News¬†(4/9/23): National Taiwan Library repairs 500-year-old Quran'Book Hospital' tasked with repairing ancient Quran damaged by time, elementsBy Sean Scanlan 500-year-old Quran being repaired (CNA photo) The National Taiwan Library accepted a commission to restore a 500-year-old handwritten version of the Quran. The ancient text was given to Tzu Chi Buddhist Humanitarian Foundation Master [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[religions in the classroom]]></title>
        <id>http://xianblog.wordpress.com/?p=52661</id>
        <link href="https://xianblog.wordpress.com/2023/04/10/religions-in-the-classroom/"/>
        <updated>2023-04-09T22:23:36.000Z</updated>
        <summary type="html"><![CDATA[Two recent stories reported in the New York Times about U.S. professors being fired for posting art pieces that students or parents found offensive to their beliefs. One (above) was a painting within a 14th-century Islamic history book supposed to represent G and M. As showed [with much warning] during an art class at Hamline [‚Ä¶]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perspective: How to overcome dynamical density functional theory]]></title>
        <id>http://arxiv.org/abs/2301.12156</id>
        <link href="http://arxiv.org/abs/2301.12156"/>
        <updated>2023-04-09T07:12:22.577Z</updated>
        <summary type="html"><![CDATA[Daniel de las Heras, Toni Zimmermann, Florian Samm\"uller, Sophie
  Hermann, Matthias Schmidt]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximating the Prime Counting Function via an Operation on a Unique
  Prime Number Subsequence]]></title>
        <id>http://arxiv.org/abs/2112.08941</id>
        <link href="http://arxiv.org/abs/2112.08941"/>
        <updated>2023-04-09T07:12:22.569Z</updated>
        <summary type="html"><![CDATA[Michael P. May]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Please, please, please, please, please]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58386</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58386&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=please-please-please-please-please"/>
        <updated>2023-04-09T04:36:48.000Z</updated>
        <summary type="html"><![CDATA[The Chinese on the sign says: Wƒìnxƒ´n t√≠sh√¨ Ê∫´È¶®ÊèêÁ§∫ ("Gentle Reminder") b«éoch√≠ ƒÅnj√¨ng ‰øùÊåÅÂÆâÈùô ("keep quiet") b«éoch√≠ qƒ´ngji√© ‰øùÊåÅÊ∏ÖÊ¥Å ("keep clean") q«êng w√π xƒ´yƒÅn ËØ∑ÂãøÂê∏ÁÉü ("no smoking") q«êng w√π pƒÅish√® ËØ∑ÂãøÊãçÊëÑ ("no photos") q«êng w√π f«îm≈ç ËØ∑ÂãøÊäöÊë∏ ("don't touch") Such politeness! Selected readings "Beginning of the Semester Blues" (9/5/10) "Gentle reminder for women who approach [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating means of bounded random variables by betting]]></title>
        <id>http://xianblog.wordpress.com/?p=52728</id>
        <link href="https://xianblog.wordpress.com/2023/04/09/estimating-means-of-bounded-random-variables-by-betting/"/>
        <updated>2023-04-08T22:23:18.000Z</updated>
        <summary type="html"><![CDATA[Ian Waudby-Smith and Aaditya Ramdas are presenting next month a Read Paper to the Royal Statistical Society in London on constructing a conservative confidence interval on the mean of a bounded random variable. Here is an extended abstract from within the paper: For each m ‚àà [0, 1], we set up a ‚Äúfair‚Äù multi-round game [‚Ä¶]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combinatory Sound Alternations in Proto-, Pre-, and Real Tibetan]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58375</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58375&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=combinatory-sound-alternations-in-proto-pre-and-real-tibetan"/>
        <updated>2023-04-08T12:24:50.000Z</updated>
        <summary type="html"><![CDATA[Sino-Platonic Papers is pleased to announce the publication of its three-hundred-and-thirty-first issue: ‚ÄúCombinatory Sound Alternations in Proto-, Pre-, and Real Tibetan: The Case of the Word Family *Mra(o) ‚ÄòSpeak,‚Äô ‚ÄòSpeaker,‚Äô ‚ÄòHuman,‚Äô ‚ÄòLord‚Äô,‚Äù by Bettina Zeisler. (free pdf) Among many other terms, discusses the Eurasian word for "horse" often mentioned on Language Log (see "Selected readings" [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StanCon 2023 [20-23 July 2023]]]></title>
        <id>http://xianblog.wordpress.com/?p=52753</id>
        <link href="https://xianblog.wordpress.com/2023/04/08/stancon-2023/"/>
        <updated>2023-04-07T22:23:46.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards ML-enabled cleaning robots]]></title>
        <id>http://ai.googleblog.com/2023/04/towards-ml-enabled-cleaning-robots.html</id>
        <link href="http://ai.googleblog.com/2023/04/towards-ml-enabled-cleaning-robots.html"/>
        <updated>2023-04-07T21:06:00.003Z</updated>
        <summary type="html"><![CDATA[Posted by Thomas Lew, Research Intern, and Montserrat Gonzalez Arenas, Research Engineer, Google Research, Brain Team


Over the past several years, the capabilities of robotic systems have improved dramatically. As the technology continues to improve and robotic agents are more routinely deployed in real-world environments, their capacity to assist in day-to-day activities will take on increasing importance. Repetitive tasks like wiping surfaces, folding clothes, and cleaning a room seem well-suited for robots, but remain challenging for robotic systems designed for structured environments like factories. Performing these types of tasks in more complex environments, like offices or homes, requires dealing with greater levels of environmental variability captured by high-dimensional sensor‚Ä¶]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Directing ML toward natural hazard mitigation through collaboration]]></title>
        <id>http://ai.googleblog.com/2023/04/directing-ml-toward-natural-hazard.html</id>
        <link href="http://ai.googleblog.com/2023/04/directing-ml-toward-natural-hazard.html"/>
        <updated>2023-04-07T16:09:00.003Z</updated>
        <summary type="html"><![CDATA[Posted by Oren Gilon, Software Engineer, and Grey Nearing, Research Scientist, Google Research




Floods are the most common type of natural disaster, affecting more than 250 million people globally each year. As part of Google's Crisis Response and our efforts to address the climate crisis, we are using machine learning (ML) models for Flood Forecasting to alert people in areas that are impacted before disaster strikes.


Collaboration between researchers in the industry and academia is essential for accelerating progress towards mutual goals in ML-related research. Indeed, Google's current ML-based flood forecasting approach was developed in collaboration with researchers (1, 2) at the Johannes Kepler University in Vienna, Austria, the University of Alabama, and the Hebrew University of‚Ä¶]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Kerr Learning]]></title>
        <id>http://arxiv.org/abs/2205.12004</id>
        <link href="http://arxiv.org/abs/2205.12004"/>
        <updated>2023-04-07T07:13:38.602Z</updated>
        <summary type="html"><![CDATA[Junyu Liu, Changchun Zhong, Matthew Otten, Anirban Chandra, Cristian
  L. Cortes, Chaoyang Ti, Stephen K Gray, Xu Han]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heavy-Tailed Regularization of Weight Matrices in Deep Neural Networks]]></title>
        <id>http://arxiv.org/abs/2304.02911</id>
        <link href="http://arxiv.org/abs/2304.02911"/>
        <updated>2023-04-07T07:13:38.558Z</updated>
        <summary type="html"><![CDATA[Xuanzhe Xiao, Zeng Li, Chuanlong Xie, Fengwei Zhou]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Lipschitz Functions by GD-trained Shallow Overparameterized
  ReLU Neural Networks]]></title>
        <id>http://arxiv.org/abs/2212.13848</id>
        <link href="http://arxiv.org/abs/2212.13848"/>
        <updated>2023-04-07T07:13:38.552Z</updated>
        <summary type="html"><![CDATA[Ilja Kuzborskij, Csaba Szepesv\'ari]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Efficient MCMC Sampling in Bayesian Neural Networks by
  Exploiting Symmetry]]></title>
        <id>http://arxiv.org/abs/2304.02902</id>
        <link href="http://arxiv.org/abs/2304.02902"/>
        <updated>2023-04-07T07:13:37.582Z</updated>
        <summary type="html"><![CDATA[Jonas Gregor Wiese, Lisa Wimmer, Theodore Papamarkou, Bernd Bischl,
  Stephan G\"unnemann, David R\"ugamer]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A review of ensemble learning and data augmentation models for class
  imbalanced problems: combination, implementation and evaluation]]></title>
        <id>http://arxiv.org/abs/2304.02858</id>
        <link href="http://arxiv.org/abs/2304.02858"/>
        <updated>2023-04-07T07:13:37.565Z</updated>
        <summary type="html"><![CDATA[Azal Ahmad Khan, Omkar Chaudhari, Rohitash Chandra]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Celebrate PyTorch* 2.0 with New Performance Features for AI Developers]]></title>
        <id>https://pytorch.org/blog/celebrate-pytorch-2.0/</id>
        <link href="https://pytorch.org/blog/celebrate-pytorch-2.0/"/>
        <updated>2023-04-07T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[Congratulations to the PyTorch Foundation for its release of PyTorch* 2.0! In this blog, I discuss the four features for which Intel made significant contributions to PyTorch 2.0:]]></summary>
        <author>
            <name>PyTorch Website</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Straggler Mitigation On PyTorch DDP By Hierarchical SGD]]></title>
        <id>https://pytorch.org/blog/straggler-mitigation/</id>
        <link href="https://pytorch.org/blog/straggler-mitigation/"/>
        <updated>2023-04-07T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[PyTorch DDP has been widely adopted across the industry for distributed training, which by default runs synchronous SGD to synchronize gradients across model replicas at every step. The performance of this technique is critical for fast iteration during model exploration as well as resource and cost saving. The performance is critical for fast iteration and cost saving of model development and exploration. To resolve a ubiquitous performance bottleneck introduced by slow nodes in large-scale training, Cruise and Meta co-developed a solution based on the Hierarchical SGD algorithm to significantly accelerate training in the presence of these stragglers.]]></summary>
        <author>
            <name>PyTorch Website</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polyglot Manchu emperor]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58356</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58356&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=polyglot-manchu-emperor"/>
        <updated>2023-04-07T04:50:28.000Z</updated>
        <summary type="html"><![CDATA[From the British Museum: The Chinese characters say: Mƒõi su√¨ni√°n bƒÅn M√©ngg«î, Hu√≠ b√π, FƒÅn b√π dƒõng d√†o jƒ´ng jiƒìji√†n, j√≠ y«ê q√≠ y«î w√®iw√®n, w√∫ j√≠ t≈çngy√¨. Qi√°nl√≥ng hu√°ngd√¨ y√πzh√¨ shƒ´ ÊØèÊ≠≤Âπ¥Áè≠ËíôÂè§ÔºåÂõûÈÉ®ÔºåÁï™ÈÉ®Á≠âÂà∞‰∫¨Êé•Ë¶ãÔºåÂç≥‰ª•ÂÖ∂Ë™ûÊÖ∞ÂïèÔºåÁÑ°ËóâÈÄöË≠Ø„ÄÇ‰πæÈöÜÁöáÂ∏ù¬†„ÄäÂæ°Ë£ΩË©©„Äã We will start with the assumption that the English translation on the wall of the BM is serviceable, but will find, by [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[English for Singapore]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58370</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58370&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=english-for-singapore"/>
        <updated>2023-04-06T23:38:00.000Z</updated>
        <summary type="html"><![CDATA["Majority supports adding English requirement for applicants for Singapore citizenship: poll" Pinyin News (4/6/23) The opposition leader of Singapore, Pritam Singh, said in late February that he supported adding an English test to the requirements for applications for citizenship or permanent residency in Singapore. A recent poll of five hundred Singapore-born citizens found strong popular [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fusing Simulation with Data Science [ CRiSM workshop, 18-9/07/23]]]></title>
        <id>http://xianblog.wordpress.com/?p=52631</id>
        <link href="https://xianblog.wordpress.com/2023/04/07/fusing-simulation-with-data-science-crism-workshop-18-9-07-23/"/>
        <updated>2023-04-06T22:23:44.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Project Starline improves remote communication]]></title>
        <id>http://ai.googleblog.com/2023/04/how-project-starline-improves-remote.html</id>
        <link href="http://ai.googleblog.com/2023/04/how-project-starline-improves-remote.html"/>
        <updated>2023-04-06T19:22:00.000Z</updated>
        <summary type="html"><![CDATA[Posted by Greg Blascovich and Eric Gomez, User Researchers, Google




As companies settle into a new normal of hybrid and distributed work, remote communication technology remains critical for connecting and collaborating with colleagues. While this technology has improved, the core user experience often falls short: conversation can feel stilted, attention can be difficult to maintain, and usage can be fatiguing. 





Project Starline renders people at natural scale on a 3D display and enables natural eye contact.

Project Starline, a technology project that combines advances in hardware and software to create a remote communication experience that feels like you‚Äôre together, even when you‚Äôre thousands of miles apart. This perception of co-presence is created by representing users in 3D‚Ä¶]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-trained Gaussian processes for Bayesian optimization]]></title>
        <id>http://ai.googleblog.com/2023/04/pre-trained-gaussian-processes-for.html</id>
        <link href="http://ai.googleblog.com/2023/04/pre-trained-gaussian-processes-for.html"/>
        <updated>2023-04-06T17:02:00.002Z</updated>
        <summary type="html"><![CDATA[Posted by Zi Wang and Kevin Swersky, Research Scientists, Google Research, Brain Team 





Bayesian optimization (BayesOpt) is a powerful tool widely used for global optimization tasks, such as hyperparameter tuning, protein engineering, synthetic chemistry, robot learning, and even baking cookies. BayesOpt is a great strategy for these problems because they all involve optimizing black-box functions that are expensive to evaluate. A black-box function‚Äôs underlying mapping from inputs (configurations of the thing we want to optimize) to outputs (a measure of performance) is unknown. However, we can attempt to understand its internal workings by evaluating the function for different combinations of inputs. Because each evaluation can be computationally expensive, we need to find the best i‚Ä¶]]></summary>
        <author>
            <name>Google AI Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Fleet Learning]]></title>
        <id>http://bair.berkeley.edu/blog/2023/04/06/ifl/</id>
        <link href="http://bair.berkeley.edu/blog/2023/04/06/ifl/"/>
        <updated>2023-04-06T09:00:00.000Z</updated>
        <summary type="html"><![CDATA[Figure 1: ‚ÄúInteractive Fleet Learning‚Äù (IFL) refers to robot fleets in industry and academia that fall back on human teleoperators when necessary and continually learn from them over time.


In the last few years we have seen an exciting development in robotics and artificial intelligence: large fleets of robots have left the lab and entered the real world. Waymo, for example, has over 700 self-driving cars operating in Phoenix and San Francisco and is currently expanding to Los Angeles. Other industrial deployments of robot fleets include applications like e-commerce order fulfillment at Amazon and Ambi Robotics as well as food delivery at Nuro and Kiwibot.
Commercial and industrial deployments of robot fleets: package delivery (top left), food delivery (bottom left), e-commerce order ful‚Ä¶]]></summary>
        <author>
            <name>The Berkeley Artificial Intelligence Research Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the universal approximation property of radial basis function neural
  networks]]></title>
        <id>http://arxiv.org/abs/2304.02220</id>
        <link href="http://arxiv.org/abs/2304.02220"/>
        <updated>2023-04-06T07:13:33.299Z</updated>
        <summary type="html"><![CDATA[Aysu Ismayilova, Muhammad Ismayilov]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian neural networks via MCMC: a Python-based tutorial]]></title>
        <id>http://arxiv.org/abs/2304.02595</id>
        <link href="http://arxiv.org/abs/2304.02595"/>
        <updated>2023-04-06T07:13:33.294Z</updated>
        <summary type="html"><![CDATA[Rohitash Chandra, Royce Chen, Joshua Simmons]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithm-Dependent Bounds for Representation Learning of Multi-Source
  Domain Adaptation]]></title>
        <id>http://arxiv.org/abs/2304.02064</id>
        <link href="http://arxiv.org/abs/2304.02064"/>
        <updated>2023-04-06T07:13:33.269Z</updated>
        <summary type="html"><![CDATA[Qi Chen, Mario Marchand]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Data Representations with Joint Diffusion Models]]></title>
        <id>http://arxiv.org/abs/2301.13622</id>
        <link href="http://arxiv.org/abs/2301.13622"/>
        <updated>2023-04-06T07:13:33.198Z</updated>
        <summary type="html"><![CDATA[Kamil Deja, Tomasz Trzcinski, Jakub M. Tomczak]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Free Energy of Deep ReLU Neural Network in Overparametrized
  Cases]]></title>
        <id>http://arxiv.org/abs/2303.15739</id>
        <link href="http://arxiv.org/abs/2303.15739"/>
        <updated>2023-04-06T07:13:33.192Z</updated>
        <summary type="html"><![CDATA[Shuya Nagayasu, Sumio Watanabe]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mandarin with an English accent]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58368</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58368&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=mandarin-with-an-english-accent"/>
        <updated>2023-04-06T01:31:13.000Z</updated>
        <summary type="html"><![CDATA[Something very funny happened to me earlier today, funny enough that I would like to share it with all Language Log readers who may be desirous of something more than a cup of coffee to perk them up on a gray, midweek morning. I entered the following Mandarin expression into Google Translate and wanted to [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[[A]ABC in Hawai‚Äôi]]></title>
        <id>http://xianblog.wordpress.com/?p=52670</id>
        <link href="https://xianblog.wordpress.com/2023/04/06/aabc-in-hawaii/"/>
        <updated>2023-04-05T22:23:46.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An AI Miracle Malcontent]]></title>
        <id>https://hunch.net/?p=13763005</id>
        <link href="https://hunch.net/?p=13763005"/>
        <updated>2023-04-05T21:44:38.000Z</updated>
        <summary type="html"><![CDATA[The stark success of OpenAI‚Äôs GPT4 model surprised me shifting my view from ‚Äúreally good autocomplete‚Äù (roughly inline with intuitions here) to a dialog agent exhibiting a significant scope of reasoning and intelligence. Some of the MSR folks did a fairly thorough study of capabilities which seems like a good reference. I think of GPT4 ‚Ä¶ 
Continue reading "An AI Miracle Malcontent"]]></summary>
        <author>
            <name>John Langford</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An AI Miracle Malcontent]]></title>
        <id>https://hunch.net/?p=13763005</id>
        <link href="https://hunch.net/?p=13763005"/>
        <updated>2023-04-05T21:44:38.000Z</updated>
        <summary type="html"><![CDATA[The stark success of OpenAI‚Äôs GPT4 model surprised me shifting my view from ‚Äúreally good autocomplete‚Äù (roughly inline with intuitions here) to a dialog agent exhibiting a significant scope of reasoning and intelligence. Some of the MSR folks did a fairly thorough study of capabilities which seems like a good reference. I think of GPT4 ‚Ä¶ 
Continue reading "An AI Miracle Malcontent"]]></summary>
        <author>
            <name>John Langford</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-Rate-Free Learning by D-Adaptation]]></title>
        <id>http://arxiv.org/abs/2301.07733</id>
        <link href="http://arxiv.org/abs/2301.07733"/>
        <updated>2023-04-05T07:15:00.853Z</updated>
        <summary type="html"><![CDATA[Aaron Defazio, Konstantin Mishchenko]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework and Benchmark for Deep Batch Active Learning for Regression]]></title>
        <id>http://arxiv.org/abs/2203.09410</id>
        <link href="http://arxiv.org/abs/2203.09410"/>
        <updated>2023-04-05T07:15:00.846Z</updated>
        <summary type="html"><![CDATA[David Holzm\"uller, Viktor Zaverkin, Johannes K\"astner, Ingo
  Steinwart]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal rates of approximation by shallow ReLU$^k$ neural networks and
  applications to nonparametric regression]]></title>
        <id>http://arxiv.org/abs/2304.01561</id>
        <link href="http://arxiv.org/abs/2304.01561"/>
        <updated>2023-04-05T07:15:00.838Z</updated>
        <summary type="html"><![CDATA[Yunfei Yang, Ding-Xuan Zhou]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning and Concentration for High Dimensional Linear Gaussians: an
  Invariant Subspace Approach]]></title>
        <id>http://arxiv.org/abs/2304.01708</id>
        <link href="http://arxiv.org/abs/2304.01708"/>
        <updated>2023-04-05T07:15:00.822Z</updated>
        <summary type="html"><![CDATA[Muhammad Abdullah Naeem]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Unlabelled Data into Bayesian Neural Networks]]></title>
        <id>http://arxiv.org/abs/2304.01762</id>
        <link href="http://arxiv.org/abs/2304.01762"/>
        <updated>2023-04-05T07:15:00.816Z</updated>
        <summary type="html"><![CDATA[Mrinank Sharma, Tom Rainforth, Yee Whye Teh, Vincent Fortuin]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polars for initial data analysis, Polars for production]]></title>
        <id>https://pythonspeed.com/articles/polars-exploratory-data-analysis-vs-production/</id>
        <link href="https://pythonspeed.com/articles/polars-exploratory-data-analysis-vs-production/"/>
        <updated>2023-04-05T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Initial data analysis (IDA) has different goals than your final, production data analysis:
With IDA you need to examine the initial data and intermediate results, check your assumptions, and try different approaches.
Exploratory data analysis has similar requirements.
Once you‚Äôre happy with your approach, and you‚Äôre ready to run the analysis in an automated manner, you care a lot more about speed and resource usage.
These different goals often benefit from different implementation strategies and tools‚Äîunless you have a sufficiently flexible tool like Polars, the super-fast dataframe library.
In particular, Polars has two fundamental APIs, each of which is useful in different situations:
‚ÄúEager‚Äù mode, which is similar to how Pandas works, is well-suited for initial and exploratory data analysis.
For production use, ‚Äúlazy‚Äù mode often execute much faster, with lower memory usage, at the cost of not giving you access to intermediate result.
In this article we‚Äôll use both two APIs and see how Polars lets you transition from looking at the data to something we can run even more efficiently in production.
Read more...]]></summary>
        <author>
            <name>Python‚áíSpeed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[common leftovers]]></title>
        <id>http://xianblog.wordpress.com/?p=52527</id>
        <link href="https://xianblog.wordpress.com/2023/04/05/common-leftovers/"/>
        <updated>2023-04-04T22:23:25.000Z</updated>
        <summary type="html"><![CDATA[The Riddler was about a number game, which meant starting a sum at 0 and adding i to the current sum at time i, almost a Fibonacci sequence but not exactly, since the sequence is simply which is easy to code, is of period 200, and this code showed that 3, 28, 53, and 78 [‚Ä¶]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topical weather prediction]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58361</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58361&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=topical-weather-prediction"/>
        <updated>2023-04-04T21:03:36.000Z</updated>
        <summary type="html"><![CDATA[Weather outlook: MOSTLY STORMY on Tuesday. üòâ pic.twitter.com/qKcqZa6svL ‚Äî Jon Cooper (@joncoopertweets) April 3, 2023]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Desultory philological, literary, and historical notes on Xanadu]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58348</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58348&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=desultory-philological-literary-and-historical-notes-on-xanadu"/>
        <updated>2023-04-04T12:46:53.000Z</updated>
        <summary type="html"><![CDATA[Our previous post was on "Hallucinations: In Xanadu did LLMs vainly fancify" (4/3/23).¬† If you were wondering where such an evocative, exotic name came from, it has a direct lineage back to the Mongol Yuan Dynasty (1271-1368) of China where it was called Sh√†ngd≈´ ‰∏äÈÉΩ ("Upper Capital") in Mandarin, ultimately from early Mandarin  Ça≈ã` t≈´.¬† [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Fitness Functions for Machine Programming]]></title>
        <id>http://arxiv.org/abs/1908.08783</id>
        <link href="http://arxiv.org/abs/1908.08783"/>
        <updated>2023-04-04T07:13:40.997Z</updated>
        <summary type="html"><![CDATA[Shantanu Mandal, Todd A. Anderson, Javier S. Turek, Justin
  Gottschlich, Shengtian Zhou, Abdullah Muzahid]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modelling customer churn for the retail industry in a deep learning
  based sequential framework]]></title>
        <id>http://arxiv.org/abs/2304.00575</id>
        <link href="http://arxiv.org/abs/2304.00575"/>
        <updated>2023-04-04T07:13:40.990Z</updated>
        <summary type="html"><![CDATA[Juan Pablo Equihua, Henrik Nordmark, Maged Ali, Berthold Lausen]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Indeterminate Probability Neural Network]]></title>
        <id>http://arxiv.org/abs/2303.11536</id>
        <link href="http://arxiv.org/abs/2303.11536"/>
        <updated>2023-04-04T07:13:40.985Z</updated>
        <summary type="html"><![CDATA[Tao Yang, Chuang Liu, Xiaofeng Ma, Weijia Lu, Ning Wu, Bingyang Li,
  Zhifei Yang, Peng Liu, Lin Sun, Xiaodong Zhang, Can Zhang]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient human-in-loop deep learning model training with iterative
  refinement and statistical result validation]]></title>
        <id>http://arxiv.org/abs/2304.00990</id>
        <link href="http://arxiv.org/abs/2304.00990"/>
        <updated>2023-04-04T07:13:40.978Z</updated>
        <summary type="html"><![CDATA[Manuel Zahn, Douglas P. Perrin]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linguistic nationalism news from Ohio, Italy, and California]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58351</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58351&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=linguistic-nationalism-news-from-ohio-italy-and-california"/>
        <updated>2023-04-04T02:43:07.000Z</updated>
        <summary type="html"><![CDATA[Adam Schrader, "Ohio Sen. J.D. Vance introduces bill to make English official U.S. language", UPI 3/30/2023. A press release from Vance's office is here, and here's the text of the bill. "Meloni's party looks to shield Italian language from foreign contamination", Reuters 3/31/2023. Prime Minister Giorgia Meloni's party has proposed imposing fines of up to [‚Ä¶]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[portrait of the author as a young man [X rated]]]></title>
        <id>http://xianblog.wordpress.com/?p=52454</id>
        <link href="https://xianblog.wordpress.com/2023/04/04/portrait-of-the-author-as-a-young-man-x-ray/"/>
        <updated>2023-04-03T22:23:17.000Z</updated>
        <summary type="html"><![CDATA[Visit the post for more.]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alef Corner: Deep Learning 2020, 2030, 2040]]></title>
        <id>http://gilkalai.wordpress.com/?p=24111</id>
        <link href="https://gilkalai.wordpress.com/2023/04/03/alef-corner-deep-learning-2020-2030-2040/"/>
        <updated>2023-04-03T20:36:02.000Z</updated>
        <summary type="html"><![CDATA[Deep learning 2020 Deep learning 2030 Deep learning 2040]]></summary>
        <author>
            <name>Gil Kalai</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alef Corner: Deep Learning 2020, 2030, 2040]]></title>
        <id>http://gilkalai.wordpress.com/?p=24111</id>
        <link href="https://gilkalai.wordpress.com/2023/04/03/alef-corner-deep-learning-2020-2030-2040/"/>
        <updated>2023-04-03T20:36:02.000Z</updated>
        <summary type="html"><![CDATA[Deep learning 2020 Deep learning 2030 Deep learning 2040]]></summary>
        <author>
            <name>Gil Kalai</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hallucinations:  In Xanadu did LLMs vainly fancify]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58338</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58338&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=hallucinations-in-xanadu-did-llms-vainly-fancify"/>
        <updated>2023-04-03T15:55:16.000Z</updated>
        <summary type="html"><![CDATA[Bill Benzon has been our most prolific humanistic commentator about GPTs, almost as prolific as GPTs themselves.¬† Here he introduces his latest creation in / on the genre: "From 'Kubla Khan' through GPT and beyond", 3 Quarks Daily (3/27/23) In a covering note to me, Bill writes: A story about how I came to be [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Koala: A Dialogue Model for Academic Research]]></title>
        <id>http://bair.berkeley.edu/blog/2023/04/03/koala/</id>
        <link href="http://bair.berkeley.edu/blog/2023/04/03/koala/"/>
        <updated>2023-04-03T14:30:00.000Z</updated>
        <summary type="html"><![CDATA[In this post, we introduce Koala, a chatbot trained by fine-tuning Meta‚Äôs LLaMA on dialogue data gathered from the web. We describe the dataset curation and training process of our model, and also present the results of a user study that compares our model to ChatGPT and Stanford‚Äôs Alpaca. Our results show that Koala can effectively respond to a variety of user queries, generating responses that are often preferred over Alpaca, and at least tied with ChatGPT in over half of the cases.
We hope that these results contribute further to the discourse around the relative performance of large closed-source models to smaller public models. In particular, it suggests that models that are small enough to be run locally can capture much of the performance of their larger cousins if trained on carefu‚Ä¶]]></summary>
        <author>
            <name>The Berkeley Artificial Intelligence Research Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiasÈ°πÁöÑÁ•ûÂ•á‰ΩúÁî®ÔºöRoPE + Bias = Êõ¥Â•ΩÁöÑÈïøÂ∫¶Â§ñÊé®ÊÄß]]></title>
        <id>https://kexue.fm/archives/9577</id>
        <link href="https://kexue.fm/archives/9577"/>
        <updated>2023-04-03T12:56:00.000Z</updated>
        <summary type="html"><![CDATA[‰∏á‰∏áÊ≤°ÊÉ≥Âà∞ÔºåBiasÈ°πËÉΩË∑üTransformerÁöÑÈïøÂ∫¶Â§ñÊé®ÊÄßËÅîÁ≥ªÂú®‰∏ÄËµ∑ÔºÅÈïøÂ∫¶Â§ñÊé®ÊÄßÊòØÊàë‰ª¨Â∏åÊúõTransformerÂÖ∑ÊúâÁöÑ‰∏Ä‰∏™ÁêÜÊÉ≥ÊÄßË¥®ÔºåÁ¨îËÄÖÊõæÂú®„ÄäTransformerÂçáÁ∫ß‰πãË∑ØÔºö7„ÄÅÈïøÂ∫¶Â§ñÊé®ÊÄß‰∏éÂ±Ä...]]></summary>
        <author>
            <name>ËãèÂâëÊûó</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An interpretable neural network-based non-proportional odds model for
  ordinal regression with continuous response]]></title>
        <id>http://arxiv.org/abs/2303.17823</id>
        <link href="http://arxiv.org/abs/2303.17823"/>
        <updated>2023-04-03T08:19:34.861Z</updated>
        <summary type="html"><![CDATA[Akifumi Okuno, Kazuharu Harada]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Estimators Show Information Compression in Deep Neural Networks]]></title>
        <id>http://arxiv.org/abs/1902.09037</id>
        <link href="http://arxiv.org/abs/1902.09037"/>
        <updated>2023-04-03T07:14:03.000Z</updated>
        <summary type="html"><![CDATA[Ivan Chelombiev, Conor Houghton, Cian O'Donnell]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rapid prediction of lab-grown tissue properties using deep learning]]></title>
        <id>http://arxiv.org/abs/2303.18017</id>
        <link href="http://arxiv.org/abs/2303.18017"/>
        <updated>2023-04-03T07:14:02.992Z</updated>
        <summary type="html"><![CDATA[Allison E. Andrews, Hugh Dickinson, James P. Hague]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-Optimal Learning of Extensive-Form Games with Imperfect Information]]></title>
        <id>http://arxiv.org/abs/2202.01752</id>
        <link href="http://arxiv.org/abs/2202.01752"/>
        <updated>2023-04-03T07:14:02.958Z</updated>
        <summary type="html"><![CDATA[Yu Bai, Chi Jin, Song Mei, Tiancheng Yu]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Kernel Approach for PDE Discovery and Operator Learning]]></title>
        <id>http://arxiv.org/abs/2210.08140</id>
        <link href="http://arxiv.org/abs/2210.08140"/>
        <updated>2023-04-03T07:14:02.939Z</updated>
        <summary type="html"><![CDATA[Da Long, Nicole Mrvaljevic, Shandian Zhe, Bamdad Hosseini]]></summary>
        <author>
            <name>arxivist feed</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PyTorch &amp; OpenXLA: The Path Forward]]></title>
        <id>https://pytorch.org/blog/pytorch-2.0-xla-path-forward/</id>
        <link href="https://pytorch.org/blog/pytorch-2.0-xla-path-forward/"/>
        <updated>2023-04-03T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[As we celebrate the release of OpenXLA, PyTorch 2.0, and PyTorch/XLA 2.0, it‚Äôs worth taking a step back and sharing where we see it all going in the short to medium term. With PyTorch adoption leading in the AI space and XLA supporting best-in-class compiler features, PyTorch/XLA is well positioned to provide a cutting edge development stack for both model training and inference. To achieve this, we see investments in three main areas:]]></summary>
        <author>
            <name>PyTorch Website</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lapparadish]]></title>
        <id>http://xianblog.wordpress.com/?p=52618</id>
        <link href="https://xianblog.wordpress.com/2023/04/03/lapparadish/"/>
        <updated>2023-04-02T22:23:08.000Z</updated>
        <summary type="html"><![CDATA[On top of BayesComp 2023 being rich and exciting, spending a week above the Arctic circle, by about 68‚Å∞ North was most pleasurable. If we did not really see Northern Lights/auroras, as the above was taken by one of us with a long exposure shot on a particularly cold evening, rather than the diaphanous veils [‚Ä¶]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invented Chinese name of an LA lawyer]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58318</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58318&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=invented-chinese-name-of-an-la-lawyer"/>
        <updated>2023-04-02T13:31:03.000Z</updated>
        <summary type="html"><![CDATA[Around 60% of the people living in the San Gabriel area are Asians, and the largest proportion among them are Chinese.¬† To attract the business of the local population, attorney Scott Warmuth decided to put up Chinese billboards in Monterey Park about a decade ago.¬† How it happened is described in this article: "Column: Racial [‚Ä¶]]]></summary>
        <author>
            <name>Victor Mair</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thrilling linguistics?]]></title>
        <id>https://languagelog.ldc.upenn.edu/nll/?p=58333</id>
        <link href="https://languagelog.ldc.upenn.edu/nll/?p=58333&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=thrilling-linguistics"/>
        <updated>2023-04-02T13:16:40.000Z</updated>
        <summary type="html"><![CDATA[A recent linguistic message from Dinosaur Comics: The first five panels have a clear socio- and psycholinguistic message: T-Rex: Gasp! I should use the word "frisson" more often! There's been so many OPPORTUNITIES to use the word, Dromiceiomimus, but I let them all pass me by because I say "thrill" instead. But wouldn't I seem [‚Ä¶]]]></summary>
        <author>
            <name>Mark Liberman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monte Carlo swindles]]></title>
        <id>http://xianblog.wordpress.com/?p=52480</id>
        <link href="https://xianblog.wordpress.com/2023/04/02/monte-carlo-swindles/"/>
        <updated>2023-04-01T22:23:23.000Z</updated>
        <summary type="html"><![CDATA[While reading Boos and Hugues-Olivier‚Äôs 1998 American Statistician paper on the applications of Basu‚Äôs theorem I can across the notion of Monte Carlo swindles. Where a reduced variance can be achieved without the corresponding increase in Monte Carlo budget. For instance, approximating the variance of the median statistic Œú for a Normal location family can [‚Ä¶]]]></summary>
        <author>
            <name>xi'an</name>
        </author>
    </entry>
</feed>